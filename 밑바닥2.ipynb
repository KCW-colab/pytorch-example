{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "밑바닥2",
      "provenance": [],
      "collapsed_sections": [
        "Gq0ZAt6v5HXp",
        "goYE__qvQVft",
        "-jJfSu7MfnTe",
        "Z4EizYY_H-A7",
        "Ux-B6BHdFlI0",
        "OHNOovCXll1J",
        "02TFn-kzMCuZ"
      ],
      "toc_visible": true,
      "mount_file_id": "1qmvcbV1IW_HI8b4PePRIxhS6Fgq5kf0L",
      "authorship_tag": "ABX9TyMUGm8j7HS8mnqauIhc43uP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KCW-colab/pytorch-example/blob/master/%EB%B0%91%EB%B0%94%EB%8B%A52.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbGxUNFXuVQj"
      },
      "source": [
        "행아웃 https://hangouts.google.com/call/H1BT80UlutUsEG3guXgpACEM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq0ZAt6v5HXp"
      },
      "source": [
        "### 1장 - 신경망 복습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_iXpOH0tCEC",
        "outputId": "a57c7be9-228d-416d-ce40-d029f5bd10c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "x = np.array([1, 2, 3])\n",
        "x.__class__ # 클래스 이름 표시\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNHAo01L0MKr",
        "outputId": "d00e473a-296b-4471-8ade-7302e9267237",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POHa_JbT0j-_",
        "outputId": "25c5258a-1b60-43f7-837a-6e9a67c46215",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x.ndim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bgp5ohkX0kvH",
        "outputId": "8ce62fb4-c70e-4e57-eff5-d0f2ec2eedac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "W = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "W.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pzvw5n7Z0orA",
        "outputId": "0ee1c30d-2f15-495d-e589-281689ba747d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "W.ndim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JlvLlwS0qD2",
        "outputId": "5ade66f2-deef-44dd-bb16-a8bdd0149a6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "W = np.array([[1, 2, 3,], [4, 5, 6]])\n",
        "X = np.array([[0, 1, 2], [3, 4, 5]])\n",
        "print(W + X)\n",
        "print(W*X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1  3  5]\n",
            " [ 7  9 11]]\n",
            "[[ 0  2  6]\n",
            " [12 20 30]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c1VeHdD1rKQ",
        "outputId": "ba19e7c2-4b54-4334-9bf5-316bc9b9a3f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "A = np.array([[1, 2], [3, 4]])\n",
        "A * 10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[10, 20],\n",
              "       [30, 40]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnAmMdyf1vS4",
        "outputId": "d68260c5-5f37-4722-dff2-8a546e6ebc21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "b = np.array([10, 20])\n",
        "A * b"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[10, 40],\n",
              "       [30, 80]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bch28YLU2CMO",
        "outputId": "e2024d84-c187-4d32-c7cd-390a7ed2e882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# 벡터의 내적\n",
        "a = np.array([1, 2, 3])\n",
        "b = np.array([4, 5, 6])\n",
        "print(np.dot(a,b))\n",
        "\n",
        " # 행렬의 곱\n",
        "A = np.array([[1, 2], [3, 4]])\n",
        "B = np.array([[5, 6], [7, 8]])\n",
        "np.matmul(A, B)\n",
        "\n",
        "# 2차원 까지는 np.dot이나 matmul의 역할이 같음."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[19, 22],\n",
              "       [43, 50]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yKqW4Zm28cu",
        "outputId": "674359d2-59e6-4576-d7a0-84cd960404ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "W1 = np.random.randn(2, 4) # 가중치\n",
        "b1 = np.random.randn(4) # 편향\n",
        "x = np.random.randn(10, 2) # 입력\n",
        "h = np.matmul(x, W1) + b1\n",
        "print(h.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDCirWiZ4Izj"
      },
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / ( 1+ np.exp(-x))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBreFgt94563"
      },
      "source": [
        "a = sigmoid(h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NCwSWkl49QX",
        "outputId": "ac219536-dd87-43d5-f1f2-1c033686396f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "x = np.random.randn(10, 2) \n",
        "W1 = np.random.randn(2, 4) \n",
        "b1 = np.random.randn(4)\n",
        "W2 = np.random.randn(4, 3) \n",
        "b2 = np.random.randn(3)\n",
        "\n",
        "h = np.matmul(x, W1) + b1\n",
        "a = sigmoid(h)\n",
        "s = np.matmul(a, W2) + b2\n",
        "print(s.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ls93tO35Z_r"
      },
      "source": [
        "import numpy as np\n",
        "class Sigmoid:\n",
        "  def __init__(self):\n",
        "    self.params=[]\n",
        "\n",
        "  def forward(self, x):\n",
        "    return 1 / ( 1+ np.exp(-x))\n",
        "\n",
        "class Affine:\n",
        "  def __init__(self, W, b):\n",
        "    self.params = [W, b]\n",
        "\n",
        "  def forward(self, x):\n",
        "    W, b = self.params\n",
        "    out = np.matmul(x, W) + b\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnuivKye6u3R"
      },
      "source": [
        "class TwoLayerNet:\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    I, H, O = input_size, hidden_size, output_size\n",
        "\n",
        "    # 가중치와 편향 초기화\n",
        "    W1 = np.random.randn(I, H)\n",
        "    b1 = np.random.randn(H)\n",
        "    W2 = np.random.randn(H, O)\n",
        "    b2 = np.random.randn(O)\n",
        "\n",
        "    # 게층 생성\n",
        "    self.layers = [Affine(W1, b1), Sigmoid(), Affine(W2, b2)]\n",
        "\n",
        "    # 모든 가중치를 리스트에 모은다.\n",
        "    self.params=[]\n",
        "    for layer in self.layers:\n",
        "      self.params += layer.params\n",
        "\n",
        "  def predict(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer.forward(x)\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vkg405qs8CCk",
        "outputId": "d6f14f00-2964-451f-fb46-3a098ade4f13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = np.random.randn(10, 2)\n",
        "model = TwoLayerNet(2, 4, 3)\n",
        "s = model.predict(x)\n",
        "print(s.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9PoK-sN9KZO"
      },
      "source": [
        "import numpy as np\n",
        "D, N  = 8 , 7\n",
        "x = np.random.randn(1, D) # 입력\n",
        "y = np.repeat(x, N, axis = 0) # 순전파  # 원소 복제를 시행하는 numpy의 함수 (x 를 N번 복제하는데 세로방향으로 복제함.)\n",
        "# y.shape = (7,8)\n",
        "dy = np.random.randn(N, D) # 무작위 기울기\n",
        "dx = np.sum(dy, axis = 0, keepdims=True) # 역전파 # 2차원 유지를 위해 keepdims = True로 둠."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-si--2GXE6vk"
      },
      "source": [
        "import numpy as np\n",
        "D, N = 8, 7\n",
        "x = np.random.randn(N, D) # 입력\n",
        "y = np.sum(x, axis = 0, keepdims = True) # 순전파\n",
        "dy = np.random.randn(1, D) # 무작위 기울기\n",
        "dx = np.repeat(dy, N, axis=0) # 역전파\n",
        "# Sum 노드와 repeat 노드는 반대관계임.\n",
        "# ex) Sum 노드의 순전파 => repeat 노드의 역전파"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rptCDofyFmI6"
      },
      "source": [
        "class MatMul:\n",
        "  def __init__(self, W):\n",
        "    self.params = [W]\n",
        "    self.grads = [np.zeros_like(W)]\n",
        "    self.x = None\n",
        "  \n",
        "  def forward(self, x):\n",
        "    W, = self.params\n",
        "    out = np.matmul(x, W)\n",
        "    self.x = x\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    W, = self.params\n",
        "    dx = np.matmul(dout, W.T)\n",
        "    dW = np.matmul(self.x.T, dout)\n",
        "    self.grads[0][...] = dW\n",
        "    return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hrtq_N8dGt2v",
        "outputId": "6ef3f528-91a9-4cc3-bd02-1fb7a959907a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "'''\n",
        "얕은 복사는 본품의 메모리값을 복사하는 것임.\n",
        "깊은 복사는 본품의 숫자 자체를 복사해오는 것\n",
        "\n",
        "얕은 복사 종류\n",
        "list1 = [1, 2, 3]\n",
        "list2 = list1\n",
        "\n",
        "깊은 복사 종류\n",
        "list2 = list1[:]\n",
        "list2 = list1.copy\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n얕은 복사는 본품의 메모리값을 복사하는 것임.\\n깊은 복사는 본품의 숫자 자체를 복사해오는 것\\n\\n얕은 복사 종류\\nlist1 = [1, 2, 3]\\nlist2 = list1\\n\\n깊은 복사 종류\\nlist2 = list1[:]\\nlist2 = list1.copy\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1eU4nilIJ3b"
      },
      "source": [
        "class Sigmoid:\n",
        "  def __init__(self):\n",
        "    self.params, self.grads = [], []\n",
        "    self.out = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = 1 / (1 + np.exp(-x))\n",
        "    self.out = out\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = dout * (1.0 - self.out) * self.out\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnhI1irdIqnU"
      },
      "source": [
        "class Affine:\n",
        "  def __init__(self, W, b):\n",
        "    self.params = [W, b]\n",
        "    self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
        "    self.x = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    W, b = self.params\n",
        "    out = np.matmul(x, W) + b\n",
        "    self.x = x\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    W, b = self.params\n",
        "    dx = np.matmul(dout, W.T)\n",
        "    dW = np.matmul(self.x.T, dout)\n",
        "    db = np.sum(dout, axis = 0)\n",
        "\n",
        "    self.grads[0][...] = dW\n",
        "    self.grads[1][...] = db\n",
        "    return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82M6OnzHKGSj"
      },
      "source": [
        "class SGD:\n",
        "  def __init__(self, lr=0.01):\n",
        "    self.lr = lr\n",
        "\n",
        "  def update(self, params, grads):\n",
        "    for i in range(len(params)):\n",
        "      params[i] -= self.lr * grads[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjQDfoLDK2fE",
        "outputId": "0204c60c-8a9b-4650-98db-4ecacc7e0ffa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.pardir)\n",
        "\n",
        "\n",
        "from dataset import spiral\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x,t = spiral.load_data()\n",
        "print('x', x.shape) # (300, 2)\n",
        "print('t', t.shape) # (300, 3)\n",
        "\n",
        "'''\n",
        "# 데이터점 플롯\n",
        "N = 100\n",
        "CLS_NUM = 3\n",
        "markers = ['o', 'x', '^']\n",
        "for i in range(CLS_NUM):\n",
        "    plt.scatter(x[i*N:(i+1)*N, 0], x[i*N:(i+1)*N, 1], s=40, marker=markers[i])\n",
        "plt.show()\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x (300, 2)\n",
            "t (300, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# 데이터점 플롯\\nN = 100\\nCLS_NUM = 3\\nmarkers = ['o', 'x', '^']\\nfor i in range(CLS_NUM):\\n    plt.scatter(x[i*N:(i+1)*N, 0], x[i*N:(i+1)*N, 1], s=40, marker=markers[i])\\nplt.show()\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga7r1t2C5PT_"
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.pardir)\n",
        "import numpy as np\n",
        "from common.layers import Affine, Sigmoid, SoftmaxWithLoss\n",
        "\n",
        "class TwoLayerNet:\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    I, H, O = input_size, hidden_size, output_size\n",
        "\n",
        "    # 가중치와 편향 초기화\n",
        "    W1 = 0.01 * np.random.randn(I, H)\n",
        "    b1 = np.zeros(H)\n",
        "    W2 = 0.01 * np.random.randn(H, O)\n",
        "    b2 = np.zeros(0)\n",
        "\n",
        "    # 계층 생성\n",
        "    self.layers = [Affine(W1, b1), Sigmoid(), Affine(W2, b2)]\n",
        "    self.loss_layer = SoftmaxWithLoss()\n",
        "\n",
        "    # 모든 가중치와 기울기를 리스트에 모은다.\n",
        "    self.params, self.grads = [], []\n",
        "    for layer in self.layers:\n",
        "      self.params += layer.params\n",
        "      self.grads += layer.grads\n",
        "\n",
        "  def predict(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer.forward(x)\n",
        "    return x\n",
        "\n",
        "  def forward(self, x, t):\n",
        "    score = self.predict(x)\n",
        "    loss = self.loss_layer.forward(score, t)\n",
        "    return loss\n",
        "\n",
        "\n",
        "  def backward(self, dout = 1):\n",
        "    dout = self.loss_layer.backward(dout)\n",
        "    for layer in reversed(self.layers):\n",
        "      dout = layer.backward(dout)\n",
        "    return dout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePuS_Ue19iil",
        "outputId": "b8591f2a-1f85-4c13-f639-56c4b89b341c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master\")\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master/ch01\")\n",
        "sys.path.append(os.pardir)\n",
        "import numpy as np\n",
        "from common.optimizer import SGD\n",
        "from dataset import spiral\n",
        "import matplotlib.pyplot as plt\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "max_epoch = 300 # 학습하는 에폭 수\n",
        "batch_size = 30 # 미니배치 크기\n",
        "hidden_size = 10 # 은닉층의 뉴런 수\n",
        "learning_rate = 1.0 # 학습률\n",
        "\n",
        "\n",
        "x, t = spiral.load_data()\n",
        "# 데이터 읽기\n",
        "model = TwoLayerNet(input_size = 2, hidden_size = hidden_size, output_size = 3)\n",
        "# 신경망 모델 생성\n",
        "optimizer = SGD(lr = learning_rate)\n",
        "# SGD 모델 생성\n",
        "# 에폭은 학습 단위로서, 1에폭은 학습 데이터를 모두 '살펴본' 시점.\n",
        "\n",
        "# 학습에 사용하는 변수\n",
        "data_size = len(x)\n",
        "max_iters = data_size // batch_size  # // 연산자는 나눗셈 후 소숫점을 버리는 연산자.\n",
        "total_loss = 0\n",
        "loss_count = 0\n",
        "loss_list = []\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "  # 데이터 뒤섞기\n",
        "  idx = np.random.permutation(data_size)\n",
        "  x = x[idx]\n",
        "  t = t[idx]\n",
        "\n",
        "  for iters in range(max_iters):\n",
        "    batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
        "    batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
        "\n",
        "  # 기울기를 구해 매개변수 갱신\n",
        "    loss = model.forward(batch_x, batch_t)\n",
        "    model.backward()\n",
        "    optimizer.update(model.params, model.grads)\n",
        "\n",
        "  # 역전파 순전파를 이용해서 기울기들을 모두저장\n",
        "\n",
        "    total_loss += loss\n",
        "    loss_count += 1\n",
        "\n",
        "  \n",
        "\n",
        "  # 정기적으로 학습 경과 출력\n",
        "    if (iters+1) % 10 == 0:\n",
        "      avg_loss = total_loss / loss_count\n",
        "      print('|에폭 %d| 반복 %d / %d | 손실 %.2f' %(epoch+1, iters+1, max_iters, avg_loss))\n",
        "      loss_list.append(avg_loss)\n",
        "      total_loss, loss_count = 0, 0\n",
        "\n",
        "# 학습 결과 플롯\n",
        "plt.plot(np.arange(len(loss_list)), loss_list, label='train')\n",
        "plt.xlabel('반복 (x10)')\n",
        "plt.ylabel('손실')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|에폭 1| 반복 10 / 10 | 손실 1.13\n",
            "|에폭 2| 반복 10 / 10 | 손실 1.13\n",
            "|에폭 3| 반복 10 / 10 | 손실 1.12\n",
            "|에폭 4| 반복 10 / 10 | 손실 1.12\n",
            "|에폭 5| 반복 10 / 10 | 손실 1.11\n",
            "|에폭 6| 반복 10 / 10 | 손실 1.14\n",
            "|에폭 7| 반복 10 / 10 | 손실 1.16\n",
            "|에폭 8| 반복 10 / 10 | 손실 1.11\n",
            "|에폭 9| 반복 10 / 10 | 손실 1.12\n",
            "|에폭 10| 반복 10 / 10 | 손실 1.13\n",
            "|에폭 11| 반복 10 / 10 | 손실 1.12\n",
            "|에폭 12| 반복 10 / 10 | 손실 1.11\n",
            "|에폭 13| 반복 10 / 10 | 손실 1.09\n",
            "|에폭 14| 반복 10 / 10 | 손실 1.08\n",
            "|에폭 15| 반복 10 / 10 | 손실 1.04\n",
            "|에폭 16| 반복 10 / 10 | 손실 1.03\n",
            "|에폭 17| 반복 10 / 10 | 손실 0.96\n",
            "|에폭 18| 반복 10 / 10 | 손실 0.92\n",
            "|에폭 19| 반복 10 / 10 | 손실 0.92\n",
            "|에폭 20| 반복 10 / 10 | 손실 0.87\n",
            "|에폭 21| 반복 10 / 10 | 손실 0.85\n",
            "|에폭 22| 반복 10 / 10 | 손실 0.82\n",
            "|에폭 23| 반복 10 / 10 | 손실 0.79\n",
            "|에폭 24| 반복 10 / 10 | 손실 0.78\n",
            "|에폭 25| 반복 10 / 10 | 손실 0.82\n",
            "|에폭 26| 반복 10 / 10 | 손실 0.78\n",
            "|에폭 27| 반복 10 / 10 | 손실 0.76\n",
            "|에폭 28| 반복 10 / 10 | 손실 0.76\n",
            "|에폭 29| 반복 10 / 10 | 손실 0.78\n",
            "|에폭 30| 반복 10 / 10 | 손실 0.75\n",
            "|에폭 31| 반복 10 / 10 | 손실 0.78\n",
            "|에폭 32| 반복 10 / 10 | 손실 0.77\n",
            "|에폭 33| 반복 10 / 10 | 손실 0.77\n",
            "|에폭 34| 반복 10 / 10 | 손실 0.78\n",
            "|에폭 35| 반복 10 / 10 | 손실 0.75\n",
            "|에폭 36| 반복 10 / 10 | 손실 0.74\n",
            "|에폭 37| 반복 10 / 10 | 손실 0.76\n",
            "|에폭 38| 반복 10 / 10 | 손실 0.76\n",
            "|에폭 39| 반복 10 / 10 | 손실 0.73\n",
            "|에폭 40| 반복 10 / 10 | 손실 0.75\n",
            "|에폭 41| 반복 10 / 10 | 손실 0.76\n",
            "|에폭 42| 반복 10 / 10 | 손실 0.76\n",
            "|에폭 43| 반복 10 / 10 | 손실 0.76\n",
            "|에폭 44| 반복 10 / 10 | 손실 0.74\n",
            "|에폭 45| 반복 10 / 10 | 손실 0.75\n",
            "|에폭 46| 반복 10 / 10 | 손실 0.73\n",
            "|에폭 47| 반복 10 / 10 | 손실 0.72\n",
            "|에폭 48| 반복 10 / 10 | 손실 0.73\n",
            "|에폭 49| 반복 10 / 10 | 손실 0.72\n",
            "|에폭 50| 반복 10 / 10 | 손실 0.72\n",
            "|에폭 51| 반복 10 / 10 | 손실 0.72\n",
            "|에폭 52| 반복 10 / 10 | 손실 0.72\n",
            "|에폭 53| 반복 10 / 10 | 손실 0.74\n",
            "|에폭 54| 반복 10 / 10 | 손실 0.74\n",
            "|에폭 55| 반복 10 / 10 | 손실 0.72\n",
            "|에폭 56| 반복 10 / 10 | 손실 0.72\n",
            "|에폭 57| 반복 10 / 10 | 손실 0.71\n",
            "|에폭 58| 반복 10 / 10 | 손실 0.70\n",
            "|에폭 59| 반복 10 / 10 | 손실 0.72\n",
            "|에폭 60| 반복 10 / 10 | 손실 0.70\n",
            "|에폭 61| 반복 10 / 10 | 손실 0.71\n",
            "|에폭 62| 반복 10 / 10 | 손실 0.72\n",
            "|에폭 63| 반복 10 / 10 | 손실 0.70\n",
            "|에폭 64| 반복 10 / 10 | 손실 0.71\n",
            "|에폭 65| 반복 10 / 10 | 손실 0.73\n",
            "|에폭 66| 반복 10 / 10 | 손실 0.70\n",
            "|에폭 67| 반복 10 / 10 | 손실 0.71\n",
            "|에폭 68| 반복 10 / 10 | 손실 0.69\n",
            "|에폭 69| 반복 10 / 10 | 손실 0.70\n",
            "|에폭 70| 반복 10 / 10 | 손실 0.71\n",
            "|에폭 71| 반복 10 / 10 | 손실 0.68\n",
            "|에폭 72| 반복 10 / 10 | 손실 0.69\n",
            "|에폭 73| 반복 10 / 10 | 손실 0.67\n",
            "|에폭 74| 반복 10 / 10 | 손실 0.68\n",
            "|에폭 75| 반복 10 / 10 | 손실 0.67\n",
            "|에폭 76| 반복 10 / 10 | 손실 0.66\n",
            "|에폭 77| 반복 10 / 10 | 손실 0.69\n",
            "|에폭 78| 반복 10 / 10 | 손실 0.64\n",
            "|에폭 79| 반복 10 / 10 | 손실 0.68\n",
            "|에폭 80| 반복 10 / 10 | 손실 0.64\n",
            "|에폭 81| 반복 10 / 10 | 손실 0.64\n",
            "|에폭 82| 반복 10 / 10 | 손실 0.66\n",
            "|에폭 83| 반복 10 / 10 | 손실 0.62\n",
            "|에폭 84| 반복 10 / 10 | 손실 0.62\n",
            "|에폭 85| 반복 10 / 10 | 손실 0.61\n",
            "|에폭 86| 반복 10 / 10 | 손실 0.60\n",
            "|에폭 87| 반복 10 / 10 | 손실 0.60\n",
            "|에폭 88| 반복 10 / 10 | 손실 0.61\n",
            "|에폭 89| 반복 10 / 10 | 손실 0.59\n",
            "|에폭 90| 반복 10 / 10 | 손실 0.58\n",
            "|에폭 91| 반복 10 / 10 | 손실 0.56\n",
            "|에폭 92| 반복 10 / 10 | 손실 0.56\n",
            "|에폭 93| 반복 10 / 10 | 손실 0.54\n",
            "|에폭 94| 반복 10 / 10 | 손실 0.53\n",
            "|에폭 95| 반복 10 / 10 | 손실 0.53\n",
            "|에폭 96| 반복 10 / 10 | 손실 0.52\n",
            "|에폭 97| 반복 10 / 10 | 손실 0.51\n",
            "|에폭 98| 반복 10 / 10 | 손실 0.50\n",
            "|에폭 99| 반복 10 / 10 | 손실 0.48\n",
            "|에폭 100| 반복 10 / 10 | 손실 0.48\n",
            "|에폭 101| 반복 10 / 10 | 손실 0.46\n",
            "|에폭 102| 반복 10 / 10 | 손실 0.45\n",
            "|에폭 103| 반복 10 / 10 | 손실 0.45\n",
            "|에폭 104| 반복 10 / 10 | 손실 0.44\n",
            "|에폭 105| 반복 10 / 10 | 손실 0.44\n",
            "|에폭 106| 반복 10 / 10 | 손실 0.41\n",
            "|에폭 107| 반복 10 / 10 | 손실 0.40\n",
            "|에폭 108| 반복 10 / 10 | 손실 0.41\n",
            "|에폭 109| 반복 10 / 10 | 손실 0.40\n",
            "|에폭 110| 반복 10 / 10 | 손실 0.40\n",
            "|에폭 111| 반복 10 / 10 | 손실 0.38\n",
            "|에폭 112| 반복 10 / 10 | 손실 0.38\n",
            "|에폭 113| 반복 10 / 10 | 손실 0.36\n",
            "|에폭 114| 반복 10 / 10 | 손실 0.37\n",
            "|에폭 115| 반복 10 / 10 | 손실 0.35\n",
            "|에폭 116| 반복 10 / 10 | 손실 0.34\n",
            "|에폭 117| 반복 10 / 10 | 손실 0.34\n",
            "|에폭 118| 반복 10 / 10 | 손실 0.34\n",
            "|에폭 119| 반복 10 / 10 | 손실 0.33\n",
            "|에폭 120| 반복 10 / 10 | 손실 0.34\n",
            "|에폭 121| 반복 10 / 10 | 손실 0.32\n",
            "|에폭 122| 반복 10 / 10 | 손실 0.32\n",
            "|에폭 123| 반복 10 / 10 | 손실 0.31\n",
            "|에폭 124| 반복 10 / 10 | 손실 0.31\n",
            "|에폭 125| 반복 10 / 10 | 손실 0.30\n",
            "|에폭 126| 반복 10 / 10 | 손실 0.30\n",
            "|에폭 127| 반복 10 / 10 | 손실 0.28\n",
            "|에폭 128| 반복 10 / 10 | 손실 0.28\n",
            "|에폭 129| 반복 10 / 10 | 손실 0.28\n",
            "|에폭 130| 반복 10 / 10 | 손실 0.28\n",
            "|에폭 131| 반복 10 / 10 | 손실 0.27\n",
            "|에폭 132| 반복 10 / 10 | 손실 0.27\n",
            "|에폭 133| 반복 10 / 10 | 손실 0.27\n",
            "|에폭 134| 반복 10 / 10 | 손실 0.27\n",
            "|에폭 135| 반복 10 / 10 | 손실 0.27\n",
            "|에폭 136| 반복 10 / 10 | 손실 0.26\n",
            "|에폭 137| 반복 10 / 10 | 손실 0.26\n",
            "|에폭 138| 반복 10 / 10 | 손실 0.26\n",
            "|에폭 139| 반복 10 / 10 | 손실 0.25\n",
            "|에폭 140| 반복 10 / 10 | 손실 0.24\n",
            "|에폭 141| 반복 10 / 10 | 손실 0.24\n",
            "|에폭 142| 반복 10 / 10 | 손실 0.25\n",
            "|에폭 143| 반복 10 / 10 | 손실 0.24\n",
            "|에폭 144| 반복 10 / 10 | 손실 0.24\n",
            "|에폭 145| 반복 10 / 10 | 손실 0.23\n",
            "|에폭 146| 반복 10 / 10 | 손실 0.24\n",
            "|에폭 147| 반복 10 / 10 | 손실 0.23\n",
            "|에폭 148| 반복 10 / 10 | 손실 0.23\n",
            "|에폭 149| 반복 10 / 10 | 손실 0.22\n",
            "|에폭 150| 반복 10 / 10 | 손실 0.22\n",
            "|에폭 151| 반복 10 / 10 | 손실 0.22\n",
            "|에폭 152| 반복 10 / 10 | 손실 0.22\n",
            "|에폭 153| 반복 10 / 10 | 손실 0.22\n",
            "|에폭 154| 반복 10 / 10 | 손실 0.22\n",
            "|에폭 155| 반복 10 / 10 | 손실 0.22\n",
            "|에폭 156| 반복 10 / 10 | 손실 0.21\n",
            "|에폭 157| 반복 10 / 10 | 손실 0.21\n",
            "|에폭 158| 반복 10 / 10 | 손실 0.20\n",
            "|에폭 159| 반복 10 / 10 | 손실 0.21\n",
            "|에폭 160| 반복 10 / 10 | 손실 0.20\n",
            "|에폭 161| 반복 10 / 10 | 손실 0.20\n",
            "|에폭 162| 반복 10 / 10 | 손실 0.20\n",
            "|에폭 163| 반복 10 / 10 | 손실 0.21\n",
            "|에폭 164| 반복 10 / 10 | 손실 0.20\n",
            "|에폭 165| 반복 10 / 10 | 손실 0.20\n",
            "|에폭 166| 반복 10 / 10 | 손실 0.19\n",
            "|에폭 167| 반복 10 / 10 | 손실 0.19\n",
            "|에폭 168| 반복 10 / 10 | 손실 0.19\n",
            "|에폭 169| 반복 10 / 10 | 손실 0.19\n",
            "|에폭 170| 반복 10 / 10 | 손실 0.19\n",
            "|에폭 171| 반복 10 / 10 | 손실 0.19\n",
            "|에폭 172| 반복 10 / 10 | 손실 0.18\n",
            "|에폭 173| 반복 10 / 10 | 손실 0.18\n",
            "|에폭 174| 반복 10 / 10 | 손실 0.18\n",
            "|에폭 175| 반복 10 / 10 | 손실 0.18\n",
            "|에폭 176| 반복 10 / 10 | 손실 0.18\n",
            "|에폭 177| 반복 10 / 10 | 손실 0.18\n",
            "|에폭 178| 반복 10 / 10 | 손실 0.18\n",
            "|에폭 179| 반복 10 / 10 | 손실 0.17\n",
            "|에폭 180| 반복 10 / 10 | 손실 0.17\n",
            "|에폭 181| 반복 10 / 10 | 손실 0.18\n",
            "|에폭 182| 반복 10 / 10 | 손실 0.17\n",
            "|에폭 183| 반복 10 / 10 | 손실 0.18\n",
            "|에폭 184| 반복 10 / 10 | 손실 0.17\n",
            "|에폭 185| 반복 10 / 10 | 손실 0.17\n",
            "|에폭 186| 반복 10 / 10 | 손실 0.18\n",
            "|에폭 187| 반복 10 / 10 | 손실 0.17\n",
            "|에폭 188| 반복 10 / 10 | 손실 0.17\n",
            "|에폭 189| 반복 10 / 10 | 손실 0.17\n",
            "|에폭 190| 반복 10 / 10 | 손실 0.17\n",
            "|에폭 191| 반복 10 / 10 | 손실 0.16\n",
            "|에폭 192| 반복 10 / 10 | 손실 0.17\n",
            "|에폭 193| 반복 10 / 10 | 손실 0.16\n",
            "|에폭 194| 반복 10 / 10 | 손실 0.16\n",
            "|에폭 195| 반복 10 / 10 | 손실 0.16\n",
            "|에폭 196| 반복 10 / 10 | 손실 0.16\n",
            "|에폭 197| 반복 10 / 10 | 손실 0.16\n",
            "|에폭 198| 반복 10 / 10 | 손실 0.15\n",
            "|에폭 199| 반복 10 / 10 | 손실 0.16\n",
            "|에폭 200| 반복 10 / 10 | 손실 0.16\n",
            "|에폭 201| 반복 10 / 10 | 손실 0.15\n",
            "|에폭 202| 반복 10 / 10 | 손실 0.16\n",
            "|에폭 203| 반복 10 / 10 | 손실 0.16\n",
            "|에폭 204| 반복 10 / 10 | 손실 0.15\n",
            "|에폭 205| 반복 10 / 10 | 손실 0.16\n",
            "|에폭 206| 반복 10 / 10 | 손실 0.15\n",
            "|에폭 207| 반복 10 / 10 | 손실 0.15\n",
            "|에폭 208| 반복 10 / 10 | 손실 0.15\n",
            "|에폭 209| 반복 10 / 10 | 손실 0.15\n",
            "|에폭 210| 반복 10 / 10 | 손실 0.15\n",
            "|에폭 211| 반복 10 / 10 | 손실 0.15\n",
            "|에폭 212| 반복 10 / 10 | 손실 0.15\n",
            "|에폭 213| 반복 10 / 10 | 손실 0.15\n",
            "|에폭 214| 반복 10 / 10 | 손실 0.15\n",
            "|에폭 215| 반복 10 / 10 | 손실 0.15\n",
            "|에폭 216| 반복 10 / 10 | 손실 0.14\n",
            "|에폭 217| 반복 10 / 10 | 손실 0.14\n",
            "|에폭 218| 반복 10 / 10 | 손실 0.15\n",
            "|에폭 219| 반복 10 / 10 | 손실 0.14\n",
            "|에폭 220| 반복 10 / 10 | 손실 0.14\n",
            "|에폭 221| 반복 10 / 10 | 손실 0.14\n",
            "|에폭 222| 반복 10 / 10 | 손실 0.14\n",
            "|에폭 223| 반복 10 / 10 | 손실 0.14\n",
            "|에폭 224| 반복 10 / 10 | 손실 0.14\n",
            "|에폭 225| 반복 10 / 10 | 손실 0.14\n",
            "|에폭 226| 반복 10 / 10 | 손실 0.14\n",
            "|에폭 227| 반복 10 / 10 | 손실 0.14\n",
            "|에폭 228| 반복 10 / 10 | 손실 0.14\n",
            "|에폭 229| 반복 10 / 10 | 손실 0.13\n",
            "|에폭 230| 반복 10 / 10 | 손실 0.14\n",
            "|에폭 231| 반복 10 / 10 | 손실 0.13\n",
            "|에폭 232| 반복 10 / 10 | 손실 0.14\n",
            "|에폭 233| 반복 10 / 10 | 손실 0.13\n",
            "|에폭 234| 반복 10 / 10 | 손실 0.13\n",
            "|에폭 235| 반복 10 / 10 | 손실 0.13\n",
            "|에폭 236| 반복 10 / 10 | 손실 0.13\n",
            "|에폭 237| 반복 10 / 10 | 손실 0.14\n",
            "|에폭 238| 반복 10 / 10 | 손실 0.13\n",
            "|에폭 239| 반복 10 / 10 | 손실 0.13\n",
            "|에폭 240| 반복 10 / 10 | 손실 0.14\n",
            "|에폭 241| 반복 10 / 10 | 손실 0.13\n",
            "|에폭 242| 반복 10 / 10 | 손실 0.13\n",
            "|에폭 243| 반복 10 / 10 | 손실 0.13\n",
            "|에폭 244| 반복 10 / 10 | 손실 0.13\n",
            "|에폭 245| 반복 10 / 10 | 손실 0.13\n",
            "|에폭 246| 반복 10 / 10 | 손실 0.13\n",
            "|에폭 247| 반복 10 / 10 | 손실 0.13\n",
            "|에폭 248| 반복 10 / 10 | 손실 0.13\n",
            "|에폭 249| 반복 10 / 10 | 손실 0.13\n",
            "|에폭 250| 반복 10 / 10 | 손실 0.13\n",
            "|에폭 251| 반복 10 / 10 | 손실 0.13\n",
            "|에폭 252| 반복 10 / 10 | 손실 0.12\n",
            "|에폭 253| 반복 10 / 10 | 손실 0.12\n",
            "|에폭 254| 반복 10 / 10 | 손실 0.12\n",
            "|에폭 255| 반복 10 / 10 | 손실 0.12\n",
            "|에폭 256| 반복 10 / 10 | 손실 0.12\n",
            "|에폭 257| 반복 10 / 10 | 손실 0.12\n",
            "|에폭 258| 반복 10 / 10 | 손실 0.12\n",
            "|에폭 259| 반복 10 / 10 | 손실 0.13\n",
            "|에폭 260| 반복 10 / 10 | 손실 0.12\n",
            "|에폭 261| 반복 10 / 10 | 손실 0.13\n",
            "|에폭 262| 반복 10 / 10 | 손실 0.12\n",
            "|에폭 263| 반복 10 / 10 | 손실 0.12\n",
            "|에폭 264| 반복 10 / 10 | 손실 0.13\n",
            "|에폭 265| 반복 10 / 10 | 손실 0.12\n",
            "|에폭 266| 반복 10 / 10 | 손실 0.12\n",
            "|에폭 267| 반복 10 / 10 | 손실 0.12\n",
            "|에폭 268| 반복 10 / 10 | 손실 0.12\n",
            "|에폭 269| 반복 10 / 10 | 손실 0.11\n",
            "|에폭 270| 반복 10 / 10 | 손실 0.12\n",
            "|에폭 271| 반복 10 / 10 | 손실 0.12\n",
            "|에폭 272| 반복 10 / 10 | 손실 0.12\n",
            "|에폭 273| 반복 10 / 10 | 손실 0.12\n",
            "|에폭 274| 반복 10 / 10 | 손실 0.12\n",
            "|에폭 275| 반복 10 / 10 | 손실 0.11\n",
            "|에폭 276| 반복 10 / 10 | 손실 0.12\n",
            "|에폭 277| 반복 10 / 10 | 손실 0.12\n",
            "|에폭 278| 반복 10 / 10 | 손실 0.11\n",
            "|에폭 279| 반복 10 / 10 | 손실 0.11\n",
            "|에폭 280| 반복 10 / 10 | 손실 0.11\n",
            "|에폭 281| 반복 10 / 10 | 손실 0.11\n",
            "|에폭 282| 반복 10 / 10 | 손실 0.12\n",
            "|에폭 283| 반복 10 / 10 | 손실 0.11\n",
            "|에폭 284| 반복 10 / 10 | 손실 0.11\n",
            "|에폭 285| 반복 10 / 10 | 손실 0.11\n",
            "|에폭 286| 반복 10 / 10 | 손실 0.11\n",
            "|에폭 287| 반복 10 / 10 | 손실 0.11\n",
            "|에폭 288| 반복 10 / 10 | 손실 0.12\n",
            "|에폭 289| 반복 10 / 10 | 손실 0.11\n",
            "|에폭 290| 반복 10 / 10 | 손실 0.11\n",
            "|에폭 291| 반복 10 / 10 | 손실 0.11\n",
            "|에폭 292| 반복 10 / 10 | 손실 0.11\n",
            "|에폭 293| 반복 10 / 10 | 손실 0.11\n",
            "|에폭 294| 반복 10 / 10 | 손실 0.11\n",
            "|에폭 295| 반복 10 / 10 | 손실 0.12\n",
            "|에폭 296| 반복 10 / 10 | 손실 0.11\n",
            "|에폭 297| 반복 10 / 10 | 손실 0.12\n",
            "|에폭 298| 반복 10 / 10 | 손실 0.11\n",
            "|에폭 299| 반복 10 / 10 | 손실 0.11\n",
            "|에폭 300| 반복 10 / 10 | 손실 0.11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48373 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49552 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49892 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48373 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49552 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49892 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtbT1DzpAUbQ",
        "outputId": "59218ed4-d775-47da-97b4-427f7a8cafac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "# 경계 영역을 보면 나선형 패턴을 올바르게 파악했음을 알 수 있음\n",
        "\n",
        "# 경계 영역 플롯\n",
        "h = 0.001\n",
        "x_min, x_max = x[:, 0].min() - .1, x[:, 0].max() + .1\n",
        "y_min, y_max = x[:, 1].min() - .1, x[:, 1].max() + .1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "X = np.c_[xx.ravel(), yy.ravel()]\n",
        "score = model.predict(X) \n",
        "predict_cls = np.argmax(score, axis=1)\n",
        "Z = predict_cls.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z)\n",
        "plt.axis('off')\n",
        "\n",
        "# 데이터점 플롯\n",
        "x, t = spiral.load_data()\n",
        "N = 100\n",
        "CLS_NUM = 3\n",
        "markers = ['o', 'x', '^']\n",
        "for i in range(CLS_NUM):\n",
        "    plt.scatter(x[i*N:(i+1)*N, 0], x[i*N:(i+1)*N, 1], s=40, marker=markers[i])\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhT5gUxDIzM0",
        "outputId": "e27dfeb6-1737-4aae-f949-a15b8e0fb1e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master\")\n",
        "from common.optimizer import SGD\n",
        "from common.trainer import Trainer\n",
        "from dataset import spiral\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "max_epoch = 300\n",
        "batch_size = 30\n",
        "hidden_size = 10\n",
        "learning_rate = 1.0\n",
        "x, t = spiral.load_data()\n",
        "model = TwoLayerNet(input_size = 2, hidden_size = hidden_size, output_size = 3)\n",
        "optimizer = SGD(lr=learning_rate)\n",
        "\n",
        "# 앞에 했던 내용을 간단하게 Trainer 라는 클래스로 묶었을 뿐임.\n",
        "trainer = Trainer(model, optimizer)\n",
        "trainer.fit(x, t, max_epoch, batch_size, eval_interval = 10)\n",
        "trainer.plot()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| 에폭 1 |  반복 1 / 10 | 시간 0[s] | 손실 1.10\n",
            "| 에폭 2 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 3 |  반복 1 / 10 | 시간 0[s] | 손실 1.13\n",
            "| 에폭 4 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 5 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 6 |  반복 1 / 10 | 시간 0[s] | 손실 1.10\n",
            "| 에폭 7 |  반복 1 / 10 | 시간 0[s] | 손실 1.14\n",
            "| 에폭 8 |  반복 1 / 10 | 시간 0[s] | 손실 1.16\n",
            "| 에폭 9 |  반복 1 / 10 | 시간 0[s] | 손실 1.11\n",
            "| 에폭 10 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 11 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 12 |  반복 1 / 10 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 13 |  반복 1 / 10 | 시간 0[s] | 손실 1.10\n",
            "| 에폭 14 |  반복 1 / 10 | 시간 0[s] | 손실 1.09\n",
            "| 에폭 15 |  반복 1 / 10 | 시간 0[s] | 손실 1.08\n",
            "| 에폭 16 |  반복 1 / 10 | 시간 0[s] | 손실 1.04\n",
            "| 에폭 17 |  반복 1 / 10 | 시간 0[s] | 손실 1.03\n",
            "| 에폭 18 |  반복 1 / 10 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 19 |  반복 1 / 10 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 20 |  반복 1 / 10 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 21 |  반복 1 / 10 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 22 |  반복 1 / 10 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 23 |  반복 1 / 10 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 24 |  반복 1 / 10 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 25 |  반복 1 / 10 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 26 |  반복 1 / 10 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 27 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 28 |  반복 1 / 10 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 29 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 30 |  반복 1 / 10 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 31 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 32 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 33 |  반복 1 / 10 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 34 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 35 |  반복 1 / 10 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 36 |  반복 1 / 10 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 37 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 38 |  반복 1 / 10 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 39 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 40 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 41 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 42 |  반복 1 / 10 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 43 |  반복 1 / 10 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 44 |  반복 1 / 10 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 45 |  반복 1 / 10 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 46 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 47 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 48 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 49 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 50 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 51 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 52 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 53 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 54 |  반복 1 / 10 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 55 |  반복 1 / 10 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 56 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 57 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 58 |  반복 1 / 10 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 59 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 60 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 61 |  반복 1 / 10 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 62 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 63 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 64 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 65 |  반복 1 / 10 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 66 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 67 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 68 |  반복 1 / 10 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 69 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 70 |  반복 1 / 10 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 71 |  반복 1 / 10 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 72 |  반복 1 / 10 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 73 |  반복 1 / 10 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 74 |  반복 1 / 10 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 75 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 76 |  반복 1 / 10 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 77 |  반복 1 / 10 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 78 |  반복 1 / 10 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 79 |  반복 1 / 10 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 80 |  반복 1 / 10 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 81 |  반복 1 / 10 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 82 |  반복 1 / 10 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 83 |  반복 1 / 10 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 84 |  반복 1 / 10 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 85 |  반복 1 / 10 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 86 |  반복 1 / 10 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 87 |  반복 1 / 10 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 88 |  반복 1 / 10 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 89 |  반복 1 / 10 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 90 |  반복 1 / 10 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 91 |  반복 1 / 10 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 92 |  반복 1 / 10 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 93 |  반복 1 / 10 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 94 |  반복 1 / 10 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 95 |  반복 1 / 10 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 96 |  반복 1 / 10 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 97 |  반복 1 / 10 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 98 |  반복 1 / 10 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 99 |  반복 1 / 10 | 시간 0[s] | 손실 0.50\n",
            "| 에폭 100 |  반복 1 / 10 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 101 |  반복 1 / 10 | 시간 0[s] | 손실 0.49\n",
            "| 에폭 102 |  반복 1 / 10 | 시간 0[s] | 손실 0.46\n",
            "| 에폭 103 |  반복 1 / 10 | 시간 0[s] | 손실 0.44\n",
            "| 에폭 104 |  반복 1 / 10 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 105 |  반복 1 / 10 | 시간 0[s] | 손실 0.44\n",
            "| 에폭 106 |  반복 1 / 10 | 시간 0[s] | 손실 0.43\n",
            "| 에폭 107 |  반복 1 / 10 | 시간 0[s] | 손실 0.43\n",
            "| 에폭 108 |  반복 1 / 10 | 시간 0[s] | 손실 0.39\n",
            "| 에폭 109 |  반복 1 / 10 | 시간 0[s] | 손실 0.40\n",
            "| 에폭 110 |  반복 1 / 10 | 시간 0[s] | 손실 0.41\n",
            "| 에폭 111 |  반복 1 / 10 | 시간 0[s] | 손실 0.38\n",
            "| 에폭 112 |  반복 1 / 10 | 시간 0[s] | 손실 0.38\n",
            "| 에폭 113 |  반복 1 / 10 | 시간 0[s] | 손실 0.38\n",
            "| 에폭 114 |  반복 1 / 10 | 시간 0[s] | 손실 0.37\n",
            "| 에폭 115 |  반복 1 / 10 | 시간 0[s] | 손실 0.36\n",
            "| 에폭 116 |  반복 1 / 10 | 시간 0[s] | 손실 0.34\n",
            "| 에폭 117 |  반복 1 / 10 | 시간 0[s] | 손실 0.35\n",
            "| 에폭 118 |  반복 1 / 10 | 시간 0[s] | 손실 0.33\n",
            "| 에폭 119 |  반복 1 / 10 | 시간 0[s] | 손실 0.35\n",
            "| 에폭 120 |  반복 1 / 10 | 시간 0[s] | 손실 0.33\n",
            "| 에폭 121 |  반복 1 / 10 | 시간 0[s] | 손실 0.33\n",
            "| 에폭 122 |  반복 1 / 10 | 시간 0[s] | 손실 0.32\n",
            "| 에폭 123 |  반복 1 / 10 | 시간 0[s] | 손실 0.31\n",
            "| 에폭 124 |  반복 1 / 10 | 시간 0[s] | 손실 0.31\n",
            "| 에폭 125 |  반복 1 / 10 | 시간 0[s] | 손실 0.31\n",
            "| 에폭 126 |  반복 1 / 10 | 시간 0[s] | 손실 0.30\n",
            "| 에폭 127 |  반복 1 / 10 | 시간 0[s] | 손실 0.30\n",
            "| 에폭 128 |  반복 1 / 10 | 시간 0[s] | 손실 0.27\n",
            "| 에폭 129 |  반복 1 / 10 | 시간 0[s] | 손실 0.30\n",
            "| 에폭 130 |  반복 1 / 10 | 시간 0[s] | 손실 0.28\n",
            "| 에폭 131 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
            "| 에폭 132 |  반복 1 / 10 | 시간 0[s] | 손실 0.27\n",
            "| 에폭 133 |  반복 1 / 10 | 시간 0[s] | 손실 0.27\n",
            "| 에폭 134 |  반복 1 / 10 | 시간 0[s] | 손실 0.28\n",
            "| 에폭 135 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
            "| 에폭 136 |  반복 1 / 10 | 시간 0[s] | 손실 0.28\n",
            "| 에폭 137 |  반복 1 / 10 | 시간 0[s] | 손실 0.25\n",
            "| 에폭 138 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
            "| 에폭 139 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
            "| 에폭 140 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
            "| 에폭 141 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
            "| 에폭 142 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
            "| 에폭 143 |  반복 1 / 10 | 시간 0[s] | 손실 0.26\n",
            "| 에폭 144 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
            "| 에폭 145 |  반복 1 / 10 | 시간 0[s] | 손실 0.24\n",
            "| 에폭 146 |  반복 1 / 10 | 시간 0[s] | 손실 0.24\n",
            "| 에폭 147 |  반복 1 / 10 | 시간 0[s] | 손실 0.25\n",
            "| 에폭 148 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
            "| 에폭 149 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
            "| 에폭 150 |  반복 1 / 10 | 시간 0[s] | 손실 0.22\n",
            "| 에폭 151 |  반복 1 / 10 | 시간 0[s] | 손실 0.22\n",
            "| 에폭 152 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
            "| 에폭 153 |  반복 1 / 10 | 시간 0[s] | 손실 0.23\n",
            "| 에폭 154 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
            "| 에폭 155 |  반복 1 / 10 | 시간 0[s] | 손실 0.22\n",
            "| 에폭 156 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
            "| 에폭 157 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
            "| 에폭 158 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
            "| 에폭 159 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
            "| 에폭 160 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
            "| 에폭 161 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
            "| 에폭 162 |  반복 1 / 10 | 시간 0[s] | 손실 0.22\n",
            "| 에폭 163 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
            "| 에폭 164 |  반복 1 / 10 | 시간 0[s] | 손실 0.21\n",
            "| 에폭 165 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
            "| 에폭 166 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
            "| 에폭 167 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
            "| 에폭 168 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
            "| 에폭 169 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
            "| 에폭 170 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
            "| 에폭 171 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
            "| 에폭 172 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
            "| 에폭 173 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 174 |  반복 1 / 10 | 시간 0[s] | 손실 0.20\n",
            "| 에폭 175 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
            "| 에폭 176 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 177 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 178 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 179 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
            "| 에폭 180 |  반복 1 / 10 | 시간 0[s] | 손실 0.19\n",
            "| 에폭 181 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 182 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
            "| 에폭 183 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 184 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
            "| 에폭 185 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
            "| 에폭 186 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 187 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 188 |  반복 1 / 10 | 시간 0[s] | 손실 0.18\n",
            "| 에폭 189 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 190 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 191 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 192 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 193 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 194 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 195 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 196 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 197 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 198 |  반복 1 / 10 | 시간 0[s] | 손실 0.17\n",
            "| 에폭 199 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 200 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 201 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 202 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 203 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 204 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 205 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 206 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 207 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 208 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 209 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 210 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 211 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 212 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 213 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 214 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 215 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 216 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 217 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 218 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 219 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 220 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 221 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 222 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 223 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 224 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 225 |  반복 1 / 10 | 시간 0[s] | 손실 0.16\n",
            "| 에폭 226 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 227 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 228 |  반복 1 / 10 | 시간 0[s] | 손실 0.15\n",
            "| 에폭 229 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 230 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 231 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 232 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 233 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 234 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 235 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 236 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 237 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 238 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 239 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 240 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 241 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 242 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 243 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 244 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 245 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 246 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 247 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 248 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 249 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 250 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 251 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 252 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 253 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 254 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 255 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 256 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 257 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 258 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 259 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 260 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 261 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 262 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 263 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 264 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 265 |  반복 1 / 10 | 시간 0[s] | 손실 0.14\n",
            "| 에폭 266 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 267 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 268 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 269 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 270 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 271 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 272 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 273 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 274 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 275 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 276 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 277 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 278 |  반복 1 / 10 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 279 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 280 |  반복 1 / 10 | 시간 0[s] | 손실 0.10\n",
            "| 에폭 281 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 282 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 283 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 284 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 285 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 286 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 287 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 288 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 289 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 290 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 291 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 292 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 293 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 294 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 295 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 296 |  반복 1 / 10 | 시간 0[s] | 손실 0.12\n",
            "| 에폭 297 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 298 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 299 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 300 |  반복 1 / 10 | 시간 0[s] | 손실 0.11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48373 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49552 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49892 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48373 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49552 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49892 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rILak7IH9vv",
        "outputId": "775e0bd1-e5e2-4201-a47c-faecccbc9aa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import numpy as np\n",
        "a = np.random.randn(3)\n",
        "print(a.dtype)\n",
        "# 넘파이의 부동소수점 수는 기본적으로 64비트 데이터 타입을 사용함\n",
        "# 그러나 신경망의 추론과 학습은 32비트에서도 문제없이 잘 작동하기 때문에\n",
        "# 32비트로 떨어뜨려주는게 계산속도를 증가시키는데에 용이함.\n",
        "b = np.random.randn(3).astype(np.float32)\n",
        "print(b.dtype)\n",
        "c = np.random.randn(3).astype('f')\n",
        "print(c.dtype)\n",
        "# np.float32 나 f로 지정하면 32비트로 변경\n",
        "# 신경망 추론으로 한정하면 16비트를 사용해도 인식률이 거의 떨어지지 않음.\n",
        "# 다만 일반적으로 CPU와 GPU는 연산자체를 32비트로 수행하기 때문에 \n",
        "# 처리 속도 측면에서는 혜택이 없을 수도 있음.\n",
        "# 그러나 학습된 가중치를 저장할 때는 16비트 부동소수점 수가 여전히 유효함. (데이터가 절반이므로)\n",
        "# 그래서 이 책에서는 학습된 가중치를 저장하는 경우에 한해 16비트 부동소수점 수로 변환함.\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "float64\n",
            "float32\n",
            "float32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrJx3pqNLqSf",
        "outputId": "a8e993a1-4d00-44d7-dc3b-9cc7f6a2e130",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "'''\n",
        "import cupy as cp\n",
        "x = cp.arange(6).reshape(2, 3).astype('f')\n",
        "print(x)\n",
        "'''\n",
        "# 대량의 곱하기 연산은 병렬로 계산되는데 이 점에서는\n",
        "# CPU 보다 GPU가 훨씬 유리함. 그래서 GPU 사용에 대해 설명\n",
        "# 쿠파이는 GPU를 이용해 병렬 계산을 수행해 주는 라이브러리임.\n",
        "# 쿠파이의 사용방법은 기본적으로 넘파이와 같음.\n",
        "# 사용법은 같지만 뒤에서 열심히 GPU를 사용해 계산하는 것.\n",
        "# 보통은 numpy를 cupy로 대체해주기만 하면 끝"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nimport cupy as cp\\nx = cp.arange(6).reshape(2, 3).astype('f')\\nprint(x)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goYE__qvQVft"
      },
      "source": [
        "### 2장 - 자연어와 단어의 분산 표현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF_qxoMYOey3",
        "outputId": "96194c46-485f-4ec9-ab7f-9fd500a78399",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "text = 'You say goodbye and I say hello.'\n",
        "text = text.lower() # 모든 문자를 소문자로 변환\n",
        "text = text.replace('.', ' .')  # (.) 을  ( .) 로 변환\n",
        "text\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'you say goodbye and i say hello .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9KrxrCBxYD8",
        "outputId": "83d5da2c-4f76-4b85-817f-110393c56971",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "words = text.split(' ') # 공백을 기준으로 분할\n",
        "words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwoY9gXHxcmQ",
        "outputId": "02ff8c33-5d48-4cf0-e369-4acb10acee4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "word_to_id = {}\n",
        "id_to_word = {}\n",
        "\n",
        "for word in words:\n",
        "  if word not in word_to_id:\n",
        "    new_id = len(word_to_id)\n",
        "    word_to_id[word] = new_id\n",
        "    id_to_word[new_id] = word\n",
        "\n",
        "print(id_to_word)\n",
        "print(word_to_id)\n",
        "\n",
        "print(id_to_word[1])\n",
        "print(word_to_id['hello'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
            "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
            "say\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lopjwd4NyRJy",
        "outputId": "33188fcb-fb0b-4a53-e0b0-4386a7342e56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "corpus = [word_to_id[w] for w in words]\n",
        "corpus = np.array(corpus)\n",
        "corpus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 1, 5, 6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zan1UjN7zzf6"
      },
      "source": [
        "import numpy as np\n",
        "def preprocess(text):\n",
        "  text = text.lower()\n",
        "  text = text.replace('.', ' .')\n",
        "  words = text.split(' ')\n",
        "  word_to_id = {}\n",
        "  id_to_word = {}\n",
        "  for word in words:\n",
        "    if word not in word_to_id:\n",
        "      new_id = len(word_to_id)\n",
        "      word_to_id[word] = new_id\n",
        "      id_to_word[new_id] = word\n",
        "\n",
        "  corpus = np.array([word_to_id[w] for w in words])\n",
        "\n",
        "\n",
        "  return corpus, word_to_id, id_to_word "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf_k9ANU0QLU"
      },
      "source": [
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65qXnN2Q02pg",
        "outputId": "7578ea06-3683-4293-9fe8-384f2a0558d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.pardir)\n",
        "import numpy as np\n",
        "from common.util import preprocess\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "\n",
        "print(corpus)\n",
        "\n",
        "print(id_to_word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3 4 1 5 6]\n",
            "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDEF2xno1V1B"
      },
      "source": [
        "def create_co_matrix(corpus, vocab_size, window_size = 1):\n",
        "  corpus_size = len(corpus)\n",
        "  co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
        "\n",
        "  for idx, word_id in enumerate(corpus): # 순서가 있는 자료형에서 index 번호와 index 값을 반환함.    \n",
        "  # 이 경우에서 corpus는 [0 1 2 3 4 1 5 6] 이므로 idx는 [0 1 2 3 4 5 6 7] word_id는 [0 1 2 3 4 1 5 6] 임.\n",
        "    for i in range(1, window_size + 1):\n",
        "      left_idx = idx - i\n",
        "      right_idx = idx + i\n",
        "      # 제일 왼쪽 인덱스랑 오른쪽 인덱스의 왼쪽/ 오른쪽은 word의 len를 벗어나니까 그걸 방지하기 위해서 밑에 코드가 있음.\n",
        "\n",
        "      if left_idx >= 0 :  \n",
        "        left_word_id = corpus[left_idx]\n",
        "        co_matrix[word_id, left_word_id] += 1\n",
        "\n",
        "      if right_idx < corpus_size:\n",
        "        right_word_id = corpus[right_idx] # 여기서 corpus[] 는 value 값임.\n",
        "        co_matrix[word_id, right_word_id] += 1\n",
        "  return co_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-psULK73AI8"
      },
      "source": [
        "'''\n",
        "def cos_similarity(x, y):\n",
        "  nx = x / np.sqrt(np.sum(x**2)) # x의 정규화\n",
        "  ny = y / np.sqrt(np.sum(y**2)) # y의 정규화\n",
        "  return np.dot(nx, ny)\n",
        "'''\n",
        " # 위에서는 x나 y가 0일 때 분모에 문제가 생기므로 분모에 아주 작은 값을 더해주면 효율적인 계산이 가능함\n",
        "def cos_similarity(x, y, eps=1e-8):\n",
        "  nx = x / np.sqrt(np.sum(x**2) + eps) \n",
        "  ny = y / np.sqrt(np.sum(y**2) + eps) \n",
        "  return np.dot(nx, ny)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0nB9UuX7-4P",
        "outputId": "4d436b5e-eb02-4a24-9c6e-dcb2ee920654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.pardir)\n",
        "from common.util import preprocess, create_co_matrix, cos_similarity\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "vocab_size = len(word_to_id)\n",
        "C = create_co_matrix(corpus, vocab_size)\n",
        "\n",
        "c0 = C[word_to_id['you']] # \"you\" 의 단어 벡터\n",
        "c1 = C[word_to_id['i']] # \"i\" 의  단어 벡터\n",
        "print(cos_similarity(c0, c1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7071067691154799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FImnDivB-qyZ"
      },
      "source": [
        "def most_similar(query, word_to_id, id_to_word, word_matrix, top = 5):\n",
        "  # 검색어를 꺼낸다.\n",
        "  if query not in word_to_id:\n",
        "    print('%s(을)를 찾을 수 없습니다.' %query)\n",
        "    return\n",
        "  \n",
        "  print('\\n[query]' + query)\n",
        "  query_id = word_to_id[query]\n",
        "  query_vec = word_matrix[query_id]\n",
        "\n",
        "  # 코사인 유사도 계산\n",
        "  vocab_size = len(id_to_word)\n",
        "  similarity = np.zeros(vocab_size)\n",
        "  \n",
        "  for i in range(vocab_size):\n",
        "    similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
        "\n",
        "  # 코사인 유사도를 기준으로 내림차순으로 출력\n",
        "  count = 0\n",
        "  for i in ( -1 * similarity).argsort(): # argsort의 의미는 배열의 원소를 오름차순으로 정렬하는 것. 그런데 여기서는 similarity 에 -1을 곱했으니 내림차순으로 바뀜.\n",
        "    if id_to_word[i] == query:\n",
        "      continue\n",
        "    print(' %s: %s' %(id_to_word[i], similarity[i]))\n",
        "\n",
        "    count += 1\n",
        "    if count >= top:\n",
        "      return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0FkvwszGruk",
        "outputId": "d8665969-fc2f-4e31-c8f0-3c2ef47c7a5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "C = create_co_matrix(corpus, vocab_size)\n",
        "most_similar('you', word_to_id, id_to_word, C)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[query]you\n",
            " goodbye: 0.7071067691154799\n",
            " i: 0.7071067691154799\n",
            " hello: 0.7071067691154799\n",
            " say: 0.0\n",
            " and: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDD55JTMKSL7",
        "outputId": "c2a66641-144a-4b40-ecc1-a702ec0237ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# argsort()의 의미를 잘 생각해 볼 것.\n",
        "x = np.array([100, -20, 2]) \n",
        "print(x.argsort()) # 오름차순 정렬\n",
        "print((-x).argsort()) # 각 원소에 -1을 곱했으니 내림차순 정렬이 됨."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 0]\n",
            "[0 2 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIAI03HtKvnE",
        "outputId": "16606b27-90de-4704-f27d-d43a528191a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.pardir)\n",
        "\n",
        "from common.util import preprocess, create_co_matrix, most_similar\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "vocab_size = len(word_to_id) \n",
        "# word to id {'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6} \n",
        "# 단어들을 어떤 value로 표현해준다고 생각하면 쉬움.\n",
        "C = create_co_matrix(corpus, vocab_size)\n",
        "\n",
        "most_similar('you', word_to_id, id_to_word, C, top=5)\n",
        "\n",
        "# 위의 코드는 기존 적어놨던 코드를 이용해서 구현했음."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[query] you\n",
            " goodbye: 0.7071067691154799\n",
            " i: 0.7071067691154799\n",
            " hello: 0.7071067691154799\n",
            " say: 0.0\n",
            " and: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kn6gEfcLqK0"
      },
      "source": [
        "# 상호정보량\n",
        "# x,y의 관련성에 대해 설명하는 내용인데 얼마나 많이 등장하느냐에 대한 확률로 표현함.\n",
        "# 이것이 왜 중요하냐면, 예컨대 the, car, drive 사이의 양의 상호정보량에 대해 생각을 해보자.\n",
        "# 분명히 car 앞에는 the가 많이 붙어있을 것임. 그러나 car는 the와 별로 관련이 없는 단어이고 오히려 drive와 관련이 큼.\n",
        "# 예로, the가 1000번, car가 20번, drive가 10번 등장하는 말뭉치가 있다고 가정해봄.\n",
        "# 이 때, the와 car가 동시에 등장하는 횟수는 10 , car와 drive가 동시에 등장하는 횟수는 5라고 생각해보자.\n",
        "# 이러면 상호정보량은 당연히 car와 drive 사이에서의 값이 더 높음. \n",
        "# 왜냐면 the는 car가 아니어도 많이 나오기 때문. 즉 독립적으로 나오는 횟수를 고려하기 때문임.\n",
        "# 근데 여기서, log 함수를 사용하기 때문에 log가 0에 가까워지면, 즉 x,y가 출현을 거의 안한다면 문제가 됨.\n",
        "# 따라서 PPMI = max(0,PMI(x,y)) 로 양의 값으로 바꿔주면 계산에 아주 유용할 듯."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ziQpdRVNTKq"
      },
      "source": [
        "# 단어 사이의 관련성을 0 이상의 실수로 나타내는 함수\n",
        "# verbose는 진행상황 출력 여부를 결정하는 플래그\n",
        "# 큰 말뭉치를 다룰 때 verbose= True 로 설정하면 중간중간 진행 상황을 알려줌.\n",
        "def ppmi(C, verbose=False, eps = 1e-8):   \n",
        "  M = np.zeros_like(C, dtype=np.float32)\n",
        "  N = np.sum(C)\n",
        "  S = np.sum(C, axis=0)\n",
        "  total = C.shpae[0] * C.shape[1]\n",
        "  cnt = 0\n",
        "\n",
        "  for i in range(C.shape[0]):\n",
        "    for j in range(C.shape[1]):\n",
        "      pmi = np.log2(C[i, j] * N / (S[j]*S[i] + eps)) # log2는 밑이 2인 로그\n",
        "      M[i, j] = max(0, pmi)\n",
        "\n",
        "      if verbose:\n",
        "        cnt += 1\n",
        "        if cnt % (total // 100) == 0:\n",
        "          print('%.1f%% 완료' %(100*cnt/total))\n",
        "  return M\n",
        "\n",
        "# 이 코드는 동시발생 행렬에 대해서만 PPMI 행렬을 구할 수 있도록 하고자 단순화 해서 구현했음.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSA7X3B8OR39",
        "outputId": "669f50cf-2c50-45fc-ddcd-b3fe4a7d7594",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "# 동시발생 행렬을 PPMI 행렬로 변환해보자.\n",
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.pardir)\n",
        "import numpy as np\n",
        "from common.util import preprocess, create_co_matrix, cos_similarity, ppmi\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "vocab_size = len(word_to_id)\n",
        "C = create_co_matrix(corpus, vocab_size)\n",
        "W = ppmi(C)\n",
        "\n",
        "np.set_printoptions(precision=3) # 유효 자릿수를 세 자리로 표시\n",
        "print('동시발생 행렬')\n",
        "print(C)\n",
        "print('-'* 50)\n",
        "print('PPMI')\n",
        "print(W)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "동시발생 행렬\n",
            "[[0 1 0 0 0 0 0]\n",
            " [1 0 1 0 1 1 0]\n",
            " [0 1 0 1 0 0 0]\n",
            " [0 0 1 0 1 0 0]\n",
            " [0 1 0 1 0 0 0]\n",
            " [0 1 0 0 0 0 1]\n",
            " [0 0 0 0 0 1 0]]\n",
            "--------------------------------------------------\n",
            "PPMI\n",
            "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
            " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
            " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
            " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
            " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
            " [0.    0.807 0.    0.    0.    0.    2.807]\n",
            " [0.    0.    0.    0.    0.    2.807 0.   ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLdfTJEyQDoH"
      },
      "source": [
        "# 차원감소\n",
        "# PPMI 행렬에도 큰 문제가 있음. 말뭉치의 어휘 수가 10만 개라면 그 벡터의 차원 수도 똑같이 10만이 됨.\n",
        "# 10만 차원의 벡터를 다룬다는 것은 그다지 현실적이지 않음.\n",
        "# 행렬의 내용을 들여다보면 원소 대부분이 0인 것을 알 수 있음. \n",
        "# 벡터의 원소 대부분이 중요하지 않다는 뜻인데 그렇다면 '중요한 정보'는 최대한 유지하면서 차원을 줄이는게 핵심이 됨.\n",
        "# 새로운 축을 도입하여 똑같은 데이터를 좌표축 하나만으로 표시한 그림 확인.\n",
        "# 여기서 중요한 것은 가장 적합한 축을 찾아내는 일로, 1차원 값만으로도 데이터의 본질적인 차이를 구별할 수 있어야 함.\n",
        "# 그 방법으로 SVD 방법을 사용함. 책을 확인하면 더 자세하게 나와있음."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6n6ztCyRb_W"
      },
      "source": [
        "# SVD를 파이썬 코드로 살펴보자. \n",
        "# linalg 모듈이 제공하는 svd 메서드로 실행할 수 있음. linalg는 선형대수의 약어임.\n",
        "import os, sys\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.pardir)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from common.util import preprocess, create_co_matrix, ppmi\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "vocab_size = len(id_to_word)\n",
        "C = create_co_matrix(corpus, vocab_size, window_size = 1)\n",
        "W = ppmi(C)\n",
        "\n",
        "# SVD\n",
        "U, S, V = np.linalg.svd(W)\n",
        "\n",
        "# 동시 발생 행렬을 만들어 PPMI 행렬로 변환한 다음 SVD를 적용시킴."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnJc6l_dR9sO",
        "outputId": "3ecd66f4-38f2-4cf6-9a46-94c55ae9dbf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print(C[0]) # 동시발생 행렬\n",
        "print(W[0]) # PPMI 행렬\n",
        "print(U[0]) # SVD\n",
        "print(U[0, :2]) # 2차원 벡터로 줄이려면 단순히 처음의 두 원소를 꺼내면 됨."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 0 0 0 0]\n",
            "[0.    1.807 0.    0.    0.    0.    0.   ]\n",
            "[ 3.409e-01  0.000e+00 -1.205e-01 -3.886e-16 -9.323e-01 -1.110e-16\n",
            " -2.426e-17]\n",
            "[0.341 0.   ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVvm1l-3SLLj",
        "outputId": "a52d1d8e-e312-4655-e05b-a7dd23f82150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "for word, word_id in word_to_id.items():\n",
        "  plt.annotate(word, (U[word_id, 0], U[word_id, 1]))\n",
        "\n",
        "plt.scatter(U[:,0], U[:,1], alpha=0.5)\n",
        "plt.show()\n",
        "\n",
        "# plt.annotate(word,x,y) 메서드는 2차원 그래프상에서 좌표(x,y) 지점에 word에 담긴 텍스트를 그림.\n",
        "# 그림을 보면 goodbye 와 hello, you, i 가 제법 가까이 있음을 알 수 있음."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD4CAYAAAAD6PrjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAaZUlEQVR4nO3df3RV5Z3v8feXJECqkiDakGIRrNhSAwgcFGvF9vIrq9oKpVpttSjFVJS5beeOV7vo6g/tzKAyY63jup3oCLF1BgoslWJhEVAHqTqS2PC7JUWwkMZAqUkLJhbI9/6RzTMhc/KLzclJ0s9rLVb2c86z9/Nxe+TD3uccMXdHREQEoE+6A4iISPehUhARkUClICIigUpBREQClYKIiASZ6Q7QmvPOO8+HDRuW7hgiIj1KeXn5H9z9/NPdv9uWwrBhwygrK0t3DBGRHsXM3o6zv24fiYhIoFIQ6QU+8YlPnNHj7du3j4KCAgCWLFnC/Pnzz+jxpX3N/x10xPe+9z0WLVoEgJktMbMvnM66KgWRXuDVV19NdwTpJVQKIm34zne+ww9/+MMwXrBgAY8++ij33HMPBQUFjBo1imXLlgHw8ssvc91114W58+fPZ8mSJV2Ss1+/fnz0ox/lk5/8JDfffDOLFi2ioqKCiRMnMnr0aGbOnMm7774L0Orj5eXljBkzhjFjxvD444+fcvz9+/fzqU99ihEjRvD9738faP3cADz88MNMmDCB0aNH893vfrcrTkGvdOLECe644w4uvfRSpk2bRn19PXv27KGwsJDx48dz9dVX8+tf/7rNY5jZZDP7lZltM7OnzKxfW/NVCiJtmDNnDk8//TQAjY2NLF26lAsuuICKigq2bNnC+vXrueeee6iurk5bxs2bN3P8+HG2bNnCmjVrwgc0vvKVr/Dggw+ydetWRo0aFX4zb+3x22+/nccee4wtW7b8jzXeeOMNVq5cydatW1m+fDllZWVJz80tt9zCunXrqKys5I033qCiooLy8nI2btzYRWejd6msrOTuu+9mx44d5ObmsnLlSoqKinjssccoLy9n0aJF3HXXXa3ub2b9gSXAF919FE0fLprX1ppn5NNHZlYIPApkAE+6+8IWz/cDngbGA4ejgPvOxNoiqbCruo6122uoqq3nKNmsXLeRsxrfY+zYsWzatImbb76ZjIwM8vLyuOaaa9i8eTMDBgzo0owvbK2i5LXfUf7CT3Hrw4bdh7l29BA++9nPcvToUWpra7nmmmsAmD17NjfccAN1dXVJH6+traW2tpZJkyYBcOutt7JmzZqw1tSpUxk0aBAAn//859m0aRPf+MY3GDRoEL/61a+oqalh7NixDBo0iHXr1rFu3TrGjh0LwJEjR6isrAzHltY1f91lNxxmyNALueyyywAYP348+/bt49VXX+WGG24I+7z//vttHfKjwF533x2NS4C7gR+2tkPsUjCzDOBxYCpwANhsZqvcfWezaV8F3nX3i83sJuBB4Itx1xZJhV3VdRRv3EtOdhb5Of0ZNXkmP3jkxwzOauBv7pxLaWlp0v0yMzNpbGwM44aGhpRlfGFrFQvX/Iaz+mVyTr+m/4wXrvlNytYzs6TjuXPnsmTJEt555x3mzJkDgLvzrW99i6997Wspy9MbtXzd7a89ztFjxq7qOkbm55CRkUFNTQ25ublUVFSkLMeZuH10OfBbd3/L3f8CLAWubzHnepoaCmAFMNlavspEuom122vIyc4iJzuLPmZc8elC9m99jTc2b2b69OlcffXVLFu2jBMnTnDo0CE2btzI5ZdfzoUXXsjOnTt5//33qa2tZcOGDSnLWPLa7zirXyY52Vmcf/FovPEE/fuc4N9e+jWrV6/mrLPOYuDAgbzyyisA/OQnP+Gaa64hJycn6eO5ubnk5uayadMmAJ555plT1istLeWPf/wj9fX1PPfcc1x11VUAzJw5k7Vr17I5OjcA06dP56mnnuLIkSMAVFVVcfDgwZSdi96i5evunP6Z9OljrN1eE+YMGDCA4cOHs3z5cqCpgJPd7mvmN8AwM7s4Gt8K/GdbO5yJ20dDgP3NxgeAK1qb4+7HzawOGAT8ofkkMysCigCGDh16BqKJdF5VbT35Of3DODOrLyMuu4ITWR8gIyODmTNn8tprrzFmzBjMjIceeojBgwcDcOONN1JQUMDw4cPD7ZNUqPlTAx88uy8A5w77ONYng9cXzaHPBwYyZdwocnJyKCkp4c477+S9997joosuYvHixQCtPr548WLmzJmDmTFt2rRT1rv88suZNWsWBw4c4JZbbiGRSADQt29fPv3pT5Obm0tGRgYA06ZNY9euXVx55ZUAnH322fz0pz/lgx/8YMrOR2/Q8nUH0MeMqtr6Ux575plnmDdvHj/4wQ84duwYN910E2PGjEl6THdvMLPbgeVmlglsBn7cVg6L+5fsRJ+FLXT3udH4VuAKd5/fbM72aM6BaLwnmvOHZMcESCQSrm80Szo8Urqbuvpj5GRnAU1voj48bwZzvvMj/uG2ae3s3TVu/NfX+FOzjMca3uM9z+IDGSf4Xck9FBcXM27cuJTnaGxsZNy4cSxfvpwRI0akfL3erOXrDgjjb069pMPHMbNyd0+cbo4zcfuoCvhws/EF0WNJ50RtlUPTG84i3U5hQR519ceoqz/G7/dV8oPZUxny8QncOr3lBXD6zL5yKEffP05d/TEaGxt57el/ZNNDc9j8z3cwa9asLimEnTt3cvHFFzN58mQVwhnQ/HXX6B62CwvyujTHmbhSyAR2A5Np+s1/M/Ald9/RbM7dwCh3vzN6o/nz7n5jW8fVlYKkU/NPgQzJzaawII+R+TnpjnWKk58+qvlTA3kD+jP7yqFcO3pIumNJDGfidRf3SiF2KUQhPkPTR5wygKfc/e/N7H6gzN1XRZ+V/QkwFvgjcJO7v9XWMVUKIiKdF7cUzsj3FNz9F8AvWjz2nWbbDcANLfcTEZHuRd9oFhGRQKUgIiKBSkFERAKVgoiIBCoFEREJVAoiIhKoFEREJFApiIhIoFIQEZFApSAiIoFKQUREApWCiIgEKgUREQlUCiIiEqgUREQkUCmIiEigUhARkUClICIigUpBREQClYKIiAQqBRERCVQKIiISqBRERCSIVQpmdq6ZlZpZZfRzYCvz1ppZrZmtjrOeiIikVtwrhfuADe4+AtgQjZN5GLg15loiIpJicUvheqAk2i4BZiSb5O4bgD/HXEtERFIsbinkuXt1tP0OkBfnYGZWZGZlZlZ26NChmNFERKSzMtubYGbrgcFJnlrQfODubmYeJ4y7FwPFAIlEItaxRESk89otBXef0tpzZlZjZvnuXm1m+cDBM5pORES6VNzbR6uA2dH2bOD5mMcTEZE0ilsKC4GpZlYJTInGmFnCzJ48OcnMXgGWA5PN7ICZTY+5roiIpEC7t4/a4u6HgclJHi8D5jYbXx1nHRER6Rr6RrOIiAQqBRERCVQKIiISqBRERCRQKYiISKBSEBGRQKUgIiKBSkFERAKVgoiIBCoFEREJVAoiIhKoFEREJFApiIhIoFIQEZFApSAiIoFKQUREApWCiIgEKgUREQlUCiIiEqgUREQkUCmIiEigUhARkSBWKZjZuWZWamaV0c+BSeZcZmavmdkOM9tqZl+Ms6aIiKRO3CuF+4AN7j4C2BCNW3oP+Iq7XwoUAj80s9yY64qISArELYXrgZJouwSY0XKCu+9298po+/fAQeD8mOuKiEgKxC2FPHevjrbfAfLammxmlwN9gT2tPF9kZmVmVnbo0KGY0UREpLMy25tgZuuBwUmeWtB84O5uZt7GcfKBnwCz3b0x2Rx3LwaKARKJRKvHEhGR1Gi3FNx9SmvPmVmNmeW7e3X0m/7BVuYNAF4AFrj766edVkREUiru7aNVwOxoezbwfMsJZtYXeBZ42t1XxFxPRERSKG4pLASmmlklMCUaY2YJM3symnMjMAm4zcwqol+XxVxXRERSwNy75637RCLhZWVl6Y4hItKjmFm5uydOd399o1lERAKVgoiIBCoFEREJVAoiIhKoFEREJFApiIhIoFIQEZFApSAiIoFKQUREApWCiIgEKgUREQlUCiIiEqgUREQkUCmIiEigUhARkUClICIigUpBREQClYKIiAQqBRERCVQKIiISqBRERCRQKYiISBCrFMzsXDMrNbPK6OfAJHMuNLM3zazCzHaY2Z1x1hQRkdSJe6VwH7DB3UcAG6JxS9XAle5+GXAFcJ+ZfSjmuiIikgJxS+F6oCTaLgFmtJzg7n9x9/ejYb8zsKaIiKRI3N+g89y9Otp+B8hLNsnMPmxmW4H9wIPu/vuY64qISApktjfBzNYDg5M8taD5wN3dzDzZMdx9PzA6um30nJmtcPeaJGsVAUUAQ4cO7UB8ERE5k9otBXef0tpzZlZjZvnuXm1m+cDBdo71ezPbDlwNrEjyfDFQDJBIJJIWjIiIpE7c20ergNnR9mzg+ZYTzOwCM8uOtgcCnwR+E3NdERFJgbilsBCYamaVwJRojJklzOzJaM5I4L/MbAvwn8Aid98Wc10REUmBdm8ftcXdDwOTkzxeBsyNtkuB0XHWERGRrqGPh4qISKBSEBGRQKUgIiKBSkFERAKVgoiIBCoFEREJVAoiIhKoFEREJFApiIhIoFIQEZFApSAiIoFKQUREApWCiIgEKgUREQlUCiIiEqgUREQkUCmIiEigUhARkUClICIigUpBREQClYKIiAQqBRERCVQKIiISxCoFMzvXzErNrDL6ObCNuQPM7ICZ/UucNUVEJHXiXincB2xw9xHAhmjcmgeAjTHXExGRFIpbCtcDJdF2CTAj2SQzGw/kAetiriciIikUtxTy3L062n6Hpt/4T2FmfYB/Av6uvYOZWZGZlZlZ2aFDh2JGExGRzspsb4KZrQcGJ3lqQfOBu7uZeZJ5dwG/cPcDZtbmWu5eDBQDJBKJZMcSEZEUarcU3H1Ka8+ZWY2Z5bt7tZnlAweTTLsSuNrM7gLOBvqa2RF3b+v9BxERSYN2S6Edq4DZwMLo5/MtJ7j7l09um9ltQEKFICLSPcV9T2EhMNXMKoEp0RgzS5jZk3HDiYhI1zL37nnrPpFIeFlZWbpjiIj0KGZW7u6J091f32gWEZFApSAiIoFKQUREApWCiIgEKgUREQlUCiIiEqgUREQkUCmIiEigUhARkUClICIigUpBREQClYKIiAQqBRERCVQKIiISqBRERCRQKYiISKBSEBGRQKUgIiKBSqEVZ599drojiIh0OZWCiIgEvboUZsyYwfjx47n00kspLi4Gmq4AFixYwJgxY5g4cSI1NTUA7N27lyuvvJJRo0bx7W9/O52xRUTSpleXwlNPPUV5eTllZWX86Ec/4vDhwxw9epSJEyeyZcsWJk2axBNPPAHA17/+debNm8e2bdvIz89Pc3IRkfTIjLOzmZ0LLAOGAfuAG9393STzTgDbouHv3P1zcdZty67qOtZur6Gqtp5tq57k7Tdfol9mBvv376eyspK+ffty3XXXATB+/HhKS0sB+OUvf8nKlSsBuPXWW7n33ntTFVFEpNuKe6VwH7DB3UcAG6JxMvXufln0K6WFULxxL3X1xzi6bwu7yn/JlHufYOnajYwdO5aGhgaysrIwMwAyMjI4fvx42P/k4yIif63ilsL1QEm0XQLMiHm8WNZuryEnO4uc7Cz+8t4RzhmQy3m5Ayj5xau8/vrrbe571VVXsXTpUgCeeeaZrogrItLtxC2FPHevjrbfAfJamdffzMrM7HUzS1lxVNXWc07/pjtiH0tMovHEcf7f/M+x4scPMXHixDb3ffTRR3n88ccZNWoUVVVVqYooItKtmbu3PcFsPTA4yVMLgBJ3z2029113H5jkGEPcvcrMLgJeBCa7+54k84qAIoChQ4eOf/vttzv1D/NI6W7q6o+Rk50VHjs5/ubUSzp1LBGRnsjMyt09cbr7t3ul4O5T3L0gya/ngRozy4+C5AMHWzlGVfTzLeBlYGwr84rdPeHuifPPP7/T/zCFBXnU1R+jrv4Yje5hu7CgtQsYERFpLu7to1XA7Gh7NvB8ywlmNtDM+kXb5wFXATtjrpvUyPwciiYNJyc7i+q6BnKysyiaNJyR+TmpWE5EpNeJ9ZFUYCHwMzP7KvA2cCOAmSWAO919LjAS+Fcza6SphBa6e0pKAZqKQSUgInJ6YpWCux8GJid5vAyYG22/CoyKs46IiHSNXv2NZhER6RyVgoiIBCoFEREJVAoiIhKoFEREJFApiIhIoFIQEZFApSAiIoFKQUREApWCiIgEKgUREQlUCiIiEqgUREQkUCmIiEigUhARkUClICIigUpBREQClYKIiAQqBRERCVQKIiIS9NpSOHr0KNdeey1jxoyhoKCAZcuWcf/99zNhwgQKCgooKirC3dmzZw/jxo0L+1VWVp4yFhH5a9JrS2Ht2rV86EMfYsuWLWzfvp3CwkLmz5/P5s2b2b59O/X19axevZqPfOQj5OTkUFFRAcDixYu5/fbb05xeRCQ9el0p7Kqu45HS3bxwIIuVP1/D3Lu/wSuvvEJOTg4vvfQSV1xxBaNGjeLFF19kx44dAMydO5fFixdz4sQJli1bxpe+9KU0/1OIiKRHrFIws3PNrNTMKqOfA1uZN9TM1pnZLjPbaWbD4qzbml3VdRRv3Etd/TEuHflR7vinn3HAzudv/+993H///dx1112sWLGCbdu2cccdd9DQ0ADArFmzWLNmDatXr2b8+PEMGjQoFfFERLq9uFcK9wEb3H0EsCEaJ/M08LC7jwQuBw7GXDeptdtryMnOIic7iz//8SCDcs9h4rQZjPnMV3jzzTcBOO+88zhy5AgrVqwI+/Xv35/p06czb9483ToSkb9qmTH3vx74VLRdArwM3Nt8gpl9HMh091IAdz8Sc81WVdXWk5/TH4Dqvbv5+RMPYdaHE9aH1UtLeO655ygoKGDw4MFMmDDhlH2//OUv8+yzzzJt2rRUxRMR6fbM3U9/Z7Nad8+Ntg149+S42ZwZwFzgL8BwYD1wn7ufSHK8IqAIYOjQoePffvvtTuV5pHQ3dfXHyMnOCo+dHH9z6iVt7rto0SLq6up44IEHOrWmiEh3Ymbl7p443f3bvVIws/XA4CRPLWg+cHc3s2QNkwlcDYwFfgcsA24D/q3lRHcvBooBEolEp9uqsCCP4o17ATinfyZ/bjhOXf0xvjjhgjb3mzlzJnv27OHFF1/s7JIiIr1Ku6Xg7lNae87Masws392rzSyf5O8VHAAq3P2taJ/ngIkkKYW4RubnUDRpOGu311BVW8+Q3Gy+OOECRubntLnfs88+e6ajiIj0SHHfU1gFzAYWRj+fTzJnM5BrZue7+yHgfwFlMddt1cj8nHZLQEREkov76aOFwFQzqwSmRGPMLGFmTwJE7x38HbDBzLYBBjwRc10REUmBWFcK7n4YmJzk8TKa3lw+OS4FRsdZS0REUi/u7aNuZ1d13SnvKRQW5Ol2kohIB/Wq/81F82805+f0p67+GMUb97Krui7d0UREeoReVQrNv9Hcxyxsr91ek+5oIiI9Qq8qharaes7p/993xIoX3EHj0cNU1danMZWISM/Rq0phSG42f244HsZFf/8Efc4axJDc7DSmEhHpOXpVKRQW5FFXf4y6+mM0uoftwoK8dEcTEekRelUpnPxGc052FtV1DeRkZ1E0abg+fSQi0kG97iOp+kaziMjp61VXCiIiEo9KQUREApWCiIgEKgUREQlUCiIiEsT66zhTycwOAZ37+zhPdR7whzMUJ9V6StaekhOUNVWUNTXOZNYL3f38092525ZCXGZWFufvKe1KPSVrT8kJypoqypoa3Smrbh+JiEigUhARkaA3l0JxugN0Qk/J2lNygrKmirKmRrfJ2mvfUxARkc7rzVcKIiLSSSoFEREJenQpmFmhmf3GzH5rZvcleb6fmS2Lnv8vMxvW9SlDlvayTjKzN83suJl9IR0Zm2VpL+vfmtlOM9tqZhvM7MJ05IyytJf1TjPbZmYVZrbJzD6ejpxRljazNps3y8zczNL2EcUOnNfbzOxQdF4rzGxuOnJGWdo9r2Z2Y/Sa3WFm/97VGZvlaO+8PtLsnO42s9ouD+nuPfIXkAHsAS4C+gJbgI+3mHMX8ONo+yZgWTfOOgwYDTwNfKGbn9dPAx+Itud18/M6oNn254C13TVrNO8cYCPwOpDorlmB24B/SUe+08g6AvgVMDAaf7C7Zm0x/2+Ap7o6Z0++Urgc+K27v+XufwGWAte3mHM9UBJtrwAmm5l1YcaT2s3q7vvcfSvQmIZ8zXUk60vu/l40fB24oIszntSRrH9qNjwLSNcnKzryegV4AHgQaOjKcC10NGt30JGsdwCPu/u7AO5+sIszntTZ83oz8B9dkqyZnlwKQ4D9zcYHoseSznH340AdMKhL0rWSI5Isa3fR2axfBdakNFHrOpTVzO42sz3AQ8D/7qJsLbWb1czGAR929xe6MlgSHX0NzIpuIa4wsw93TbT/oSNZLwEuMbNfmtnrZlbYZelO1eH/tqJbssOBF7sg1yl6cilImpnZLUACeDjdWdri7o+7+0eAe4FvpztPMmbWB/hn4P+kO0sH/RwY5u6jgVL++4q8O8qk6RbSp2j60/cTZpab1kTtuwlY4e4nunrhnlwKVUDzP51cED2WdI6ZZQI5wOEuSddKjkiyrN1Fh7Ka2RRgAfA5d3+/i7K11NnzuhSYkdJErWsv6zlAAfCyme0DJgKr0vRmc7vn1d0PN/v3/iQwvouytdSR18ABYJW7H3P3vcBumkqiq3Xm9XoTabh1BPToN5ozgbdousQ6+abNpS3m3M2pbzT/rLtmbTZ3Cel9o7kj53UsTW+YjegBr4ERzbY/C5R116wt5r9M+t5o7sh5zW+2PRN4vRtnLQRKou3zaLqFM6g7Zo3mfQzYR/Tl4i7PmY5Fz+BJ/gxNrb8HWBA9dj9Nf3oF6A8sB34LvAFc1I2zTqDpTzRHabqa2dGNs64HaoCK6Neqbpz1UWBHlPOltn4jTnfWFnPTVgodPK//GJ3XLdF5/Vg3zmo03ZrbCWwDbuquWaPx94CF6cqo/82FiIgEPfk9BREROcNUCiIiEqgUREQkUCmIiEigUhARkUClICIigUpBRESC/w+wG7EbthPKGQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxTXyAlZSjCr",
        "outputId": "b1bacf4b-19bc-44e7-c9c9-487d043c8364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.chdir)\n",
        "\n",
        "from dataset import ptb\n",
        "\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "\n",
        "print('말뭉치 크기:' , len(corpus))\n",
        "print('corpus[:30]', corpus[:30])\n",
        "print()\n",
        "print('id_to_word[0]:', id_to_word[0])\n",
        "print('id_to_word[1]:', id_to_word[1])\n",
        "print('id_to_word[2]:', id_to_word[2])\n",
        "print()\n",
        "print(\"word_to_id['car']:\", word_to_id['car'])\n",
        "print(\"word_to_id['happy']:\", word_to_id['happy'])\n",
        "print(\"word_to_id['lexus']:\", word_to_id['lexus'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "말뭉치 크기: 929589\n",
            "corpus[:30] [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24 25 26 27 28 29]\n",
            "\n",
            "id_to_word[0]: aer\n",
            "id_to_word[1]: banknote\n",
            "id_to_word[2]: berlitz\n",
            "\n",
            "word_to_id['car']: 3856\n",
            "word_to_id['happy']: 4428\n",
            "word_to_id['lexus']: 7426\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6TlThHF4lnm",
        "outputId": "eba00f94-3d72-4695-dd75-c4a3e6079cc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import sys\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.pardir)\n",
        "import numpy as np\n",
        "from common.util import most_similar, create_co_matrix, ppmi\n",
        "from dataset import ptb\n",
        "\n",
        "window_size = 2\n",
        "wordvec_size = 100\n",
        "\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "vocab_size = len(word_to_id)\n",
        "print('동시발생 수 계산 ...')\n",
        "C = create_co_matrix(corpus, vocab_size, window_size) \n",
        "# 윈도우 크기가 2면 타깃 단어 좌우 2단어씩을 맥락에 포함\n",
        "print('PPMI 계산 ...')\n",
        "W = ppmi(C, verbose=True)\n",
        "\n",
        "print('SVD 계산 ...')\n",
        "try:\n",
        "  # truncated SVD( 빠르다! )\n",
        "  from sklearn.utils.extmath import randomized_svd\n",
        "  U, S, V = randomized_svd(W, n_components = wordvec_size, n_iter=5, random_state=None)\n",
        "\n",
        "except ImportError:\n",
        "  # SVD (느리다)\n",
        "  U, S, V = np.linalg.svd(W)\n",
        "\n",
        "word_vecs = U[:, :wordvec_size] # U의 행전체, 열은 100까지만. (차원축소 인듯.)\n",
        " \n",
        "querys = ['you', 'year', 'car', 'toyota']\n",
        "for query in querys:\n",
        "  most_similar(query, word_to_id, id_to_word, word_vecs, top = 5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "동시발생 수 계산 ...\n",
            "PPMI 계산 ...\n",
            "1.0% 완료\n",
            "2.0% 완료\n",
            "3.0% 완료\n",
            "4.0% 완료\n",
            "5.0% 완료\n",
            "6.0% 완료\n",
            "7.0% 완료\n",
            "8.0% 완료\n",
            "9.0% 완료\n",
            "10.0% 완료\n",
            "11.0% 완료\n",
            "12.0% 완료\n",
            "13.0% 완료\n",
            "14.0% 완료\n",
            "15.0% 완료\n",
            "16.0% 완료\n",
            "17.0% 완료\n",
            "18.0% 완료\n",
            "19.0% 완료\n",
            "20.0% 완료\n",
            "21.0% 완료\n",
            "22.0% 완료\n",
            "23.0% 완료\n",
            "24.0% 완료\n",
            "25.0% 완료\n",
            "26.0% 완료\n",
            "27.0% 완료\n",
            "28.0% 완료\n",
            "29.0% 완료\n",
            "30.0% 완료\n",
            "31.0% 완료\n",
            "32.0% 완료\n",
            "33.0% 완료\n",
            "34.0% 완료\n",
            "35.0% 완료\n",
            "36.0% 완료\n",
            "37.0% 완료\n",
            "38.0% 완료\n",
            "39.0% 완료\n",
            "40.0% 완료\n",
            "41.0% 완료\n",
            "42.0% 완료\n",
            "43.0% 완료\n",
            "44.0% 완료\n",
            "45.0% 완료\n",
            "46.0% 완료\n",
            "47.0% 완료\n",
            "48.0% 완료\n",
            "49.0% 완료\n",
            "50.0% 완료\n",
            "51.0% 완료\n",
            "52.0% 완료\n",
            "53.0% 완료\n",
            "54.0% 완료\n",
            "55.0% 완료\n",
            "56.0% 완료\n",
            "57.0% 완료\n",
            "58.0% 완료\n",
            "59.0% 완료\n",
            "60.0% 완료\n",
            "61.0% 완료\n",
            "62.0% 완료\n",
            "63.0% 완료\n",
            "64.0% 완료\n",
            "65.0% 완료\n",
            "66.0% 완료\n",
            "67.0% 완료\n",
            "68.0% 완료\n",
            "69.0% 완료\n",
            "70.0% 완료\n",
            "71.0% 완료\n",
            "72.0% 완료\n",
            "73.0% 완료\n",
            "74.0% 완료\n",
            "75.0% 완료\n",
            "76.0% 완료\n",
            "77.0% 완료\n",
            "78.0% 완료\n",
            "79.0% 완료\n",
            "80.0% 완료\n",
            "81.0% 완료\n",
            "82.0% 완료\n",
            "83.0% 완료\n",
            "84.0% 완료\n",
            "85.0% 완료\n",
            "86.0% 완료\n",
            "87.0% 완료\n",
            "88.0% 완료\n",
            "89.0% 완료\n",
            "90.0% 완료\n",
            "91.0% 완료\n",
            "92.0% 완료\n",
            "93.0% 완료\n",
            "94.0% 완료\n",
            "95.0% 완료\n",
            "96.0% 완료\n",
            "97.0% 완료\n",
            "98.0% 완료\n",
            "99.0% 완료\n",
            "100.0% 완료\n",
            "SVD 계산 ...\n",
            "\n",
            "[query] you\n",
            " i: 0.6468660831451416\n",
            " we: 0.6108704805374146\n",
            " anybody: 0.6040822863578796\n",
            " do: 0.5814424157142639\n",
            " always: 0.5622100234031677\n",
            "\n",
            "[query] year\n",
            " month: 0.729286253452301\n",
            " quarter: 0.6694890260696411\n",
            " earlier: 0.6335133910179138\n",
            " february: 0.6110854744911194\n",
            " last: 0.5926453471183777\n",
            "\n",
            "[query] car\n",
            " auto: 0.6244403719902039\n",
            " luxury: 0.6134950518608093\n",
            " truck: 0.5803208351135254\n",
            " vehicle: 0.5529912114143372\n",
            " cars: 0.5499685406684875\n",
            "\n",
            "[query] toyota\n",
            " motor: 0.7061044573783875\n",
            " nissan: 0.6875864863395691\n",
            " motors: 0.6356449723243713\n",
            " honda: 0.6053351759910583\n",
            " lexus: 0.5914918780326843\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jJfSu7MfnTe"
      },
      "source": [
        "## 3장 - word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHNYh06hpYUj",
        "outputId": "2f740438-d534-42f4-ef72-62c2cbf5f760",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from common.layers import MatMul\n",
        "\n",
        "c = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
        "W = np.random.randn(7,3)\n",
        "layer = MatMul(W)\n",
        "h = layer.forward(c)\n",
        "print(h)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.04013674 -0.34643992  0.40698846]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blQvMkpqpqxC",
        "outputId": "71ce5620-595d-4c05-a6aa-7469c7aa9e95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from common.layers import MatMul\n",
        "\n",
        "# 샘플 맥락 데이터\n",
        "c0 = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
        "c1 = np.array([[0, 0, 1, 0, 0, 0, 0]])\n",
        "\n",
        "# 가중치 초기화\n",
        "W_in = np.random.randn(7, 3)\n",
        "W_out = np.random.randn(3, 7)\n",
        "\n",
        "# 계층 생성\n",
        "in_layer0 = MatMul(W_in)\n",
        "in_layer1 = MatMul(W_in)\n",
        "out_layer = MatMul(W_out)\n",
        "\n",
        "# 순전파\n",
        "h0 = in_layer0.forward(c0)\n",
        "h1 = in_layer1.forward(c1)\n",
        "h = 0.5 * (h0 + h1)\n",
        "s = out_layer.forward(h)\n",
        "\n",
        "print(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.93924377  0.83604034  1.75272435  0.98822214  3.16537359 -0.75249687\n",
            "  -1.07426292]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0l12czhqc48",
        "outputId": "523c17d4-0cf2-4eea-a286-69da090cc84b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.util import preprocess\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "print(corpus)\n",
        "\n",
        "print(id_to_word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3 4 1 5 6]\n",
            "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQqnxFSJqrMi"
      },
      "source": [
        "def create_contexts_target(corpus, window_size=1):\n",
        "  target = corpus[window_size:-window_size]\n",
        "  contexts = []\n",
        "\n",
        "  for idx in range(window_size, len(corpus) - window_size):\n",
        "    cs = []\n",
        "    for t in range(-window_size, window_size + 1):\n",
        "      if t == 0:\n",
        "        continue\n",
        "      cs.append(corpus[idx] + t)\n",
        "\n",
        "    contexts.append(cs)\n",
        "  \n",
        "  return np.array(contexts), np.array(target)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqEOopr-rGsF",
        "outputId": "d8dc607a-3892-4e67-a2d5-89edbef7a71f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "contexts, target = create_contexts_target(corpus, window_size=1)\n",
        "print(contexts)\n",
        "\n",
        "print(target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 2]\n",
            " [1 3]\n",
            " [2 4]\n",
            " [3 5]\n",
            " [0 2]\n",
            " [4 6]]\n",
            "[1 2 3 4 1 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-QnQkiofF6p"
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append(os.pardir)\n",
        "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
        "\n",
        "text = 'You say goodbye and I say hello'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "\n",
        "contexts, target = create_contexts_target(corpus, window_size=1)\n",
        "\n",
        "vocab_size = len(word_to_id)\n",
        "target = convert_one_hot(target, vocab_size)\n",
        "contexts = convert_one_hot(contexts, vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePwy00fao4e2"
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from common.layers import MatMul, SoftmaxWithLoss\n",
        "\n",
        "class SimpleCBOW:\n",
        "  def __init__(self, vocab_size, hidden_size):\n",
        "    V, H = vocab_size, hidden_size\n",
        "\n",
        "    # 가중치 초기화\n",
        "    W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
        "    W_out = 0.01 * np.random.randn(V, H).astype('f')\n",
        "\n",
        "    # 계층 생성\n",
        "    self.in_layer0 = MatMul(W_in)\n",
        "    self.in_layer1 = MatMul(W_in)\n",
        "    self.out_layer = MatMul(W_out)\n",
        "    self.loss_layer = SoftmaxWithLoss()\n",
        "\n",
        "    # 모든 가중치와 기울기를 리스트에 모은다.\n",
        "    layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
        "    self.params, self.grads = [], []\n",
        "    for layer in layers:\n",
        "      self.params += layer.params\n",
        "      self.grads += layer.grads\n",
        "\n",
        "    # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
        "    self.word_vecs = W_in"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZyQBa-GwBIo"
      },
      "source": [
        " def forward(self, contexts, target):\n",
        "   h0 = self.in_layer0.forward(contexts[:, 0])\n",
        "   h1 = self.in_layer1.forward(contexts[:, 1])\n",
        "   h = (h0 + h1) * 0.5\n",
        "   score = self.out_layer.forward(h)\n",
        "   loss = self.loss_layer.forward(score, traget)\n",
        "   return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRzl8WQtwspK"
      },
      "source": [
        "def backward(self, dout=1):\n",
        "  ds = self.loss_layer.backward(dout)\n",
        "  da = self.out_layer.backward(ds)\n",
        "  da *= 0.5\n",
        "  self.in_layer1.backward(da)\n",
        "  self.in_layer0.backward(da)\n",
        "  return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeClrTYaw37a",
        "outputId": "e816f9a5-a0d7-4a88-b4dc-e953f371c909",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import sys, os\n",
        "sys.path.append('..')\n",
        "from common.trainer import Trainer\n",
        "from common.optimizer import Adam\n",
        "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master/ch03\")\n",
        "sys.path.append('..')\n",
        "from simple_cbow import SimpleCBOW\n",
        "\n",
        "\n",
        "window_size = 1\n",
        "hidden_size = 5\n",
        "batch_size = 3\n",
        "max_epoch = 1000\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "\n",
        "vocab_size = len(word_to_id)\n",
        "contexts, target = create_contexts_target(corpus, window_size)\n",
        "target = convert_one_hot(target, vocab_size)\n",
        "contexts = convert_one_hot(contexts, vocab_size)\n",
        "\n",
        "model = SimpleCBOW(vocab_size, hidden_size)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "trainer.fit(contexts, target, max_epoch, batch_size)\n",
        "trainer.plot()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| 에폭 1 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 2 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 3 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 4 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 5 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 6 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 7 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 8 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 9 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 10 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 11 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 12 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 13 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 14 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 15 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 16 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 17 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 18 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 19 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 20 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 21 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 22 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 23 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 24 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 25 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 26 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 27 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 28 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 29 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 30 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 31 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 32 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 33 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 34 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 35 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 36 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 37 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
            "| 에폭 38 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
            "| 에폭 39 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
            "| 에폭 40 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
            "| 에폭 41 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
            "| 에폭 42 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
            "| 에폭 43 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
            "| 에폭 44 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
            "| 에폭 45 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
            "| 에폭 46 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
            "| 에폭 47 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
            "| 에폭 48 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
            "| 에폭 49 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
            "| 에폭 50 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
            "| 에폭 51 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
            "| 에폭 52 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
            "| 에폭 53 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
            "| 에폭 54 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
            "| 에폭 55 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
            "| 에폭 56 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
            "| 에폭 57 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
            "| 에폭 58 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
            "| 에폭 59 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
            "| 에폭 60 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
            "| 에폭 61 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
            "| 에폭 62 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
            "| 에폭 63 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
            "| 에폭 64 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
            "| 에폭 65 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
            "| 에폭 66 |  반복 1 / 2 | 시간 0[s] | 손실 1.84\n",
            "| 에폭 67 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
            "| 에폭 68 |  반복 1 / 2 | 시간 0[s] | 손실 1.85\n",
            "| 에폭 69 |  반복 1 / 2 | 시간 0[s] | 손실 1.84\n",
            "| 에폭 70 |  반복 1 / 2 | 시간 0[s] | 손실 1.85\n",
            "| 에폭 71 |  반복 1 / 2 | 시간 0[s] | 손실 1.83\n",
            "| 에폭 72 |  반복 1 / 2 | 시간 0[s] | 손실 1.82\n",
            "| 에폭 73 |  반복 1 / 2 | 시간 0[s] | 손실 1.83\n",
            "| 에폭 74 |  반복 1 / 2 | 시간 0[s] | 손실 1.82\n",
            "| 에폭 75 |  반복 1 / 2 | 시간 0[s] | 손실 1.83\n",
            "| 에폭 76 |  반복 1 / 2 | 시간 0[s] | 손실 1.83\n",
            "| 에폭 77 |  반복 1 / 2 | 시간 0[s] | 손실 1.81\n",
            "| 에폭 78 |  반복 1 / 2 | 시간 0[s] | 손실 1.80\n",
            "| 에폭 79 |  반복 1 / 2 | 시간 0[s] | 손실 1.81\n",
            "| 에폭 80 |  반복 1 / 2 | 시간 0[s] | 손실 1.81\n",
            "| 에폭 81 |  반복 1 / 2 | 시간 0[s] | 손실 1.78\n",
            "| 에폭 82 |  반복 1 / 2 | 시간 0[s] | 손실 1.81\n",
            "| 에폭 83 |  반복 1 / 2 | 시간 0[s] | 손실 1.78\n",
            "| 에폭 84 |  반복 1 / 2 | 시간 0[s] | 손실 1.78\n",
            "| 에폭 85 |  반복 1 / 2 | 시간 0[s] | 손실 1.78\n",
            "| 에폭 86 |  반복 1 / 2 | 시간 0[s] | 손실 1.79\n",
            "| 에폭 87 |  반복 1 / 2 | 시간 0[s] | 손실 1.78\n",
            "| 에폭 88 |  반복 1 / 2 | 시간 0[s] | 손실 1.77\n",
            "| 에폭 89 |  반복 1 / 2 | 시간 0[s] | 손실 1.76\n",
            "| 에폭 90 |  반복 1 / 2 | 시간 0[s] | 손실 1.76\n",
            "| 에폭 91 |  반복 1 / 2 | 시간 0[s] | 손실 1.74\n",
            "| 에폭 92 |  반복 1 / 2 | 시간 0[s] | 손실 1.78\n",
            "| 에폭 93 |  반복 1 / 2 | 시간 0[s] | 손실 1.72\n",
            "| 에폭 94 |  반복 1 / 2 | 시간 0[s] | 손실 1.77\n",
            "| 에폭 95 |  반복 1 / 2 | 시간 0[s] | 손실 1.74\n",
            "| 에폭 96 |  반복 1 / 2 | 시간 0[s] | 손실 1.73\n",
            "| 에폭 97 |  반복 1 / 2 | 시간 0[s] | 손실 1.73\n",
            "| 에폭 98 |  반복 1 / 2 | 시간 0[s] | 손실 1.71\n",
            "| 에폭 99 |  반복 1 / 2 | 시간 0[s] | 손실 1.75\n",
            "| 에폭 100 |  반복 1 / 2 | 시간 0[s] | 손실 1.70\n",
            "| 에폭 101 |  반복 1 / 2 | 시간 0[s] | 손실 1.71\n",
            "| 에폭 102 |  반복 1 / 2 | 시간 0[s] | 손실 1.70\n",
            "| 에폭 103 |  반복 1 / 2 | 시간 0[s] | 손실 1.70\n",
            "| 에폭 104 |  반복 1 / 2 | 시간 0[s] | 손실 1.71\n",
            "| 에폭 105 |  반복 1 / 2 | 시간 0[s] | 손실 1.67\n",
            "| 에폭 106 |  반복 1 / 2 | 시간 0[s] | 손실 1.67\n",
            "| 에폭 107 |  반복 1 / 2 | 시간 0[s] | 손실 1.70\n",
            "| 에폭 108 |  반복 1 / 2 | 시간 0[s] | 손실 1.67\n",
            "| 에폭 109 |  반복 1 / 2 | 시간 0[s] | 손실 1.68\n",
            "| 에폭 110 |  반복 1 / 2 | 시간 0[s] | 손실 1.65\n",
            "| 에폭 111 |  반복 1 / 2 | 시간 0[s] | 손실 1.65\n",
            "| 에폭 112 |  반복 1 / 2 | 시간 0[s] | 손실 1.68\n",
            "| 에폭 113 |  반복 1 / 2 | 시간 0[s] | 손실 1.64\n",
            "| 에폭 114 |  반복 1 / 2 | 시간 0[s] | 손실 1.66\n",
            "| 에폭 115 |  반복 1 / 2 | 시간 0[s] | 손실 1.65\n",
            "| 에폭 116 |  반복 1 / 2 | 시간 0[s] | 손실 1.61\n",
            "| 에폭 117 |  반복 1 / 2 | 시간 0[s] | 손실 1.64\n",
            "| 에폭 118 |  반복 1 / 2 | 시간 0[s] | 손실 1.65\n",
            "| 에폭 119 |  반복 1 / 2 | 시간 0[s] | 손실 1.62\n",
            "| 에폭 120 |  반복 1 / 2 | 시간 0[s] | 손실 1.61\n",
            "| 에폭 121 |  반복 1 / 2 | 시간 0[s] | 손실 1.59\n",
            "| 에폭 122 |  반복 1 / 2 | 시간 0[s] | 손실 1.63\n",
            "| 에폭 123 |  반복 1 / 2 | 시간 0[s] | 손실 1.60\n",
            "| 에폭 124 |  반복 1 / 2 | 시간 0[s] | 손실 1.58\n",
            "| 에폭 125 |  반복 1 / 2 | 시간 0[s] | 손실 1.58\n",
            "| 에폭 126 |  반복 1 / 2 | 시간 0[s] | 손실 1.59\n",
            "| 에폭 127 |  반복 1 / 2 | 시간 0[s] | 손실 1.60\n",
            "| 에폭 128 |  반복 1 / 2 | 시간 0[s] | 손실 1.57\n",
            "| 에폭 129 |  반복 1 / 2 | 시간 0[s] | 손실 1.58\n",
            "| 에폭 130 |  반복 1 / 2 | 시간 0[s] | 손실 1.54\n",
            "| 에폭 131 |  반복 1 / 2 | 시간 0[s] | 손실 1.56\n",
            "| 에폭 132 |  반복 1 / 2 | 시간 0[s] | 손실 1.56\n",
            "| 에폭 133 |  반복 1 / 2 | 시간 0[s] | 손실 1.57\n",
            "| 에폭 134 |  반복 1 / 2 | 시간 0[s] | 손실 1.50\n",
            "| 에폭 135 |  반복 1 / 2 | 시간 0[s] | 손실 1.57\n",
            "| 에폭 136 |  반복 1 / 2 | 시간 0[s] | 손실 1.56\n",
            "| 에폭 137 |  반복 1 / 2 | 시간 0[s] | 손실 1.53\n",
            "| 에폭 138 |  반복 1 / 2 | 시간 0[s] | 손실 1.53\n",
            "| 에폭 139 |  반복 1 / 2 | 시간 0[s] | 손실 1.49\n",
            "| 에폭 140 |  반복 1 / 2 | 시간 0[s] | 손실 1.54\n",
            "| 에폭 141 |  반복 1 / 2 | 시간 0[s] | 손실 1.48\n",
            "| 에폭 142 |  반복 1 / 2 | 시간 0[s] | 손실 1.51\n",
            "| 에폭 143 |  반복 1 / 2 | 시간 0[s] | 손실 1.53\n",
            "| 에폭 144 |  반복 1 / 2 | 시간 0[s] | 손실 1.47\n",
            "| 에폭 145 |  반복 1 / 2 | 시간 0[s] | 손실 1.53\n",
            "| 에폭 146 |  반복 1 / 2 | 시간 0[s] | 손실 1.47\n",
            "| 에폭 147 |  반복 1 / 2 | 시간 0[s] | 손실 1.48\n",
            "| 에폭 148 |  반복 1 / 2 | 시간 0[s] | 손실 1.48\n",
            "| 에폭 149 |  반복 1 / 2 | 시간 0[s] | 손실 1.49\n",
            "| 에폭 150 |  반복 1 / 2 | 시간 0[s] | 손실 1.45\n",
            "| 에폭 151 |  반복 1 / 2 | 시간 0[s] | 손실 1.42\n",
            "| 에폭 152 |  반복 1 / 2 | 시간 0[s] | 손실 1.47\n",
            "| 에폭 153 |  반복 1 / 2 | 시간 0[s] | 손실 1.46\n",
            "| 에폭 154 |  반복 1 / 2 | 시간 0[s] | 손실 1.45\n",
            "| 에폭 155 |  반복 1 / 2 | 시간 0[s] | 손실 1.42\n",
            "| 에폭 156 |  반복 1 / 2 | 시간 0[s] | 손실 1.46\n",
            "| 에폭 157 |  반복 1 / 2 | 시간 0[s] | 손실 1.46\n",
            "| 에폭 158 |  반복 1 / 2 | 시간 0[s] | 손실 1.39\n",
            "| 에폭 159 |  반복 1 / 2 | 시간 0[s] | 손실 1.48\n",
            "| 에폭 160 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
            "| 에폭 161 |  반복 1 / 2 | 시간 0[s] | 손실 1.45\n",
            "| 에폭 162 |  반복 1 / 2 | 시간 0[s] | 손실 1.33\n",
            "| 에폭 163 |  반복 1 / 2 | 시간 0[s] | 손실 1.46\n",
            "| 에폭 164 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
            "| 에폭 165 |  반복 1 / 2 | 시간 0[s] | 손실 1.42\n",
            "| 에폭 166 |  반복 1 / 2 | 시간 0[s] | 손실 1.39\n",
            "| 에폭 167 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
            "| 에폭 168 |  반복 1 / 2 | 시간 0[s] | 손실 1.43\n",
            "| 에폭 169 |  반복 1 / 2 | 시간 0[s] | 손실 1.38\n",
            "| 에폭 170 |  반복 1 / 2 | 시간 0[s] | 손실 1.35\n",
            "| 에폭 171 |  반복 1 / 2 | 시간 0[s] | 손실 1.37\n",
            "| 에폭 172 |  반복 1 / 2 | 시간 0[s] | 손실 1.39\n",
            "| 에폭 173 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
            "| 에폭 174 |  반복 1 / 2 | 시간 0[s] | 손실 1.35\n",
            "| 에폭 175 |  반복 1 / 2 | 시간 0[s] | 손실 1.35\n",
            "| 에폭 176 |  반복 1 / 2 | 시간 0[s] | 손실 1.33\n",
            "| 에폭 177 |  반복 1 / 2 | 시간 0[s] | 손실 1.40\n",
            "| 에폭 178 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
            "| 에폭 179 |  반복 1 / 2 | 시간 0[s] | 손실 1.31\n",
            "| 에폭 180 |  반복 1 / 2 | 시간 0[s] | 손실 1.44\n",
            "| 에폭 181 |  반복 1 / 2 | 시간 0[s] | 손실 1.26\n",
            "| 에폭 182 |  반복 1 / 2 | 시간 0[s] | 손실 1.31\n",
            "| 에폭 183 |  반복 1 / 2 | 시간 0[s] | 손실 1.31\n",
            "| 에폭 184 |  반복 1 / 2 | 시간 0[s] | 손실 1.28\n",
            "| 에폭 185 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
            "| 에폭 186 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
            "| 에폭 187 |  반복 1 / 2 | 시간 0[s] | 손실 1.27\n",
            "| 에폭 188 |  반복 1 / 2 | 시간 0[s] | 손실 1.34\n",
            "| 에폭 189 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
            "| 에폭 190 |  반복 1 / 2 | 시간 0[s] | 손실 1.27\n",
            "| 에폭 191 |  반복 1 / 2 | 시간 0[s] | 손실 1.28\n",
            "| 에폭 192 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
            "| 에폭 193 |  반복 1 / 2 | 시간 0[s] | 손실 1.27\n",
            "| 에폭 194 |  반복 1 / 2 | 시간 0[s] | 손실 1.30\n",
            "| 에폭 195 |  반복 1 / 2 | 시간 0[s] | 손실 1.27\n",
            "| 에폭 196 |  반복 1 / 2 | 시간 0[s] | 손실 1.28\n",
            "| 에폭 197 |  반복 1 / 2 | 시간 0[s] | 손실 1.20\n",
            "| 에폭 198 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
            "| 에폭 199 |  반복 1 / 2 | 시간 0[s] | 손실 1.32\n",
            "| 에폭 200 |  반복 1 / 2 | 시간 0[s] | 손실 1.25\n",
            "| 에폭 201 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
            "| 에폭 202 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
            "| 에폭 203 |  반복 1 / 2 | 시간 0[s] | 손실 1.23\n",
            "| 에폭 204 |  반복 1 / 2 | 시간 0[s] | 손실 1.23\n",
            "| 에폭 205 |  반복 1 / 2 | 시간 0[s] | 손실 1.23\n",
            "| 에폭 206 |  반복 1 / 2 | 시간 0[s] | 손실 1.16\n",
            "| 에폭 207 |  반복 1 / 2 | 시간 0[s] | 손실 1.32\n",
            "| 에폭 208 |  반복 1 / 2 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 209 |  반복 1 / 2 | 시간 0[s] | 손실 1.27\n",
            "| 에폭 210 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
            "| 에폭 211 |  반복 1 / 2 | 시간 0[s] | 손실 1.26\n",
            "| 에폭 212 |  반복 1 / 2 | 시간 0[s] | 손실 1.25\n",
            "| 에폭 213 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
            "| 에폭 214 |  반복 1 / 2 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 215 |  반복 1 / 2 | 시간 0[s] | 손실 1.19\n",
            "| 에폭 216 |  반복 1 / 2 | 시간 0[s] | 손실 1.26\n",
            "| 에폭 217 |  반복 1 / 2 | 시간 0[s] | 손실 1.23\n",
            "| 에폭 218 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
            "| 에폭 219 |  반복 1 / 2 | 시간 0[s] | 손실 1.23\n",
            "| 에폭 220 |  반복 1 / 2 | 시간 0[s] | 손실 1.18\n",
            "| 에폭 221 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
            "| 에폭 222 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
            "| 에폭 223 |  반복 1 / 2 | 시간 0[s] | 손실 1.22\n",
            "| 에폭 224 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
            "| 에폭 225 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
            "| 에폭 226 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
            "| 에폭 227 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
            "| 에폭 228 |  반복 1 / 2 | 시간 0[s] | 손실 1.22\n",
            "| 에폭 229 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
            "| 에폭 230 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
            "| 에폭 231 |  반복 1 / 2 | 시간 0[s] | 손실 1.27\n",
            "| 에폭 232 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
            "| 에폭 233 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
            "| 에폭 234 |  반복 1 / 2 | 시간 0[s] | 손실 1.21\n",
            "| 에폭 235 |  반복 1 / 2 | 시간 0[s] | 손실 1.18\n",
            "| 에폭 236 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 237 |  반복 1 / 2 | 시간 0[s] | 손실 1.26\n",
            "| 에폭 238 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
            "| 에폭 239 |  반복 1 / 2 | 시간 0[s] | 손실 1.13\n",
            "| 에폭 240 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
            "| 에폭 241 |  반복 1 / 2 | 시간 0[s] | 손실 1.18\n",
            "| 에폭 242 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
            "| 에폭 243 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
            "| 에폭 244 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
            "| 에폭 245 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
            "| 에폭 246 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
            "| 에폭 247 |  반복 1 / 2 | 시간 0[s] | 손실 1.23\n",
            "| 에폭 248 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
            "| 에폭 249 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
            "| 에폭 250 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
            "| 에폭 251 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
            "| 에폭 252 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
            "| 에폭 253 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
            "| 에폭 254 |  반복 1 / 2 | 시간 0[s] | 손실 1.07\n",
            "| 에폭 255 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
            "| 에폭 256 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 257 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
            "| 에폭 258 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
            "| 에폭 259 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
            "| 에폭 260 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 261 |  반복 1 / 2 | 시간 0[s] | 손실 1.13\n",
            "| 에폭 262 |  반복 1 / 2 | 시간 0[s] | 손실 1.13\n",
            "| 에폭 263 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 264 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
            "| 에폭 265 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
            "| 에폭 266 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
            "| 에폭 267 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
            "| 에폭 268 |  반복 1 / 2 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 269 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 270 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
            "| 에폭 271 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
            "| 에폭 272 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
            "| 에폭 273 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 274 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
            "| 에폭 275 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
            "| 에폭 276 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 277 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
            "| 에폭 278 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
            "| 에폭 279 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
            "| 에폭 280 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 281 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
            "| 에폭 282 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
            "| 에폭 283 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 284 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
            "| 에폭 285 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 286 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
            "| 에폭 287 |  반복 1 / 2 | 시간 0[s] | 손실 1.07\n",
            "| 에폭 288 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
            "| 에폭 289 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 290 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 291 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
            "| 에폭 292 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
            "| 에폭 293 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 294 |  반복 1 / 2 | 시간 0[s] | 손실 1.16\n",
            "| 에폭 295 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
            "| 에폭 296 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 297 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
            "| 에폭 298 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 299 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
            "| 에폭 300 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 301 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
            "| 에폭 302 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
            "| 에폭 303 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
            "| 에폭 304 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
            "| 에폭 305 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 306 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
            "| 에폭 307 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 308 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 309 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 310 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 311 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
            "| 에폭 312 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 313 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
            "| 에폭 314 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
            "| 에폭 315 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 316 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 317 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 318 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 319 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 320 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 321 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
            "| 에폭 322 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 323 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 324 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 325 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
            "| 에폭 326 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
            "| 에폭 327 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 328 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 329 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 330 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 331 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 332 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
            "| 에폭 333 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 334 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 335 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
            "| 에폭 336 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 337 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 338 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
            "| 에폭 339 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 340 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 341 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 342 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
            "| 에폭 343 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 344 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 345 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 346 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 347 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 348 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 349 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 350 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 351 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 352 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 353 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
            "| 에폭 354 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 355 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
            "| 에폭 356 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 357 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 358 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 359 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 360 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 361 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 362 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 363 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 364 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
            "| 에폭 365 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 366 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 367 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 368 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 369 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 370 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 371 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
            "| 에폭 372 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 373 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
            "| 에폭 374 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 375 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 376 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 377 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 378 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 379 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 380 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 381 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 382 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 383 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 384 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 385 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 386 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 387 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
            "| 에폭 388 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 389 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 390 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 391 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 392 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 393 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 394 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 395 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 396 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 397 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 398 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 399 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 400 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 401 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 402 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 403 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 404 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 405 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 406 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 407 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 408 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 409 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 410 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 411 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 412 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 413 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 414 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 415 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 416 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 417 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 418 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 419 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 420 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 421 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 422 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 423 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 424 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 425 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 426 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 427 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 428 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 429 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 430 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 431 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 432 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
            "| 에폭 433 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 434 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 435 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 436 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 437 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 438 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 439 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 440 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 441 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
            "| 에폭 442 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 443 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 444 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 445 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 446 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 447 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 448 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 449 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 450 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 451 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 452 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 453 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 454 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 455 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 456 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 457 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 458 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 459 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 460 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 461 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 462 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 463 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
            "| 에폭 464 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 465 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 466 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 467 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 468 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 469 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 470 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 471 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 472 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 473 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 474 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 475 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 476 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 477 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 478 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 479 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 480 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 481 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 482 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 483 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 484 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 485 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 486 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 487 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 488 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 489 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 490 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 491 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 492 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 493 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 494 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 495 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 496 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 497 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 498 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 499 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 500 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 501 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 502 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 503 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 504 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 505 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
            "| 에폭 506 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 507 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 508 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
            "| 에폭 509 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 510 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 511 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 512 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 513 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 514 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 515 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 516 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 517 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 518 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 519 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 520 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 521 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 522 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 523 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 524 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 525 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 526 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 527 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 528 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 529 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 530 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 531 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 532 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 533 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 534 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 535 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 536 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 537 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 538 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 539 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 540 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 541 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 542 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 543 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 544 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 545 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 546 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 547 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 548 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 549 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 550 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 551 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 552 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 553 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 554 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 555 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 556 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 557 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 558 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 559 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 560 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 561 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 562 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 563 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 564 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 565 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 566 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 567 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 568 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 569 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 570 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 571 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 572 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 573 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 574 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 575 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 576 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 577 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 578 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 579 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 580 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 581 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 582 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 583 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 584 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
            "| 에폭 585 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 586 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 587 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 588 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 589 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 590 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 591 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 592 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 593 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 594 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 595 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 596 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 597 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 598 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 599 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 600 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 601 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 602 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 603 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 604 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 605 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 606 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 607 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 608 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 609 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 610 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 611 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 612 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 613 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 614 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 615 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 616 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 617 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 618 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 619 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 620 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
            "| 에폭 621 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 622 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 623 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 624 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 625 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 626 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 627 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 628 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 629 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 630 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 631 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 632 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 633 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 634 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 635 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 636 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 637 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 638 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
            "| 에폭 639 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 640 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 641 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 642 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 643 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 644 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 645 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 646 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 647 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 648 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 649 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 650 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 651 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 652 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 653 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 654 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 655 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 656 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 657 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 658 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 659 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 660 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 661 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 662 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 663 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 664 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 665 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
            "| 에폭 666 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 667 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 668 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 669 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 670 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
            "| 에폭 671 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 672 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 673 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 674 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 675 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 676 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 677 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 678 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 679 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 680 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 681 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 682 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 683 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 684 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 685 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 686 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 687 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 688 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 689 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 690 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 691 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 692 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 693 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 694 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 695 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 696 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 697 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 698 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 699 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 700 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 701 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 702 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 703 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 704 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 705 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 706 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 707 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 708 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
            "| 에폭 709 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 710 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 711 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 712 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 713 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 714 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 715 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 716 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 717 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 718 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 719 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 720 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
            "| 에폭 721 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 722 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 723 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 724 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 725 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 726 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 727 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 728 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 729 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 730 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 731 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 732 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 733 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 734 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 735 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 736 |  반복 1 / 2 | 시간 0[s] | 손실 0.41\n",
            "| 에폭 737 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 738 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 739 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 740 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 741 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 742 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 743 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 744 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 745 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 746 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 747 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 748 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 749 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
            "| 에폭 750 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 751 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 752 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 753 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 754 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
            "| 에폭 755 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 756 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
            "| 에폭 757 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 758 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 759 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 760 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 761 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 762 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 763 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 764 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 765 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 766 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
            "| 에폭 767 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 768 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 769 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
            "| 에폭 770 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 771 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 772 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 773 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 774 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 775 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
            "| 에폭 776 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 777 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 778 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 779 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
            "| 에폭 780 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 781 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 782 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 783 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 784 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 785 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 786 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 787 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 788 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
            "| 에폭 789 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 790 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 791 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
            "| 에폭 792 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 793 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 794 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 795 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 796 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
            "| 에폭 797 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 798 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
            "| 에폭 799 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 800 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 801 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
            "| 에폭 802 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 803 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 804 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
            "| 에폭 805 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 806 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 807 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 808 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 809 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 810 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 811 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 812 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 813 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 814 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 815 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 816 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 817 |  반복 1 / 2 | 시간 0[s] | 손실 0.41\n",
            "| 에폭 818 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 819 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 820 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
            "| 에폭 821 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 822 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 823 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 824 |  반복 1 / 2 | 시간 0[s] | 손실 0.41\n",
            "| 에폭 825 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 826 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 827 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 828 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 829 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 830 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 831 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 832 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 833 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 834 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
            "| 에폭 835 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 836 |  반복 1 / 2 | 시간 0[s] | 손실 0.38\n",
            "| 에폭 837 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 838 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 839 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 840 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 841 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 842 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 843 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 844 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 845 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 846 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 847 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 848 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
            "| 에폭 849 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 850 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 851 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 852 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
            "| 에폭 853 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 854 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
            "| 에폭 855 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 856 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 857 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 858 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 859 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 860 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 861 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 862 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 863 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 864 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 865 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 866 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 867 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 868 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
            "| 에폭 869 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 870 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 871 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 872 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
            "| 에폭 873 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 874 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 875 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 876 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 877 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 878 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 879 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 880 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 881 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 882 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
            "| 에폭 883 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 884 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 885 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
            "| 에폭 886 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 887 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 888 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 889 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 890 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
            "| 에폭 891 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 892 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 893 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 894 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 895 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 896 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 897 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
            "| 에폭 898 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 899 |  반복 1 / 2 | 시간 0[s] | 손실 0.41\n",
            "| 에폭 900 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 901 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 902 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
            "| 에폭 903 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 904 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 905 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 906 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
            "| 에폭 907 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 908 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
            "| 에폭 909 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 910 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 911 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
            "| 에폭 912 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 913 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 914 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 915 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
            "| 에폭 916 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 917 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 918 |  반복 1 / 2 | 시간 0[s] | 손실 0.35\n",
            "| 에폭 919 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 920 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 921 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
            "| 에폭 922 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 923 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 924 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 925 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 926 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 927 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
            "| 에폭 928 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 929 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
            "| 에폭 930 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 931 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 932 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 933 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
            "| 에폭 934 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 935 |  반복 1 / 2 | 시간 0[s] | 손실 0.35\n",
            "| 에폭 936 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 937 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 938 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 939 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 940 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 941 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 942 |  반복 1 / 2 | 시간 0[s] | 손실 0.41\n",
            "| 에폭 943 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 944 |  반복 1 / 2 | 시간 0[s] | 손실 0.41\n",
            "| 에폭 945 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 946 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 947 |  반복 1 / 2 | 시간 0[s] | 손실 0.41\n",
            "| 에폭 948 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 949 |  반복 1 / 2 | 시간 0[s] | 손실 0.41\n",
            "| 에폭 950 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 951 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 952 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 953 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 954 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
            "| 에폭 955 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 956 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 957 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 958 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 959 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 960 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 961 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 962 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 963 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 964 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 965 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
            "| 에폭 966 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
            "| 에폭 967 |  반복 1 / 2 | 시간 1[s] | 손실 0.76\n",
            "| 에폭 968 |  반복 1 / 2 | 시간 1[s] | 손실 0.27\n",
            "| 에폭 969 |  반복 1 / 2 | 시간 1[s] | 손실 0.80\n",
            "| 에폭 970 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
            "| 에폭 971 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
            "| 에폭 972 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
            "| 에폭 973 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
            "| 에폭 974 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
            "| 에폭 975 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
            "| 에폭 976 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
            "| 에폭 977 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
            "| 에폭 978 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 979 |  반복 1 / 2 | 시간 1[s] | 손실 0.44\n",
            "| 에폭 980 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
            "| 에폭 981 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
            "| 에폭 982 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
            "| 에폭 983 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
            "| 에폭 984 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
            "| 에폭 985 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
            "| 에폭 986 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
            "| 에폭 987 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
            "| 에폭 988 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
            "| 에폭 989 |  반복 1 / 2 | 시간 1[s] | 손실 0.40\n",
            "| 에폭 990 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
            "| 에폭 991 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
            "| 에폭 992 |  반복 1 / 2 | 시간 1[s] | 손실 0.45\n",
            "| 에폭 993 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
            "| 에폭 994 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
            "| 에폭 995 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
            "| 에폭 996 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
            "| 에폭 997 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
            "| 에폭 998 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
            "| 에폭 999 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
            "| 에폭 1000 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48373 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49552 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49892 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48373 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49552 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49892 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwZMWb0P2H8v",
        "outputId": "2547534c-1e9a-472b-bde3-6afca65db076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "word_vecs = model.word_vecs\n",
        "for word_id, word in id_to_word.items():\n",
        "  print(word, word_vecs[word_id])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "you [-0.6887224  1.1797768  1.1360782 -1.383061  -1.1102952]\n",
            "say [ 1.1622263  -1.1165427  -1.1491416  -0.16698028  1.139369  ]\n",
            "goodbye [-1.1734868   0.7112666   0.7624399  -0.62024933 -0.74520564]\n",
            "and [ 0.8489317  -0.8212782  -0.79062176 -1.9364924   0.818058  ]\n",
            "i [-1.1527313   0.6998706   0.7475278  -0.61636424 -0.7317284 ]\n",
            "hello [-0.69144654  1.1967703   1.1260719  -1.3723357  -1.1221762 ]\n",
            ". [ 1.1347976 -1.1420269 -1.176498   1.6596682  1.1989195]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4EizYY_H-A7"
      },
      "source": [
        "## 4장 - word2vec 속도 개선"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUpmFACY2_yf",
        "outputId": "eb8a1576-d7b5-41eb-a1ff-bbb4b129204a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "import numpy as np\n",
        "W = np.arange(21).reshape(7, 3)\n",
        "W"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  1,  2],\n",
              "       [ 3,  4,  5],\n",
              "       [ 6,  7,  8],\n",
              "       [ 9, 10, 11],\n",
              "       [12, 13, 14],\n",
              "       [15, 16, 17],\n",
              "       [18, 19, 20]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM1DpOcKLl4u",
        "outputId": "eaa2c633-d4a5-438e-d5ac-a2aacf33ab55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(W[2]) ; W[5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6 7 8]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([15, 16, 17])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFWLYnX6Lny1",
        "outputId": "9fd7bea0-6382-4d58-f203-d34cfe9ebf4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "idx = np.array([1, 0, 3, 0])\n",
        "W[idx]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3,  4,  5],\n",
              "       [ 0,  1,  2],\n",
              "       [ 9, 10, 11],\n",
              "       [ 0,  1,  2]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvnWN3-tLwEt"
      },
      "source": [
        "class Embedding:\n",
        "  def __init__(self, W):\n",
        "    self.params = [W]\n",
        "    self.grads = [np.zeros_like(W)]\n",
        "    self.idx = None\n",
        "\n",
        "  def forward(self, idx):\n",
        "    W, = self.params # , 가 뭔지 잘 모름\n",
        "    self.idx = idx\n",
        "    out = W[idx]\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dW, = self.grads\n",
        "    dW[...] = 0\n",
        "    np.add.at(dW, self.idx, dout)\n",
        "    return None\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek04yVlLPP4M",
        "outputId": "c6c4e311-d648-4f86-ea38-8870dcfe4c8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# 나쁜예\n",
        "'''\n",
        "def backward(self, dout):\n",
        "  dW, = self.grads\n",
        "  dW[...] = 0  # dW 의 배열의 원소를 전부 0으로 만듬.\n",
        "  dW[self.idx] = dout\n",
        "  return None \n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef backward(self, dout):\\n  dW, = self.grads\\n  dW[...] = 0  # dW 의 배열의 원소를 전부 0으로 만듬.\\n  dW[self.idx] = dout\\n  return None \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2aGxI4wP3Ar"
      },
      "source": [
        "# 개선\n",
        "def backward(self, dout):\n",
        "  dW, = self.grads\n",
        "  dW[...] = 0\n",
        "\n",
        "  for i, word_id in enumerate(self.idx): # i에는 인덱스 , word_id 에는 실제 값이 할당됨.\n",
        "    dW[word_id] += dout[i] # 왜 더하기인지는 각자 생각해보자.\n",
        "\n",
        "  # 혹은\n",
        "  # np.add.at(dW, self.idx, dout)\n",
        "  # 일반적으로 파이썬에서 for 문보다는 넘파이의 내장 메서드를 사용하는 편이 더 빠르기에 효율을 높이려면 위 코드를 사용해야함.\n",
        "\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbFtF9swQyE0"
      },
      "source": [
        "class EmbeddingDot:\n",
        "  def __init__(self, W):\n",
        "    self.embed = Embedding(W)\n",
        "    self.params = self.embed.params\n",
        "    self.grads = self.embed.grads\n",
        "    self.cache = None\n",
        "\n",
        "  def forward(self, h, idx):\n",
        "    target_W = self.embed.forward(idx) # 임베딩 한 행렬들 합침.\n",
        "    out = np.sum(target_W * h, axis = 1) # 내적 계산\n",
        "\n",
        "    self.cache = (h, target_W)\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    h, target_W = self.cache\n",
        "    dout = dout.reshape(dout.shape[0], 1)\n",
        "\n",
        "    dtarget_W = dout * h # + 방향으로 가는거\n",
        "    self.embed.backward(dtarget_W) # 여기서는 사실 반환값이 없기 때문에 안봐도 될듯.\n",
        "    dh = dout * target_W # 0.5방향으로 가는거\n",
        "    return dh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAWrPYv3ZuQp",
        "outputId": "00403934-130f-4bfb-d41e-6c5fd73b9e8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# 내적 계산의 구체적인 예\n",
        "W = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11], [12, 13 ,14], [15, 16 ,17], [18, 19, 20]])\n",
        "idx = np.array([0, 3 ,1])\n",
        "h = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n",
        "embed = Embedding(W)\n",
        "target_W = embed.forward(idx)\n",
        "print(target_W)\n",
        "out = np.sum(target_W * h, axis = 1) # 행 단위로\n",
        "print()\n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  1  2]\n",
            " [ 9 10 11]\n",
            " [ 3  4  5]]\n",
            "\n",
            "[  5 122  86]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43liDMd0HLgt",
        "outputId": "a61f6726-13dc-4357-d37c-a3fe46d16aaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "# 네거티브 샘플링에서 어떻게 부정적 예를 선택하는가?\n",
        "# 임의로 뽑는 것 보다 좋은 방법이 있음. 그것은 출현 빈도가 높은 단어들을 많이, 낮은 단어들을 조금 추출하는 것.\n",
        "# 그래서 통계분포를 이용해서 추출할 예정임.\n",
        "import numpy as np\n",
        "\n",
        "# 0에서 9까지의 숫자 중 하나를 무작위로 샘플링\n",
        "print(np.random.choice(10))\n",
        "print(np.random.choice(10))\n",
        "\n",
        "# words에서 하나만 무작위로 샘플링\n",
        "words = ['you', 'say', 'goodbye', 'I', 'hello', '.']\n",
        "print(np.random.choice(words))\n",
        "\n",
        "# 5개만 무작위로 샘플링(중복 있음)\n",
        "print(np.random.choice(words, size = 5))\n",
        "\n",
        "# 5개만 무작위로 샘플링(중복 없음)\n",
        "print(np.random.choice(words, size=5, replace=False))\n",
        "\n",
        "# 확률분포에 따라 샘플링\n",
        "p = [0.5, 0.1, 0.05, 0.2, 0.05, 0.1]\n",
        "np.random.choice(words, p=p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "6\n",
            "say\n",
            "['.' 'I' 'say' '.' 'say']\n",
            "['.' 'hello' 'you' 'I' 'say']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'you'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHEDnPubJNiR",
        "outputId": "3320f390-c741-48a8-e8ef-3b30b670e989",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 0.75 제곱을 하면서 출현 확률이 낮은 단어를 버리지 않기위함.\n",
        "# 원래 확률이 낮은 단어의 확률을 살짝 높일 수 있음\n",
        "p = [0.7, 0.29, 0.01]\n",
        "new_p = np.power(p, 0.75) # 0.75 제곱\n",
        "new_p /= np.sum(new_p) # 0.75 제곱한것들의 총합으로 나눠줌\n",
        "print(new_p) # 결과값을 보면 출현 확률이 낮은 단어가 확률이 더 올라가고 높은 단어는 확률이 조금 내려감."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.64196878 0.33150408 0.02652714]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYzj_ccEKBZp",
        "outputId": "e4febbd1-a7f3-479a-bf25-b882521855de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import sys, os\n",
        "import numpy as np\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master/ch04')\n",
        "sys.path.append(os.pardir)\n",
        "from negative_sampling_layer import UnigramSampler\n",
        "corpus = np.array([0, 1, 2, 3, 4, 1, 2, 3])\n",
        "power = 0.75\n",
        "sample_size = 2 # 네거티브 샘플링할 원소 갯수\n",
        "\n",
        "sampler = UnigramSampler(corpus, power, sample_size) \n",
        "target = np.array([1, 3, 0]) # 긍정적인 예\n",
        "negative_sample = sampler.get_negative_sample(target)\n",
        "print(negative_sample)\n",
        "\n",
        "\n",
        "# [1, 3, 0] 이 긍정적 예임.\n",
        "# 여기서 1,2,3번째 행렬을 보면 긍정적 예와 겹치지 않는 행렬들이 나옴\n",
        "# 1 =/= [0, 4]\n",
        "# 2 =/= [4, 0]\n",
        "# 3 =/= [4, 2]  답이 달라질 수도 있음. 따라서 네거티브 샘플링이 가능해짐."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 4]\n",
            " [4 0]\n",
            " [4 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIrZszs7KuPR"
      },
      "source": [
        "class NegativeSamplingLoss:\n",
        "  def __init__(self, W, corpus, power = 0.75, sample_size = 5):\n",
        "    # 가중치를 나타내는 W, 말뭉치를 뜻하는 corpus, 확률분포에 제곱할 값인 power, 부정적 예의 샘플링 횟수인 sample_size\n",
        "    self.sample_size = sample_size\n",
        "    self.sampler = UnigramSampler(corpus, power, sample_size)\n",
        "    '''\n",
        "    loss_layers와 embed_dot_layers에는 원하는 계층을 리스트로 보관함\n",
        "    크기가 +1인 이유는 긍정적 예를 다루는 계층이 하나 더 필요학디 때문\n",
        "    즉, loss_layer[0] , embed_dot_layers[0]이 긍정적 예를 다루는 계층임.\n",
        "    '''\n",
        "    self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)] \n",
        "    self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
        "    self.params, self.grads =[], []\n",
        "    for layer in self.embed_dot_layers:\n",
        "      self.params += layer.params\n",
        "      self.grads += layer.grads\n",
        "  \n",
        "  # 순전파\n",
        "  def forward(self, h, target):\n",
        "    batch_size = target.shape[0]\n",
        "    negative_sample = self.sampler.get_negative_sample(target)\n",
        "\n",
        "    # 긍정적 예 순전파\n",
        "    score = self.embed_dot_layers[0].forward(h, target)\n",
        "    correct_label = np.ones(batch_size, dtype=np.int32)\n",
        "    loss = self.loss_layers[0].forward(score, correct_label)\n",
        "\n",
        "    # 부정적 예 순전파\n",
        "    negative_label = np.zeros(batch_size, dtype=np.int32)\n",
        "    for i in range(self.sample_size):\n",
        "        negative_target = negative_sample[:, i]\n",
        "        score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
        "        loss += self.loss_layers[1 + i].forward(score, negative_label)\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def backward(self, dout=1):\n",
        "    dh = 0\n",
        "    for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
        "        dscore = l0.backward(dout)\n",
        "        dh += l1.backward(dscore)\n",
        "\n",
        "    return dh\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MASvbOLINE2Y"
      },
      "source": [
        "import sys, os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master')\n",
        "sys.path.append('..')\n",
        "from common.np import *  # import numpy as np\n",
        "from common.layers import Embedding\n",
        "from ch04.negative_sampling_layer import NegativeSamplingLoss\n",
        "\n",
        "\n",
        "class CBOW:\n",
        "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
        "        V, H = vocab_size, hidden_size\n",
        "\n",
        "        # 가중치 초기화\n",
        "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
        "        W_out = 0.01 * np.random.randn(V, H).astype('f')\n",
        "\n",
        "        # 계층 생성\n",
        "        self.in_layers = []\n",
        "        for i in range(2 * window_size):\n",
        "            layer = Embedding(W_in)  # Embedding 계층 사용\n",
        "            self.in_layers.append(layer)\n",
        "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
        "\n",
        "        # 모든 가중치와 기울기를 배열에 모은다.\n",
        "        layers = self.in_layers + [self.ns_loss]\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
        "        self.word_vecs = W_in\n",
        "\n",
        "    def forward(self, contexts, target):\n",
        "        h = 0\n",
        "        for i, layer in enumerate(self.in_layers):\n",
        "            h += layer.forward(contexts[:, i])\n",
        "        h *= 1 / len(self.in_layers)\n",
        "        loss = self.ns_loss.forward(h, target)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.ns_loss.backward(dout)\n",
        "        dout *= 1 / len(self.in_layers)\n",
        "        for layer in self.in_layers:\n",
        "            layer.backward(dout) # layer = Embedding(W_in)\n",
        "        return None\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rxSA1xK4Myw",
        "outputId": "1c155174-6812-402d-82d9-6dd4a85ba56a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.util import most_similar, analogy\n",
        "import pickle\n",
        "\n",
        "\n",
        "\n",
        "pkl_file = '/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master/ch04/cbow_params.pkl'\n",
        "# pkl_file = 'skipgram_params.pkl'\n",
        "\n",
        "with open(pkl_file, 'rb') as f:\n",
        "    params = pickle.load(f)\n",
        "    word_vecs = params['word_vecs']\n",
        "    word_to_id = params['word_to_id']\n",
        "    id_to_word = params['id_to_word']\n",
        "\n",
        "# 가장 비슷한(most similar) 단어 뽑기\n",
        "querys = ['you', 'year', 'car', 'toyota']\n",
        "for query in querys:\n",
        "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)\n",
        "\n",
        "# 유추(analogy) 작업\n",
        "print('-'*50)\n",
        "analogy('king', 'man', 'queen',  word_to_id, id_to_word, word_vecs)\n",
        "analogy('take', 'took', 'go',  word_to_id, id_to_word, word_vecs)\n",
        "analogy('car', 'cars', 'child',  word_to_id, id_to_word, word_vecs)\n",
        "analogy('good', 'better', 'bad',  word_to_id, id_to_word, word_vecs)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[query] you\n",
            " we: 0.6103515625\n",
            " someone: 0.59130859375\n",
            " i: 0.55419921875\n",
            " something: 0.48974609375\n",
            " anyone: 0.47314453125\n",
            "\n",
            "[query] year\n",
            " month: 0.71875\n",
            " week: 0.65234375\n",
            " spring: 0.62744140625\n",
            " summer: 0.6259765625\n",
            " decade: 0.603515625\n",
            "\n",
            "[query] car\n",
            " luxury: 0.497314453125\n",
            " arabia: 0.47802734375\n",
            " auto: 0.47119140625\n",
            " disk-drive: 0.450927734375\n",
            " travel: 0.4091796875\n",
            "\n",
            "[query] toyota\n",
            " ford: 0.55078125\n",
            " instrumentation: 0.509765625\n",
            " mazda: 0.49365234375\n",
            " bethlehem: 0.47509765625\n",
            " nissan: 0.474853515625\n",
            "--------------------------------------------------\n",
            "\n",
            "[analogy] king:man = queen:?\n",
            " woman: 5.16015625\n",
            " veto: 4.9296875\n",
            " ounce: 4.69140625\n",
            " earthquake: 4.6328125\n",
            " successor: 4.609375\n",
            "\n",
            "[analogy] take:took = go:?\n",
            " went: 4.55078125\n",
            " points: 4.25\n",
            " began: 4.09375\n",
            " comes: 3.98046875\n",
            " oct.: 3.90625\n",
            "\n",
            "[analogy] car:cars = child:?\n",
            " children: 5.21875\n",
            " average: 4.7265625\n",
            " yield: 4.20703125\n",
            " cattle: 4.1875\n",
            " priced: 4.1796875\n",
            "\n",
            "[analogy] good:better = bad:?\n",
            " more: 6.6484375\n",
            " less: 6.0625\n",
            " rather: 5.21875\n",
            " slower: 4.734375\n",
            " greater: 4.671875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS4DzCTFFdxm"
      },
      "source": [
        "CBOW 모델은 실행이 안되는 관계로 생략."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux-B6BHdFlI0"
      },
      "source": [
        "## 5장 - 순환 신경망(RNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_ZgpayKWhuv"
      },
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from common.time_layers import *\n",
        "\n",
        "\n",
        "class SimpleRnnlm:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        # 가중치 초기화\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        rnn_Wx = (rn(D, H) / np.sqrt(D)).astype('f')\n",
        "        rnn_Wh = (rn(H, H) / np.sqrt(H)).astype('f')\n",
        "        rnn_b = np.zeros(H).astype('f')\n",
        "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = [\n",
        "            TimeEmbedding(embed_W),\n",
        "            TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True),\n",
        "            TimeAffine(affine_W, affine_b)\n",
        "        ]\n",
        "        self.loss_layer = TimeSoftmaxWithLoss()\n",
        "        self.rnn_layer = self.layers[1]\n",
        "\n",
        "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        for layer in self.layers:\n",
        "            xs = layer.forward(xs)\n",
        "        loss = self.loss_layer.forward(xs, ts)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.loss_layer.backward(dout)\n",
        "        for layer in reversed(self.layers):\n",
        "            dout = layer.backward(dout)\n",
        "        return dout\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.rnn_layer.reset_state()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIXpOC2MmTGX"
      },
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from common.time_layers import *\n",
        "\n",
        "\n",
        "class SimpleRnnlm:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        # 가중치 초기화\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        rnn_Wx = (rn(D, H) / np.sqrt(D)).astype('f')\n",
        "        rnn_Wh = (rn(H, H) / np.sqrt(H)).astype('f')\n",
        "        rnn_b = np.zeros(H).astype('f')\n",
        "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = [\n",
        "            TimeEmbedding(embed_W),\n",
        "            TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True),\n",
        "            TimeAffine(affine_W, affine_b)\n",
        "        ]\n",
        "        self.loss_layer = TimeSoftmaxWithLoss()\n",
        "        self.rnn_layer = self.layers[1]\n",
        "\n",
        "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        for layer in self.layers:\n",
        "            xs = layer.forward(xs)\n",
        "        loss = self.loss_layer.forward(xs, ts)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.loss_layer.backward(dout)\n",
        "        for layer in reversed(self.layers):\n",
        "            dout = layer.backward(dout)\n",
        "        return dout\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.rnn_layer.reset_state()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBz8V2GQZq-n"
      },
      "source": [
        "'''\n",
        "퍼플렉서티는 확률의 역수이기 때문에 값이 작을수록 좋음. (최솟값 1.0)\n",
        "퍼플렉서티의 값은 어떻게 해석되는가? => 분기 수 개념으로 해석\n",
        "예) 분기 수 = 1.25 ==> 다음에 출현할 수 있는 단어의 후보가 1.25개\n",
        "e^(L) L은 교차 엔트로피 오차의 출력.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZS-jMGL2iGw",
        "outputId": "220147b6-e99b-4711-9be8-6e27abc28c51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append('..')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from common.optimizer import SGD\n",
        "from dataset import ptb\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "batch_size = 10\n",
        "wordvec_size = 100\n",
        "hidden_size = 100 # RNN의 은닉 상태 벡터의 원소 수\n",
        "time_size = 5     # Truncated BPTT가 한 번에 펼치는 시간 크기\n",
        "lr = 0.1\n",
        "max_epoch = 100\n",
        "\n",
        "# 학습 데이터 읽기(전체 중 1000개만)\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "corpus_size = 1000\n",
        "corpus = corpus[:corpus_size]\n",
        "vocab_size = int(max(corpus) + 1)\n",
        "\n",
        "xs = corpus[:-1]  # 입력\n",
        "ts = corpus[1:]   # 출력(정답 레이블)\n",
        "data_size = len(xs)\n",
        "print('말뭉치 크기: %d, 어휘 수: %d' % (corpus_size, vocab_size))\n",
        "\n",
        "# 학습 시 사용하는 변수\n",
        "max_iters = data_size // (batch_size * time_size)\n",
        "time_idx = 0\n",
        "total_loss = 0\n",
        "loss_count = 0\n",
        "ppl_list = []\n",
        "\n",
        "# 모델 생성\n",
        "model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)\n",
        "optimizer = SGD(lr)\n",
        "\n",
        "# 미니배치의 각 샘플의 읽기 시작 위치를 계산\n",
        "jump = (corpus_size - 1) // batch_size\n",
        "offsets = [i * jump for i in range(batch_size)]\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "    for iter in range(max_iters):\n",
        "        # 미니배치 취득\n",
        "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
        "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
        "        for t in range(time_size):\n",
        "            for i, offset in enumerate(offsets):\n",
        "                batch_x[i, t] = xs[(offset + time_idx) % data_size]\n",
        "                batch_t[i, t] = ts[(offset + time_idx) % data_size]\n",
        "            time_idx += 1\n",
        "\n",
        "        # 기울기를 구하여 매개변수 갱신\n",
        "        loss = model.forward(batch_x, batch_t)\n",
        "        model.backward()\n",
        "        optimizer.update(model.params, model.grads)\n",
        "        total_loss += loss\n",
        "        loss_count += 1\n",
        "\n",
        "    # 에폭마다 퍼플렉서티 평가\n",
        "    ppl = np.exp(total_loss / loss_count)\n",
        "    print('| 에폭 %d | 퍼플렉서티 %.2f'\n",
        "          % (epoch+1, ppl))\n",
        "    ppl_list.append(float(ppl))\n",
        "    total_loss, loss_count = 0, 0\n",
        "\n",
        "# 그래프 그리기\n",
        "x = np.arange(len(ppl_list))\n",
        "plt.plot(x, ppl_list, label='train')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('perplexity')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "말뭉치 크기: 1000, 어휘 수: 418\n",
            "| 에폭 1 | 퍼플렉서티 405.12\n",
            "| 에폭 2 | 퍼플렉서티 308.23\n",
            "| 에폭 3 | 퍼플렉서티 237.76\n",
            "| 에폭 4 | 퍼플렉서티 223.79\n",
            "| 에폭 5 | 퍼플렉서티 210.88\n",
            "| 에폭 6 | 퍼플렉서티 205.67\n",
            "| 에폭 7 | 퍼플렉서티 200.95\n",
            "| 에폭 8 | 퍼플렉서티 198.14\n",
            "| 에폭 9 | 퍼플렉서티 193.28\n",
            "| 에폭 10 | 퍼플렉서티 193.72\n",
            "| 에폭 11 | 퍼플렉서티 189.06\n",
            "| 에폭 12 | 퍼플렉서티 192.47\n",
            "| 에폭 13 | 퍼플렉서티 190.19\n",
            "| 에폭 14 | 퍼플렉서티 190.32\n",
            "| 에폭 15 | 퍼플렉서티 189.12\n",
            "| 에폭 16 | 퍼플렉서티 185.17\n",
            "| 에폭 17 | 퍼플렉서티 183.49\n",
            "| 에폭 18 | 퍼플렉서티 180.30\n",
            "| 에폭 19 | 퍼플렉서티 180.65\n",
            "| 에폭 20 | 퍼플렉서티 181.25\n",
            "| 에폭 21 | 퍼플렉서티 178.86\n",
            "| 에폭 22 | 퍼플렉서티 174.80\n",
            "| 에폭 23 | 퍼플렉서티 171.92\n",
            "| 에폭 24 | 퍼플렉서티 172.81\n",
            "| 에폭 25 | 퍼플렉서티 169.59\n",
            "| 에폭 26 | 퍼플렉서티 168.00\n",
            "| 에폭 27 | 퍼플렉서티 162.61\n",
            "| 에폭 28 | 퍼플렉서티 159.80\n",
            "| 에폭 29 | 퍼플렉서티 160.21\n",
            "| 에폭 30 | 퍼플렉서티 151.77\n",
            "| 에폭 31 | 퍼플렉서티 150.76\n",
            "| 에폭 32 | 퍼플렉서티 146.29\n",
            "| 에폭 33 | 퍼플렉서티 143.97\n",
            "| 에폭 34 | 퍼플렉서티 140.15\n",
            "| 에폭 35 | 퍼플렉서티 138.33\n",
            "| 에폭 36 | 퍼플렉서티 132.47\n",
            "| 에폭 37 | 퍼플렉서티 127.35\n",
            "| 에폭 38 | 퍼플렉서티 126.24\n",
            "| 에폭 39 | 퍼플렉서티 117.49\n",
            "| 에폭 40 | 퍼플렉서티 114.00\n",
            "| 에폭 41 | 퍼플렉서티 115.12\n",
            "| 에폭 42 | 퍼플렉서티 106.97\n",
            "| 에폭 43 | 퍼플렉서티 102.75\n",
            "| 에폭 44 | 퍼플렉서티 97.29\n",
            "| 에폭 45 | 퍼플렉서티 95.67\n",
            "| 에폭 46 | 퍼플렉서티 93.78\n",
            "| 에폭 47 | 퍼플렉서티 87.90\n",
            "| 에폭 48 | 퍼플렉서티 82.30\n",
            "| 에폭 49 | 퍼플렉서티 79.46\n",
            "| 에폭 50 | 퍼플렉서티 76.61\n",
            "| 에폭 51 | 퍼플렉서티 71.60\n",
            "| 에폭 52 | 퍼플렉서티 69.24\n",
            "| 에폭 53 | 퍼플렉서티 64.03\n",
            "| 에폭 54 | 퍼플렉서티 61.63\n",
            "| 에폭 55 | 퍼플렉서티 58.78\n",
            "| 에폭 56 | 퍼플렉서티 53.29\n",
            "| 에폭 57 | 퍼플렉서티 52.77\n",
            "| 에폭 58 | 퍼플렉서티 49.35\n",
            "| 에폭 59 | 퍼플렉서티 45.93\n",
            "| 에폭 60 | 퍼플렉서티 42.78\n",
            "| 에폭 61 | 퍼플렉서티 41.57\n",
            "| 에폭 62 | 퍼플렉서티 38.78\n",
            "| 에폭 63 | 퍼플렉서티 35.36\n",
            "| 에폭 64 | 퍼플렉서티 34.38\n",
            "| 에폭 65 | 퍼플렉서티 33.08\n",
            "| 에폭 66 | 퍼플렉서티 30.49\n",
            "| 에폭 67 | 퍼플렉서티 29.10\n",
            "| 에폭 68 | 퍼플렉서티 26.99\n",
            "| 에폭 69 | 퍼플렉서티 25.72\n",
            "| 에폭 70 | 퍼플렉서티 24.13\n",
            "| 에폭 71 | 퍼플렉서티 22.78\n",
            "| 에폭 72 | 퍼플렉서티 21.37\n",
            "| 에폭 73 | 퍼플렉서티 20.82\n",
            "| 에폭 74 | 퍼플렉서티 19.38\n",
            "| 에폭 75 | 퍼플렉서티 18.41\n",
            "| 에폭 76 | 퍼플렉서티 17.36\n",
            "| 에폭 77 | 퍼플렉서티 15.83\n",
            "| 에폭 78 | 퍼플렉서티 15.10\n",
            "| 에폭 79 | 퍼플렉서티 14.60\n",
            "| 에폭 80 | 퍼플렉서티 13.68\n",
            "| 에폭 81 | 퍼플렉서티 13.07\n",
            "| 에폭 82 | 퍼플렉서티 12.72\n",
            "| 에폭 83 | 퍼플렉서티 12.05\n",
            "| 에폭 84 | 퍼플렉서티 10.89\n",
            "| 에폭 85 | 퍼플렉서티 10.53\n",
            "| 에폭 86 | 퍼플렉서티 9.86\n",
            "| 에폭 87 | 퍼플렉서티 9.94\n",
            "| 에폭 88 | 퍼플렉서티 8.95\n",
            "| 에폭 89 | 퍼플렉서티 8.21\n",
            "| 에폭 90 | 퍼플렉서티 7.88\n",
            "| 에폭 91 | 퍼플렉서티 7.41\n",
            "| 에폭 92 | 퍼플렉서티 7.17\n",
            "| 에폭 93 | 퍼플렉서티 6.93\n",
            "| 에폭 94 | 퍼플렉서티 6.62\n",
            "| 에폭 95 | 퍼플렉서티 6.31\n",
            "| 에폭 96 | 퍼플렉서티 6.16\n",
            "| 에폭 97 | 퍼플렉서티 5.60\n",
            "| 에폭 98 | 퍼플렉서티 5.61\n",
            "| 에폭 99 | 퍼플렉서티 5.32\n",
            "| 에폭 100 | 퍼플렉서티 5.12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHNOovCXll1J"
      },
      "source": [
        "# 6장 - 게이트가 추가된 RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7oBRiSwlraQ",
        "outputId": "ce9e6277-c32e-4207-e979-873f5a8e530c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "N = 2 # 미니배치 크기\n",
        "H = 3 # 은닉 상태 벡터의 차원 수\n",
        "T = 20 # 시계열 데이터의 길이\n",
        "\n",
        "dh = np.ones((N, H)) # 모든 원소가 1인 행렬을 반환\n",
        "np.random.seed(3) # 재현할 수 있도록 난수의 시드 고정\n",
        "Wh = np.random.randn(H, H)\n",
        "\n",
        "norm_list = []\n",
        "for t in range(T):\n",
        "  dh = np.matmul(dh, Wh.T)\n",
        "  norm = np.sqrt(np.sum(dh**2)) / N\n",
        "  norm_list.append(norm)\n",
        "\n",
        "plt.plot(np.arange(len(norm_list)), norm_list)\n",
        "plt.xticks([0, 4, 9, 14, 19], [1, 5, 10, 15, 20])\n",
        "plt.xlabel('시간 크기(time step)')\n",
        "plt.ylabel('노름(norm)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49884 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44036 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 53356 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44592 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45432 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47492 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49884 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44036 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 53356 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44592 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45432 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47492 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8tslHBbpJd6",
        "outputId": "ffa20bd6-071c-42d9-90f5-9b0916d3b594",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "N = 2 # 미니배치 크기\n",
        "H = 3 # 은닉 상태 벡터의 차원 수\n",
        "T = 20 # 시계열 데이터의 길이\n",
        "\n",
        "dh = np.ones((N, H)) # 모든 원소가 1인 행렬을 반환\n",
        "np.random.seed(3) # 재현할 수 있도록 난수의 시드 고정\n",
        "Wh = np.random.randn(H, H) * 0.5\n",
        "\n",
        "norm_list = []\n",
        "for t in range(T):\n",
        "  dh = np.matmul(dh, Wh.T)\n",
        "  norm = np.sqrt(np.sum(dh**2)) / N\n",
        "  norm_list.append(norm)\n",
        "\n",
        "plt.plot(np.arange(len(norm_list)), norm_list)\n",
        "plt.xticks([0, 4, 9, 14, 19], [1, 5, 10, 15, 20])\n",
        "plt.xlabel('시간 크기(time step)')\n",
        "plt.ylabel('노름(norm)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49884 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44036 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 53356 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44592 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45432 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47492 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49884 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44036 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 53356 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44592 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45432 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47492 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m7taI5bp-Tn",
        "outputId": "0ebb56e3-1aa5-4aa2-c7dd-33dddad86c08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "dW1 = np.random.rand(3, 3) * 10\n",
        "dW2 = np.random.rand(3, 3) * 10\n",
        "grads = [dW1, dW2]\n",
        "max_norm = 5.0\n",
        "\n",
        "def clip_grads(grads, max_norm):\n",
        "  total_norm = 0\n",
        "  for grad in grads:\n",
        "    total_norm += np.sum(grad**2)\n",
        "  total_norm = np.sqrt(total_norm)\n",
        "\n",
        "  rate = max_norm / (total_norm + 1e-6)\n",
        "  if rate < 1:\n",
        "    for grad in grads:\n",
        "      grad *= rate\n",
        "\n",
        "\n",
        "\n",
        "print('before:', dW1)\n",
        "clip_grads(grads, max_norm)\n",
        "print('after:', dW1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before: [[8.33687884 8.72179508 0.92131592]\n",
            " [2.15949471 8.3176109  8.48303897]\n",
            " [3.14652999 2.79294597 4.30815022]]\n",
            "after: [[1.59398002 1.66757456 0.17615215]\n",
            " [0.4128873  1.59029606 1.62192529]\n",
            " [0.60160475 0.53400081 0.82370219]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ob9cIxuupiz",
        "outputId": "4c7474e3-7e7a-4702-b6e5-dccf2c08d07a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "class LSTM:\n",
        "  def __init__(self, Wx, Wh, b):\n",
        "    self.params = [Wx, Wh, b]\n",
        "    self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "    self.cache = None\n",
        "\n",
        "  def forward(self, x, h_prev, c_prev):\n",
        "    Wx, Wh, b = self.params\n",
        "    N, H = h_prev.shape\n",
        "\n",
        "    A = np.matmul(x, Wx) + np.matmul(h_prev, Wh) + b\n",
        "\n",
        "    # slice\n",
        "    f = A[:, :H]\n",
        "    g = A[:, H:2*H]\n",
        "    i = A[:, 2*H:3*H]\n",
        "    o = A[:, 3*H:]\n",
        "\n",
        "    f = sigmoid(f)\n",
        "    g = np.tanh(g)\n",
        "    i = sigmoid(i)\n",
        "    o = sigmoid(o)\n",
        "\n",
        "    c_next = f * c_prev + g * i\n",
        "    h_next = o * np.tanh(c_next)\n",
        "\n",
        "    self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n",
        "    return h_next, c_next\n",
        "\n",
        "\n",
        "'''\n",
        "slice 노드의 역전파 \n",
        "dA = np.hstack((df, dg, di, do)) 가로로 연결한 합.\n",
        "slice 노드는 한 개를 여러개로 나눠주는 것이기 때문에 다시 다 더해주는게 끝임.\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nslice 노드의 역전파 \\ndA = np.hstack((df, dg, di, do)) 가로로 연결한 합.\\nslice 노드는 한 개를 여러개로 나눠주는 것이기 때문에 다시 다 더해주는게 끝임.\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-N2JkCEIv7m"
      },
      "source": [
        "class TimeLSTM:\n",
        "    def __init__(self, Wx, Wh, b, stateful=False):\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.layers = None\n",
        "\n",
        "        self.h, self.c = None, None\n",
        "        self.dh = None\n",
        "        self.stateful = stateful\n",
        "\n",
        "    def forward(self, xs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, D = xs.shape\n",
        "        H = Wh.shape[0]\n",
        "\n",
        "        self.layers = []\n",
        "        hs = np.empty((N, T, H), dtype='f')\n",
        "\n",
        "        if not self.stateful or self.h is None:\n",
        "            self.h = np.zeros((N, H), dtype='f')\n",
        "        if not self.stateful or self.c is None:\n",
        "            self.c = np.zeros((N, H), dtype='f')\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = LSTM(*self.params)\n",
        "            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)\n",
        "            hs[:, t, :] = self.h\n",
        "\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return hs\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, H = dhs.shape\n",
        "        D = Wx.shape[0]\n",
        "\n",
        "        dxs = np.empty((N, T, D), dtype='f')\n",
        "        dh, dc = 0, 0\n",
        "\n",
        "        grads = [0, 0, 0]\n",
        "        for t in reversed(range(T)):\n",
        "            layer = self.layers[t]\n",
        "            dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)\n",
        "            dxs[:, t, :] = dx\n",
        "            for i, grad in enumerate(layer.grads):\n",
        "                grads[i] += grad\n",
        "\n",
        "        for i, grad in enumerate(grads):\n",
        "            self.grads[i][...] = grad\n",
        "        self.dh = dh\n",
        "        return dxs\n",
        "\n",
        "    def set_state(self, h, c=None):\n",
        "        self.h, self.c = h, c\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.h, self.c = None, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LvDP2EHJSDn"
      },
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master\")\n",
        "sys.path.append('..')\n",
        "from common.time_layers import *\n",
        "from common.base_model import BaseModel\n",
        "\n",
        "# BaseModel 클래스에는 params() 와 load_params() 메서드가 구현되어 있음.\n",
        "class Rnnlm(BaseModel):\n",
        "    def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        # 가중치 초기화\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = [\n",
        "            TimeEmbedding(embed_W),\n",
        "            TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True),\n",
        "            TimeAffine(affine_W, affine_b)\n",
        "        ]\n",
        "        self.loss_layer = TimeSoftmaxWithLoss()\n",
        "        self.lstm_layer = self.layers[1]\n",
        "\n",
        "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def predict(self, xs):\n",
        "        for layer in self.layers:\n",
        "            xs = layer.forward(xs)\n",
        "        return xs\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        score = self.predict(xs)\n",
        "        loss = self.loss_layer.forward(score, ts)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.loss_layer.backward(dout)\n",
        "        for layer in reversed(self.layers):\n",
        "            dout = layer.backward(dout)\n",
        "        return dout\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.lstm_layer.reset_state()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAdYrHmwKNfn"
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.time_layers import *\n",
        "from common.np import *\n",
        "from common.base_model import BaseModel\n",
        "\n",
        "class BetterRnnlm(BaseModel):\n",
        "  def __init__(self, vocab_size =10000, wordvec_size = 650, hidden_size=  650, dropout_ratio = 0.5):\n",
        "    V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "    rn = np.random.randn\n",
        "\n",
        "    embed_W = (rn(V,D) / 100).astype('f')\n",
        "    lstm_Wx1 = (rn(D, 4 * H) / np.sqrt*(D)).astype('f')\n",
        "    lstm_Wh1 = (rn(H, 4 * H) / np.sqrt*(H)).astype('f')\n",
        "    lstm_b1 = np.zeros(4 * H).astype('f')\n",
        "    lstm_Wx2 = (rn(H, 4 * H) / np.sqrt*(H)).astype('f')\n",
        "    lstm_Wh2 = (rn(H, 4 * H) / np.sqrt*(H)).astype('f')\n",
        "    lstm_b2 = np.zeros(4 * H).astype('f')\n",
        "    affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "    # 세 가지 개선!\n",
        "    self.layers = [\n",
        "                   TimeEmbedding(embed_W),\n",
        "                   TimeDropout(dropout_ratio),\n",
        "                   TimeLSTM(lstm_Wx1, lstm_Wh1, lstm_b1, stateful = True),\n",
        "                   TimeDropout(dropout_ratio),\n",
        "                   TimeLSTM(lstm_Wx2, lstm_Wh2, lstm_b2, stateful = True),\n",
        "                   TimeDropout(dropout_ratio),\n",
        "                   TimeAffine(embed_W.T, affine_b) # 가중치 공유!!\n",
        "                \n",
        "    ]\n",
        "\n",
        "    self.loss_layer = TimeSoftmaxWithLoss()\n",
        "    self.lstm_layers = [self.layers[2], self.layers[4]]\n",
        "    self.drop_layers = [self.layers[1], self.layers[3], self.layers[5]]\n",
        "\n",
        "    self.params, self.grads = [], []\n",
        "    for layer in self.layers:\n",
        "      self.params += layer.params\n",
        "      self.grads += layer.grads\n",
        "\n",
        "    def predict(self, xs, train_flg = False):\n",
        "      for layer in self.drop_layers:\n",
        "        layer.train_flg = train_flg\n",
        "      for layer in self.layers:\n",
        "        xs = layer.forward(xs)\n",
        "      return xs\n",
        "\n",
        "    def forward(self, xs, ts, train_flg = True):\n",
        "      score = self.predict(xs, train_flg)\n",
        "      loss = self.loss_layer.forward(score, ts)\n",
        "      return loss\n",
        "\n",
        "    def backward(self ,dout = 1):\n",
        "      dout = self.loss_layer.backward(dout)\n",
        "      for layer in reversed(self.layers):\n",
        "        dout = layer.backward(dout)\n",
        "      return dout\n",
        "    \n",
        "    def reset_state(self):\n",
        "      for layer in self.lstm_layers:\n",
        "        layer.reset_state()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBLIR3ZhCAfJ",
        "outputId": "c12136a9-9357-4886-c4b8-6f8ed624cf68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        }
      },
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master/ch06\")\n",
        "sys.path.append('..')\n",
        "from common import config\n",
        "# GPU에서 실행하려면 아래 주석을 해제하세요(CuPy 필요).\n",
        "# ==============================================\n",
        "# config.GPU = True\n",
        "# ==============================================\n",
        "from common.optimizer import SGD\n",
        "from common.trainer import RnnlmTrainer\n",
        "from common.util import eval_perplexity, to_gpu\n",
        "from dataset import ptb\n",
        "from better_rnnlm import BetterRnnlm\n",
        "\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "batch_size = 20\n",
        "wordvec_size = 650\n",
        "hidden_size = 650\n",
        "time_size = 35\n",
        "lr = 20.0\n",
        "max_epoch = 40\n",
        "max_grad = 0.25\n",
        "dropout = 0.5\n",
        "\n",
        "# 학습 데이터 읽기\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "corpus_val, _, _ = ptb.load_data('val')\n",
        "corpus_test, _, _ = ptb.load_data('test')\n",
        "\n",
        "if config.GPU:\n",
        "    corpus = to_gpu(corpus)\n",
        "    corpus_val = to_gpu(corpus_val)\n",
        "    corpus_test = to_gpu(corpus_test)\n",
        "\n",
        "vocab_size = len(word_to_id)\n",
        "xs = corpus[:-1]\n",
        "ts = corpus[1:]\n",
        "\n",
        "model = BetterRnnlm(vocab_size, wordvec_size, hidden_size, dropout)\n",
        "optimizer = SGD(lr)\n",
        "trainer = RnnlmTrainer(model, optimizer)\n",
        "\n",
        "best_ppl = float('inf')\n",
        "for epoch in range(max_epoch):\n",
        "    trainer.fit(xs, ts, max_epoch=1, batch_size=batch_size,\n",
        "                time_size=time_size, max_grad=max_grad)\n",
        "\n",
        "    model.reset_state()\n",
        "    ppl = eval_perplexity(model, corpus_val)\n",
        "    print('검증 퍼플렉서티: ', ppl)\n",
        "\n",
        "    if best_ppl > ppl:\n",
        "        best_ppl = ppl\n",
        "        model.save_params()\n",
        "    else:\n",
        "        lr /= 4.0\n",
        "        optimizer.lr = lr\n",
        "\n",
        "    model.reset_state()\n",
        "    print('-' * 50)\n",
        "\n",
        "\n",
        "# 테스트 데이터로 평가\n",
        "model.reset_state()\n",
        "ppl_test = eval_perplexity(model, corpus_test)\n",
        "print('테스트 퍼플렉서티: ', ppl_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading ptb.valid.txt ... \n",
            "Done\n",
            "Downloading ptb.test.txt ... \n",
            "Done\n",
            "| 에폭 1 |  반복 1 / 1327 | 시간 5[s] | 퍼플렉서티 10000.17\n",
            "| 에폭 1 |  반복 21 / 1327 | 시간 79[s] | 퍼플렉서티 4646.49\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-09076e1d5f26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     trainer.fit(xs, ts, max_epoch=1, batch_size=batch_size,\n\u001b[0;32m---> 48\u001b[0;31m                 time_size=time_size, max_grad=max_grad)\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master/common/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, xs, ts, max_epoch, batch_size, time_size, max_grad, eval_interval)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;31m# 기울기를 구해 매개변수 갱신\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 공유된 가중치를 하나로 모음\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmax_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master/ch06/better_rnnlm.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dout)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master/common/time_layers.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dhs)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m             \u001b[0mdxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master/common/time_layers.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dh_next, dc_next)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mdh_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdh_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc_prev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02TFn-kzMCuZ"
      },
      "source": [
        "# 7장 - RNN을 사용한 문장 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgyWImNqDLyB"
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from common.functions import softmax\n",
        "from ch06.rnnlm import Rnnlm\n",
        "from ch06.better_rnnlm import BetterRnnlm\n",
        "\n",
        "class RnnlmGen(Rnnlm):\n",
        "  def generate(self, start_id, skip_ids = None, sample_size=100):\n",
        "    word_ids = [start_id]\n",
        "\n",
        "    x = start_id\n",
        "    while len(word_ids) < sample_size:\n",
        "      x = np.array(x).reshape(1, 1)\n",
        "      score = self.predict(x)\n",
        "      p = softmax(score.flatten())\n",
        "\n",
        "      sampled = np.random.choice(len(p), size=1, p=p)\n",
        "      if (skip_ids is None) or (sampled not in skip_ids):\n",
        "        x = sampled\n",
        "        word_ids.append(int(x))\n",
        "\n",
        "    return word_ids\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v83Q01rgM3jB",
        "outputId": "5ea8f18d-b042-4a09-f24f-24264352fab1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "import sys, os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master/ch07\")\n",
        "\n",
        "sys.path.append('..')\n",
        "\n",
        "from rnnlm_gen import RnnlmGen\n",
        "from dataset import ptb\n",
        "\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "vocab_size = len(word_to_id)\n",
        "corpus_size = len(corpus)\n",
        "\n",
        "model = RnnlmGen()\n",
        "\n",
        "model.load_params('../ch06/Rnnlm.pkl')\n",
        "# 미리 학습한 가중치 매개변수\n",
        "\n",
        "# 시작(start) 문자와 건너뜀(skip) 문자 설정\n",
        "start_word = 'you'\n",
        "start_id = word_to_id[start_word]\n",
        "skip_words = ['N', '<unk>', '$']\n",
        "skip_ids = [word_to_id[w] for w in skip_words]\n",
        "\n",
        "# 문장 생성\n",
        "word_ids = model.generate(start_id, skip_ids)\n",
        "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
        "txt = txt.replace(' <eos>', '.\\n')\n",
        "print(txt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "you threatened any trying without managed threatening to return but they will fine noranda branch for us he ca n't help have any other.\n",
            " in the sell repayment of it has long been resumed and early encourage bidding from the customers of the future.\n",
            " there will be offered to be pulled.\n",
            " in the industry 's great beauty sector night in los angeles ii which has joint chips citing prime time possibly succeed.\n",
            " thus often do they need to not worry what they are often pressured to harvard cars from kidder making at a comeback meeting the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7kLuWOpN4GJ",
        "outputId": "44069474-6f04-42a7-b414-adcb53943a42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "' '.join(['you', 'say', 'goodbye'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'you say goodbye'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDu9lS6UOHNE",
        "outputId": "dcf364bc-e974-4f5d-8b7c-322de344d455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.np import *\n",
        "from rnnlm_gen import BetterRnnlmGen\n",
        "from dataset import ptb\n",
        "\n",
        "\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "vocab_size = len(word_to_id)\n",
        "corpus_size = len(corpus)\n",
        "\n",
        "\n",
        "model = BetterRnnlmGen()\n",
        "model.load_params('../ch07/BetterRnnlm.pkl')\n",
        "'''\n",
        "# start 문자와 skip 문자 설정\n",
        "start_word = 'you'\n",
        "start_id = word_to_id[start_word]\n",
        "skip_words = ['N', '<unk>', '$']\n",
        "skip_ids = [word_to_id[w] for w in skip_words]\n",
        "# 문장 생성\n",
        "word_ids = model.generate(start_id, skip_ids)\n",
        "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
        "txt = txt.replace(' <eos>', '.\\n')\n",
        "\n",
        "print(txt)\n",
        "'''\n",
        "\n",
        "model.reset_state()\n",
        "\n",
        "start_words = 'the meaning of life is'\n",
        "start_ids = [word_to_id[w] for w in start_words.split(' ')]\n",
        "\n",
        "for x in start_ids[:-1]:\n",
        "    x = np.array(x).reshape(1, 1)\n",
        "    model.predict(x)\n",
        "\n",
        "word_ids = model.generate(start_ids[-1], skip_ids)\n",
        "word_ids = start_ids[:-1] + word_ids\n",
        "txt = ' '.join([id_to_word[i] for i in word_ids])\n",
        "txt = txt.replace(' <eos>', '.\\n')\n",
        "\n",
        "print('-' * 50)\n",
        "print(txt)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "the meaning of life is n't a including widespread when new york city for art announced the plan using control of the company.\n",
            " at this point the proposed purchase for a sale of the company was priced at least an anticipated amount.\n",
            " sales said nasa approached the new financing that in an investment rating had expired.\n",
            " capital funding group structure plc asked shareholders to pay bulk fees for the first six months of the year.\n",
            " the paper and space company said it will continue negotiations with the company with the new hardware and hewlett-packard company to buy realist in addition\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EuZlLkAWH8X",
        "outputId": "7d6c38cb-7e15-482d-8954-bcf13cc6853d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from dataset import sequence\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = sequence.load_data('addition.txt', seed = 1984)\n",
        "char_to_id, id_to_char = sequence.get_vocab()\n",
        "\n",
        "print(x_train.shape, t_train.shape)\n",
        "print(x_test.shape, t_test.shape)\n",
        "\n",
        "print(x_train[0])\n",
        "\n",
        "print(t_train[0])\n",
        "\n",
        "print(''.join([id_to_char[c] for c in x_train[0]]))\n",
        "print(''.join([id_to_char[c] for c in t_train[0]]))\n",
        "\n",
        "# x_train[0]\n",
        "id_to_char[5]\n",
        "char_to_id['+']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(45000, 7) (45000, 5)\n",
            "(5000, 7) (5000, 5)\n",
            "[ 3  0  2  0  0 11  5]\n",
            "[ 6  0 11  7  5]\n",
            "71+118 \n",
            "_189 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0XxOX6Gyk3Q"
      },
      "source": [
        "class Encoder:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)\n",
        "        # stateful = False => Time LSTM 계층이 상태를 유지하지 않기 때문.\n",
        "        self.params = self.embed.params + self.lstm.params\n",
        "        self.grads = self.embed.grads + self.lstm.grads\n",
        "        self.hs = None\n",
        "\n",
        "    def forward(self, xs):\n",
        "        xs = self.embed.forward(xs)\n",
        "        hs = self.lstm.forward(xs)\n",
        "        self.hs = hs\n",
        "        return hs[:, -1, :]\n",
        "\n",
        "    def backward(self, dh):\n",
        "        dhs = np.zeros_like(self.hs)\n",
        "        dhs[:, -1, :] = dh\n",
        "\n",
        "        dout = self.lstm.backward(dhs)\n",
        "        dout = self.embed.backward(dout)\n",
        "        return dout\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9UgDTEw2PGo"
      },
      "source": [
        "class Decoder:\n",
        "  def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "    V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "    rn = np.random.randn\n",
        "\n",
        "    embed_W = (rn(V,D) / 100).astype('f')\n",
        "    lstm_Wx = (rn(D, 4 * H) / np.sqrt*(D)).astype('f')\n",
        "    lstm_Wh = (rn(H, 4 * H) / np.sqrt*(H)).astype('f')\n",
        "    lstm_b = np.zeros(4 * H).astype('f')\n",
        "    affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
        "    affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "    self.embed = TimeEmbedding(embed_W)\n",
        "    self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
        "    self.affine = TimeAffine(affine_W, affine_b)\n",
        "\n",
        "    self.params, self.grads= [], []\n",
        "\n",
        "    for layer in (self.embed, self.lstm, self.affine):\n",
        "      self.params += layer.params\n",
        "      self.grads += layer.grads\n",
        "\n",
        "  def forward(self, xs, h):\n",
        "    self.lstm.set_state(h)\n",
        "\n",
        "    out = self.embed.forward(xs)\n",
        "    out = self.lstm.forward(out)\n",
        "    score = self.affine.forward(out)\n",
        "\n",
        "    return score\n",
        "\n",
        "  def backward(self, dscore):\n",
        "    dout = self.affine.backward(dscore)\n",
        "    dout = self.lstm.backward(dscore)\n",
        "    dout = self.embed.backward(dscore)\n",
        "    dh = self.lstm.dh\n",
        "    return dh\n",
        "\n",
        "  def generate(self, h, start_id, sample_size):\n",
        "    sampled = []\n",
        "    sample_id = start_id\n",
        "    self.lstm.set_state(h)\n",
        "\n",
        "    for _ in range(sample_size):\n",
        "      x = np.array(sample_id).reshape((1, 1))\n",
        "      out = self.embed.forward(x)\n",
        "      out = self.lstm.forward(out)\n",
        "      score = self.affine.forward(out)\n",
        "\n",
        "      sample_id = np.argmax(score.flatten())\n",
        "      sampled.append(int(sample_id))\n",
        "\n",
        "    return sampled\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6y15mcI4hKp"
      },
      "source": [
        "class Seq2seq(BaseModel):\n",
        "  def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "    V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "    self.encoder = Encoder(V, D, H)\n",
        "    self.decoder = Decoder(V, D, H)\n",
        "    self.softmax = TimeSoftmaxWithLoss()\n",
        "\n",
        "    self.params = self.encoder.params + self.decoder.params\n",
        "    self.grads = self.encoder.grads + self.decoder.grads \n",
        "\n",
        "\n",
        "  def forward(self, xs, ts):\n",
        "    decoder_xs, decoder_ts  = ts[:, :-1], ts[:, 1:]\n",
        "    h = self.encoder.forward(xs)\n",
        "    score = self.decoder.forward(decoder_xs, h)\n",
        "    loss = self.softmax.forward(score, decoder_ts)\n",
        "    return loss\n",
        "\n",
        "  def backward(self, dout=1):\n",
        "    dout = self.softmax.backward(dout)\n",
        "    dh = self.decoder.backward(dout)\n",
        "    dout = self.encoder.backward(dh)\n",
        "    return dout\n",
        "\n",
        "  def generate(self, xs, start_id, sample_size):\n",
        "    h = self.encoder.generate(h, start_id, sample_size)\n",
        "    return sampled\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4s5_PUA7vdw",
        "outputId": "c927b67c-bd96-4fe5-9adb-3a53bce26ba6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master')\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master/ch07')\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from dataset import sequence\n",
        "from common.optimizer import Adam\n",
        "from common.trainer import Trainer\n",
        "from common.util import eval_seq2seq\n",
        "from seq2seq import Seq2seq\n",
        "from peeky_seq2seq import PeekySeq2seq\n",
        "\n",
        "# 데이터셋 읽기\n",
        "(x_train, t_train), (x_test, t_test) = sequence.load_data('addition.txt')\n",
        "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]  # 배열의 행을 반전시키면 정답률이 매우 높아짐.\n",
        "char_to_id, id_to_char = sequence.get_vocab()\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "vocab_size=  len(char_to_id)\n",
        "wordvec_size = 16\n",
        "hidden_size = 128\n",
        "batch_size = 128\n",
        "max_epoch = 25\n",
        "max_grad = 5.0\n",
        "\n",
        "# 모델 / 옵티마이저 / 트레이너 생성\n",
        "\n",
        "model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "acc_list = []\n",
        "for epoch in range(max_epoch):\n",
        "  trainer.fit(x_train ,t_train, max_epoch = 1, batch_size = batch_size, max_grad = max_grad)\n",
        "\n",
        "  correct_num = 0 \n",
        "  for i in range(len(x_test)):\n",
        "    question, correct = x_test[[i]], t_test[[i]]\n",
        "    verbose = i < 10\n",
        "    correct_num += eval_seq2seq(model, question, correct, id_to_char, verbose)\n",
        "\n",
        "  acc = float(correct_num) / len(x_test)\n",
        "  acc_list.append(acc)\n",
        "  print('검증 정확도 %.3f%%' %(acc*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 2.56\n",
            "| 에폭 1 |  반복 21 / 351 | 시간 1[s] | 손실 2.52\n",
            "| 에폭 1 |  반복 41 / 351 | 시간 2[s] | 손실 2.17\n",
            "| 에폭 1 |  반복 61 / 351 | 시간 4[s] | 손실 1.96\n",
            "| 에폭 1 |  반복 81 / 351 | 시간 5[s] | 손실 1.91\n",
            "| 에폭 1 |  반복 101 / 351 | 시간 6[s] | 손실 1.87\n",
            "| 에폭 1 |  반복 121 / 351 | 시간 8[s] | 손실 1.86\n",
            "| 에폭 1 |  반복 141 / 351 | 시간 9[s] | 손실 1.84\n",
            "| 에폭 1 |  반복 161 / 351 | 시간 11[s] | 손실 1.80\n",
            "| 에폭 1 |  반복 181 / 351 | 시간 12[s] | 손실 1.78\n",
            "| 에폭 1 |  반복 201 / 351 | 시간 13[s] | 손실 1.77\n",
            "| 에폭 1 |  반복 221 / 351 | 시간 15[s] | 손실 1.77\n",
            "| 에폭 1 |  반복 241 / 351 | 시간 16[s] | 손실 1.76\n",
            "| 에폭 1 |  반복 261 / 351 | 시간 18[s] | 손실 1.75\n",
            "| 에폭 1 |  반복 281 / 351 | 시간 19[s] | 손실 1.74\n",
            "| 에폭 1 |  반복 301 / 351 | 시간 20[s] | 손실 1.74\n",
            "| 에폭 1 |  반복 321 / 351 | 시간 22[s] | 손실 1.74\n",
            "| 에폭 1 |  반복 341 / 351 | 시간 23[s] | 손실 1.73\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[91m☒\u001b[0m 100 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1000\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[91m☒\u001b[0m 1001\n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 100 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[91m☒\u001b[0m 1001\n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 1000\n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1000\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1000\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[91m☒\u001b[0m 1001\n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 703 \n",
            "---\n",
            "검증 정확도 0.120%\n",
            "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 1.73\n",
            "| 에폭 2 |  반복 21 / 351 | 시간 1[s] | 손실 1.72\n",
            "| 에폭 2 |  반복 41 / 351 | 시간 2[s] | 손실 1.72\n",
            "| 에폭 2 |  반복 61 / 351 | 시간 4[s] | 손실 1.72\n",
            "| 에폭 2 |  반복 81 / 351 | 시간 5[s] | 손실 1.70\n",
            "| 에폭 2 |  반복 101 / 351 | 시간 7[s] | 손실 1.70\n",
            "| 에폭 2 |  반복 121 / 351 | 시간 8[s] | 손실 1.69\n",
            "| 에폭 2 |  반복 141 / 351 | 시간 10[s] | 손실 1.68\n",
            "| 에폭 2 |  반복 161 / 351 | 시간 11[s] | 손실 1.67\n",
            "| 에폭 2 |  반복 181 / 351 | 시간 12[s] | 손실 1.66\n",
            "| 에폭 2 |  반복 201 / 351 | 시간 14[s] | 손실 1.66\n",
            "| 에폭 2 |  반복 221 / 351 | 시간 15[s] | 손실 1.65\n",
            "| 에폭 2 |  반복 241 / 351 | 시간 17[s] | 손실 1.63\n",
            "| 에폭 2 |  반복 261 / 351 | 시간 18[s] | 손실 1.62\n",
            "| 에폭 2 |  반복 281 / 351 | 시간 20[s] | 손실 1.61\n",
            "| 에폭 2 |  반복 301 / 351 | 시간 21[s] | 손실 1.60\n",
            "| 에폭 2 |  반복 321 / 351 | 시간 23[s] | 손실 1.58\n",
            "| 에폭 2 |  반복 341 / 351 | 시간 24[s] | 손실 1.56\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[91m☒\u001b[0m 100 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1000\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[91m☒\u001b[0m 690 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 1000\n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[91m☒\u001b[0m 470 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 700 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1000\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1444\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[91m☒\u001b[0m 700 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 370 \n",
            "---\n",
            "검증 정확도 0.400%\n",
            "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 1.52\n",
            "| 에폭 3 |  반복 21 / 351 | 시간 1[s] | 손실 1.53\n",
            "| 에폭 3 |  반복 41 / 351 | 시간 3[s] | 손실 1.51\n",
            "| 에폭 3 |  반복 61 / 351 | 시간 4[s] | 손실 1.49\n",
            "| 에폭 3 |  반복 81 / 351 | 시간 5[s] | 손실 1.47\n",
            "| 에폭 3 |  반복 101 / 351 | 시간 7[s] | 손실 1.45\n",
            "| 에폭 3 |  반복 121 / 351 | 시간 8[s] | 손실 1.44\n",
            "| 에폭 3 |  반복 141 / 351 | 시간 10[s] | 손실 1.42\n",
            "| 에폭 3 |  반복 161 / 351 | 시간 11[s] | 손실 1.40\n",
            "| 에폭 3 |  반복 181 / 351 | 시간 13[s] | 손실 1.38\n",
            "| 에폭 3 |  반복 201 / 351 | 시간 14[s] | 손실 1.37\n",
            "| 에폭 3 |  반복 221 / 351 | 시간 16[s] | 손실 1.35\n",
            "| 에폭 3 |  반복 241 / 351 | 시간 17[s] | 손실 1.33\n",
            "| 에폭 3 |  반복 261 / 351 | 시간 19[s] | 손실 1.32\n",
            "| 에폭 3 |  반복 281 / 351 | 시간 20[s] | 손실 1.30\n",
            "| 에폭 3 |  반복 301 / 351 | 시간 22[s] | 손실 1.29\n",
            "| 에폭 3 |  반복 321 / 351 | 시간 23[s] | 손실 1.28\n",
            "| 에폭 3 |  반복 341 / 351 | 시간 25[s] | 손실 1.27\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[91m☒\u001b[0m 158 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1148\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[91m☒\u001b[0m 662 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 162 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[91m☒\u001b[0m 382 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 818 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1008\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1434\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[91m☒\u001b[0m 838 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 202 \n",
            "---\n",
            "검증 정확도 1.940%\n",
            "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 1.26\n",
            "| 에폭 4 |  반복 21 / 351 | 시간 1[s] | 손실 1.25\n",
            "| 에폭 4 |  반복 41 / 351 | 시간 3[s] | 손실 1.23\n",
            "| 에폭 4 |  반복 61 / 351 | 시간 4[s] | 손실 1.22\n",
            "| 에폭 4 |  반복 81 / 351 | 시간 6[s] | 손실 1.20\n",
            "| 에폭 4 |  반복 101 / 351 | 시간 7[s] | 손실 1.19\n",
            "| 에폭 4 |  반복 121 / 351 | 시간 9[s] | 손실 1.18\n",
            "| 에폭 4 |  반복 141 / 351 | 시간 10[s] | 손실 1.17\n",
            "| 에폭 4 |  반복 161 / 351 | 시간 12[s] | 손실 1.15\n",
            "| 에폭 4 |  반복 181 / 351 | 시간 13[s] | 손실 1.13\n",
            "| 에폭 4 |  반복 201 / 351 | 시간 15[s] | 손실 1.12\n",
            "| 에폭 4 |  반복 221 / 351 | 시간 16[s] | 손실 1.11\n",
            "| 에폭 4 |  반복 241 / 351 | 시간 18[s] | 손실 1.09\n",
            "| 에폭 4 |  반복 261 / 351 | 시간 19[s] | 손실 1.08\n",
            "| 에폭 4 |  반복 281 / 351 | 시간 21[s] | 손실 1.07\n",
            "| 에폭 4 |  반복 301 / 351 | 시간 22[s] | 손실 1.07\n",
            "| 에폭 4 |  반복 321 / 351 | 시간 24[s] | 손실 1.05\n",
            "| 에폭 4 |  반복 341 / 351 | 시간 25[s] | 손실 1.03\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[91m☒\u001b[0m 166 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1196\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[91m☒\u001b[0m 668 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 166 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[91m☒\u001b[0m 419 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 896 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1010\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1496\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[91m☒\u001b[0m 868 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 239 \n",
            "---\n",
            "검증 정확도 5.780%\n",
            "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 1.01\n",
            "| 에폭 5 |  반복 21 / 351 | 시간 1[s] | 손실 1.01\n",
            "| 에폭 5 |  반복 41 / 351 | 시간 3[s] | 손실 1.00\n",
            "| 에폭 5 |  반복 61 / 351 | 시간 4[s] | 손실 0.99\n",
            "| 에폭 5 |  반복 81 / 351 | 시간 6[s] | 손실 0.97\n",
            "| 에폭 5 |  반복 101 / 351 | 시간 7[s] | 손실 0.95\n",
            "| 에폭 5 |  반복 121 / 351 | 시간 9[s] | 손실 0.95\n",
            "| 에폭 5 |  반복 141 / 351 | 시간 10[s] | 손실 0.94\n",
            "| 에폭 5 |  반복 161 / 351 | 시간 12[s] | 손실 0.93\n",
            "| 에폭 5 |  반복 181 / 351 | 시간 13[s] | 손실 0.93\n",
            "| 에폭 5 |  반복 201 / 351 | 시간 15[s] | 손실 0.91\n",
            "| 에폭 5 |  반복 221 / 351 | 시간 16[s] | 손실 0.89\n",
            "| 에폭 5 |  반복 241 / 351 | 시간 18[s] | 손실 0.89\n",
            "| 에폭 5 |  반복 261 / 351 | 시간 19[s] | 손실 0.88\n",
            "| 에폭 5 |  반복 281 / 351 | 시간 21[s] | 손실 0.87\n",
            "| 에폭 5 |  반복 301 / 351 | 시간 22[s] | 손실 0.86\n",
            "| 에폭 5 |  반복 321 / 351 | 시간 24[s] | 손실 0.85\n",
            "| 에폭 5 |  반복 341 / 351 | 시간 25[s] | 손실 0.84\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[91m☒\u001b[0m 161 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1192\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 166 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[91m☒\u001b[0m 421 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 860 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1066\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1414\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[91m☒\u001b[0m 865 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 232 \n",
            "---\n",
            "검증 정확도 12.460%\n",
            "| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 6 |  반복 21 / 351 | 시간 1[s] | 손실 0.82\n",
            "| 에폭 6 |  반복 41 / 351 | 시간 3[s] | 손실 0.82\n",
            "| 에폭 6 |  반복 61 / 351 | 시간 4[s] | 손실 0.81\n",
            "| 에폭 6 |  반복 81 / 351 | 시간 6[s] | 손실 0.80\n",
            "| 에폭 6 |  반복 101 / 351 | 시간 7[s] | 손실 0.80\n",
            "| 에폭 6 |  반복 121 / 351 | 시간 9[s] | 손실 0.79\n",
            "| 에폭 6 |  반복 141 / 351 | 시간 10[s] | 손실 0.78\n",
            "| 에폭 6 |  반복 161 / 351 | 시간 12[s] | 손실 0.77\n",
            "| 에폭 6 |  반복 181 / 351 | 시간 13[s] | 손실 0.77\n",
            "| 에폭 6 |  반복 201 / 351 | 시간 15[s] | 손실 0.78\n",
            "| 에폭 6 |  반복 221 / 351 | 시간 16[s] | 손실 0.76\n",
            "| 에폭 6 |  반복 241 / 351 | 시간 18[s] | 손실 0.75\n",
            "| 에폭 6 |  반복 261 / 351 | 시간 19[s] | 손실 0.74\n",
            "| 에폭 6 |  반복 281 / 351 | 시간 21[s] | 손실 0.73\n",
            "| 에폭 6 |  반복 301 / 351 | 시간 22[s] | 손실 0.73\n",
            "| 에폭 6 |  반복 321 / 351 | 시간 24[s] | 손실 0.72\n",
            "| 에폭 6 |  반복 341 / 351 | 시간 26[s] | 손실 0.72\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[91m☒\u001b[0m 161 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1141\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[91m☒\u001b[0m 661 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 162 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 851 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1061\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1391\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[91m☒\u001b[0m 866 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 234 \n",
            "---\n",
            "검증 정확도 14.260%\n",
            "| 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 7 |  반복 21 / 351 | 시간 1[s] | 손실 0.71\n",
            "| 에폭 7 |  반복 41 / 351 | 시간 3[s] | 손실 0.70\n",
            "| 에폭 7 |  반복 61 / 351 | 시간 4[s] | 손실 0.70\n",
            "| 에폭 7 |  반복 81 / 351 | 시간 6[s] | 손실 0.68\n",
            "| 에폭 7 |  반복 101 / 351 | 시간 8[s] | 손실 0.68\n",
            "| 에폭 7 |  반복 121 / 351 | 시간 9[s] | 손실 0.67\n",
            "| 에폭 7 |  반복 141 / 351 | 시간 11[s] | 손실 0.67\n",
            "| 에폭 7 |  반복 161 / 351 | 시간 12[s] | 손실 0.67\n",
            "| 에폭 7 |  반복 181 / 351 | 시간 14[s] | 손실 0.66\n",
            "| 에폭 7 |  반복 201 / 351 | 시간 16[s] | 손실 0.66\n",
            "| 에폭 7 |  반복 221 / 351 | 시간 17[s] | 손실 0.66\n",
            "| 에폭 7 |  반복 241 / 351 | 시간 19[s] | 손실 0.64\n",
            "| 에폭 7 |  반복 261 / 351 | 시간 20[s] | 손실 0.65\n",
            "| 에폭 7 |  반복 281 / 351 | 시간 22[s] | 손실 0.64\n",
            "| 에폭 7 |  반복 301 / 351 | 시간 23[s] | 손실 0.63\n",
            "| 에폭 7 |  반복 321 / 351 | 시간 25[s] | 손실 0.63\n",
            "| 에폭 7 |  반복 341 / 351 | 시간 26[s] | 손실 0.62\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1142\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 162 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 859 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1144\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1431\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[91m☒\u001b[0m 866 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 239 \n",
            "---\n",
            "검증 정확도 17.500%\n",
            "| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 8 |  반복 21 / 351 | 시간 1[s] | 손실 0.61\n",
            "| 에폭 8 |  반복 41 / 351 | 시간 3[s] | 손실 0.62\n",
            "| 에폭 8 |  반복 61 / 351 | 시간 4[s] | 손실 0.61\n",
            "| 에폭 8 |  반복 81 / 351 | 시간 6[s] | 손실 0.61\n",
            "| 에폭 8 |  반복 101 / 351 | 시간 7[s] | 손실 0.61\n",
            "| 에폭 8 |  반복 121 / 351 | 시간 9[s] | 손실 0.60\n",
            "| 에폭 8 |  반복 141 / 351 | 시간 10[s] | 손실 0.60\n",
            "| 에폭 8 |  반복 161 / 351 | 시간 12[s] | 손실 0.59\n",
            "| 에폭 8 |  반복 181 / 351 | 시간 13[s] | 손실 0.58\n",
            "| 에폭 8 |  반복 201 / 351 | 시간 15[s] | 손실 0.59\n",
            "| 에폭 8 |  반복 221 / 351 | 시간 16[s] | 손실 0.60\n",
            "| 에폭 8 |  반복 241 / 351 | 시간 18[s] | 손실 0.59\n",
            "| 에폭 8 |  반복 261 / 351 | 시간 19[s] | 손실 0.58\n",
            "| 에폭 8 |  반복 281 / 351 | 시간 21[s] | 손실 0.59\n",
            "| 에폭 8 |  반복 301 / 351 | 시간 22[s] | 손실 0.58\n",
            "| 에폭 8 |  반복 321 / 351 | 시간 24[s] | 손실 0.57\n",
            "| 에폭 8 |  반복 341 / 351 | 시간 25[s] | 손실 0.57\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[91m☒\u001b[0m 163 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1134\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 162 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[91m☒\u001b[0m 423 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 759 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1431\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[91m☒\u001b[0m 866 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 238 \n",
            "---\n",
            "검증 정확도 23.080%\n",
            "| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 9 |  반복 21 / 351 | 시간 1[s] | 손실 0.56\n",
            "| 에폭 9 |  반복 41 / 351 | 시간 3[s] | 손실 0.56\n",
            "| 에폭 9 |  반복 61 / 351 | 시간 4[s] | 손실 0.55\n",
            "| 에폭 9 |  반복 81 / 351 | 시간 6[s] | 손실 0.54\n",
            "| 에폭 9 |  반복 101 / 351 | 시간 7[s] | 손실 0.55\n",
            "| 에폭 9 |  반복 121 / 351 | 시간 9[s] | 손실 0.55\n",
            "| 에폭 9 |  반복 141 / 351 | 시간 10[s] | 손실 0.54\n",
            "| 에폭 9 |  반복 161 / 351 | 시간 12[s] | 손실 0.55\n",
            "| 에폭 9 |  반복 181 / 351 | 시간 13[s] | 손실 0.53\n",
            "| 에폭 9 |  반복 201 / 351 | 시간 15[s] | 손실 0.54\n",
            "| 에폭 9 |  반복 221 / 351 | 시간 16[s] | 손실 0.54\n",
            "| 에폭 9 |  반복 241 / 351 | 시간 18[s] | 손실 0.53\n",
            "| 에폭 9 |  반복 261 / 351 | 시간 19[s] | 손실 0.53\n",
            "| 에폭 9 |  반복 281 / 351 | 시간 21[s] | 손실 0.54\n",
            "| 에폭 9 |  반복 301 / 351 | 시간 22[s] | 손실 0.54\n",
            "| 에폭 9 |  반복 321 / 351 | 시간 24[s] | 손실 0.53\n",
            "| 에폭 9 |  반복 341 / 351 | 시간 25[s] | 손실 0.53\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[91m☒\u001b[0m 158 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1142\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[91m☒\u001b[0m 664 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 162 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 854 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1428\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[91m☒\u001b[0m 862 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 238 \n",
            "---\n",
            "검증 정확도 26.540%\n",
            "| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 0.50\n",
            "| 에폭 10 |  반복 21 / 351 | 시간 1[s] | 손실 0.51\n",
            "| 에폭 10 |  반복 41 / 351 | 시간 3[s] | 손실 0.52\n",
            "| 에폭 10 |  반복 61 / 351 | 시간 4[s] | 손실 0.55\n",
            "| 에폭 10 |  반복 81 / 351 | 시간 6[s] | 손실 0.52\n",
            "| 에폭 10 |  반복 101 / 351 | 시간 7[s] | 손실 0.51\n",
            "| 에폭 10 |  반복 121 / 351 | 시간 9[s] | 손실 0.50\n",
            "| 에폭 10 |  반복 141 / 351 | 시간 10[s] | 손실 0.51\n",
            "| 에폭 10 |  반복 161 / 351 | 시간 12[s] | 손실 0.52\n",
            "| 에폭 10 |  반복 181 / 351 | 시간 13[s] | 손실 0.53\n",
            "| 에폭 10 |  반복 201 / 351 | 시간 15[s] | 손실 0.50\n",
            "| 에폭 10 |  반복 221 / 351 | 시간 16[s] | 손실 0.50\n",
            "| 에폭 10 |  반복 241 / 351 | 시간 18[s] | 손실 0.50\n",
            "| 에폭 10 |  반복 261 / 351 | 시간 19[s] | 손실 0.50\n",
            "| 에폭 10 |  반복 281 / 351 | 시간 21[s] | 손실 0.49\n",
            "| 에폭 10 |  반복 301 / 351 | 시간 22[s] | 손실 0.48\n",
            "| 에폭 10 |  반복 321 / 351 | 시간 24[s] | 손실 0.48\n",
            "| 에폭 10 |  반복 341 / 351 | 시간 25[s] | 손실 0.48\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[91m☒\u001b[0m 163 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[91m☒\u001b[0m 664 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 162 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[91m☒\u001b[0m 421 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 859 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1054\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1431\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 235 \n",
            "---\n",
            "검증 정확도 29.820%\n",
            "| 에폭 11 |  반복 1 / 351 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 11 |  반복 21 / 351 | 시간 1[s] | 손실 0.48\n",
            "| 에폭 11 |  반복 41 / 351 | 시간 3[s] | 손실 0.48\n",
            "| 에폭 11 |  반복 61 / 351 | 시간 4[s] | 손실 0.48\n",
            "| 에폭 11 |  반복 81 / 351 | 시간 6[s] | 손실 0.47\n",
            "| 에폭 11 |  반복 101 / 351 | 시간 7[s] | 손실 0.47\n",
            "| 에폭 11 |  반복 121 / 351 | 시간 9[s] | 손실 0.47\n",
            "| 에폭 11 |  반복 141 / 351 | 시간 10[s] | 손실 0.47\n",
            "| 에폭 11 |  반복 161 / 351 | 시간 12[s] | 손실 0.48\n",
            "| 에폭 11 |  반복 181 / 351 | 시간 13[s] | 손실 0.48\n",
            "| 에폭 11 |  반복 201 / 351 | 시간 15[s] | 손실 0.47\n",
            "| 에폭 11 |  반복 221 / 351 | 시간 16[s] | 손실 0.47\n",
            "| 에폭 11 |  반복 241 / 351 | 시간 18[s] | 손실 0.46\n",
            "| 에폭 11 |  반복 261 / 351 | 시간 19[s] | 손실 0.46\n",
            "| 에폭 11 |  반복 281 / 351 | 시간 21[s] | 손실 0.46\n",
            "| 에폭 11 |  반복 301 / 351 | 시간 22[s] | 손실 0.48\n",
            "| 에폭 11 |  반복 321 / 351 | 시간 24[s] | 손실 0.45\n",
            "| 에폭 11 |  반복 341 / 351 | 시간 25[s] | 손실 0.45\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1140\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 162 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[91m☒\u001b[0m 421 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 859 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1426\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[91m☒\u001b[0m 866 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 238 \n",
            "---\n",
            "검증 정확도 28.480%\n",
            "| 에폭 12 |  반복 1 / 351 | 시간 0[s] | 손실 0.46\n",
            "| 에폭 12 |  반복 21 / 351 | 시간 1[s] | 손실 0.45\n",
            "| 에폭 12 |  반복 41 / 351 | 시간 3[s] | 손실 0.45\n",
            "| 에폭 12 |  반복 61 / 351 | 시간 4[s] | 손실 0.46\n",
            "| 에폭 12 |  반복 81 / 351 | 시간 6[s] | 손실 0.45\n",
            "| 에폭 12 |  반복 101 / 351 | 시간 7[s] | 손실 0.46\n",
            "| 에폭 12 |  반복 121 / 351 | 시간 9[s] | 손실 0.46\n",
            "| 에폭 12 |  반복 141 / 351 | 시간 10[s] | 손실 0.46\n",
            "| 에폭 12 |  반복 161 / 351 | 시간 12[s] | 손실 0.45\n",
            "| 에폭 12 |  반복 181 / 351 | 시간 13[s] | 손실 0.44\n",
            "| 에폭 12 |  반복 201 / 351 | 시간 15[s] | 손실 0.45\n",
            "| 에폭 12 |  반복 221 / 351 | 시간 16[s] | 손실 0.44\n",
            "| 에폭 12 |  반복 241 / 351 | 시간 18[s] | 손실 0.43\n",
            "| 에폭 12 |  반복 261 / 351 | 시간 19[s] | 손실 0.43\n",
            "| 에폭 12 |  반복 281 / 351 | 시간 21[s] | 손실 0.44\n",
            "| 에폭 12 |  반복 301 / 351 | 시간 22[s] | 손실 0.45\n",
            "| 에폭 12 |  반복 321 / 351 | 시간 24[s] | 손실 0.44\n",
            "| 에폭 12 |  반복 341 / 351 | 시간 25[s] | 손실 0.43\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[91m☒\u001b[0m 667 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1051\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1428\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[91m☒\u001b[0m 862 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 237 \n",
            "---\n",
            "검증 정확도 36.640%\n",
            "| 에폭 13 |  반복 1 / 351 | 시간 0[s] | 손실 0.41\n",
            "| 에폭 13 |  반복 21 / 351 | 시간 1[s] | 손실 0.43\n",
            "| 에폭 13 |  반복 41 / 351 | 시간 3[s] | 손실 0.42\n",
            "| 에폭 13 |  반복 61 / 351 | 시간 4[s] | 손실 0.42\n",
            "| 에폭 13 |  반복 81 / 351 | 시간 6[s] | 손실 0.42\n",
            "| 에폭 13 |  반복 101 / 351 | 시간 7[s] | 손실 0.44\n",
            "| 에폭 13 |  반복 121 / 351 | 시간 9[s] | 손실 0.43\n",
            "| 에폭 13 |  반복 141 / 351 | 시간 10[s] | 손실 0.41\n",
            "| 에폭 13 |  반복 161 / 351 | 시간 12[s] | 손실 0.42\n",
            "| 에폭 13 |  반복 181 / 351 | 시간 13[s] | 손실 0.42\n",
            "| 에폭 13 |  반복 201 / 351 | 시간 14[s] | 손실 0.42\n",
            "| 에폭 13 |  반복 221 / 351 | 시간 16[s] | 손실 0.43\n",
            "| 에폭 13 |  반복 241 / 351 | 시간 17[s] | 손실 0.43\n",
            "| 에폭 13 |  반복 261 / 351 | 시간 19[s] | 손실 0.41\n",
            "| 에폭 13 |  반복 281 / 351 | 시간 20[s] | 손실 0.42\n",
            "| 에폭 13 |  반복 301 / 351 | 시간 22[s] | 손실 0.41\n",
            "| 에폭 13 |  반복 321 / 351 | 시간 24[s] | 손실 0.43\n",
            "| 에폭 13 |  반복 341 / 351 | 시간 25[s] | 손실 0.40\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1140\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 162 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[91m☒\u001b[0m 424 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 856 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1054\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1429\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 239 \n",
            "---\n",
            "검증 정확도 39.420%\n",
            "| 에폭 14 |  반복 1 / 351 | 시간 0[s] | 손실 0.41\n",
            "| 에폭 14 |  반복 21 / 351 | 시간 1[s] | 손실 0.41\n",
            "| 에폭 14 |  반복 41 / 351 | 시간 3[s] | 손실 0.41\n",
            "| 에폭 14 |  반복 61 / 351 | 시간 4[s] | 손실 0.40\n",
            "| 에폭 14 |  반복 81 / 351 | 시간 6[s] | 손실 0.40\n",
            "| 에폭 14 |  반복 101 / 351 | 시간 7[s] | 손실 0.41\n",
            "| 에폭 14 |  반복 121 / 351 | 시간 9[s] | 손실 0.39\n",
            "| 에폭 14 |  반복 141 / 351 | 시간 10[s] | 손실 0.39\n",
            "| 에폭 14 |  반복 161 / 351 | 시간 12[s] | 손실 0.38\n",
            "| 에폭 14 |  반복 181 / 351 | 시간 13[s] | 손실 0.38\n",
            "| 에폭 14 |  반복 201 / 351 | 시간 15[s] | 손실 0.38\n",
            "| 에폭 14 |  반복 221 / 351 | 시간 16[s] | 손실 0.38\n",
            "| 에폭 14 |  반복 241 / 351 | 시간 18[s] | 손실 0.39\n",
            "| 에폭 14 |  반복 261 / 351 | 시간 19[s] | 손실 0.40\n",
            "| 에폭 14 |  반복 281 / 351 | 시간 21[s] | 손실 0.41\n",
            "| 에폭 14 |  반복 301 / 351 | 시간 22[s] | 손실 0.39\n",
            "| 에폭 14 |  반복 321 / 351 | 시간 24[s] | 손실 0.39\n",
            "| 에폭 14 |  반복 341 / 351 | 시간 25[s] | 손실 0.39\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1137\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[91m☒\u001b[0m 667 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 858 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1426\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 235 \n",
            "---\n",
            "검증 정확도 36.680%\n",
            "| 에폭 15 |  반복 1 / 351 | 시간 0[s] | 손실 0.38\n",
            "| 에폭 15 |  반복 21 / 351 | 시간 1[s] | 손실 0.39\n",
            "| 에폭 15 |  반복 41 / 351 | 시간 3[s] | 손실 0.39\n",
            "| 에폭 15 |  반복 61 / 351 | 시간 4[s] | 손실 0.38\n",
            "| 에폭 15 |  반복 81 / 351 | 시간 6[s] | 손실 0.38\n",
            "| 에폭 15 |  반복 101 / 351 | 시간 7[s] | 손실 0.38\n",
            "| 에폭 15 |  반복 121 / 351 | 시간 9[s] | 손실 0.38\n",
            "| 에폭 15 |  반복 141 / 351 | 시간 10[s] | 손실 0.38\n",
            "| 에폭 15 |  반복 161 / 351 | 시간 12[s] | 손실 0.38\n",
            "| 에폭 15 |  반복 181 / 351 | 시간 13[s] | 손실 0.38\n",
            "| 에폭 15 |  반복 201 / 351 | 시간 15[s] | 손실 0.38\n",
            "| 에폭 15 |  반복 221 / 351 | 시간 17[s] | 손실 0.39\n",
            "| 에폭 15 |  반복 241 / 351 | 시간 18[s] | 손실 0.38\n",
            "| 에폭 15 |  반복 261 / 351 | 시간 20[s] | 손실 0.37\n",
            "| 에폭 15 |  반복 281 / 351 | 시간 21[s] | 손실 0.37\n",
            "| 에폭 15 |  반복 301 / 351 | 시간 23[s] | 손실 0.39\n",
            "| 에폭 15 |  반복 321 / 351 | 시간 24[s] | 손실 0.39\n",
            "| 에폭 15 |  반복 341 / 351 | 시간 26[s] | 손실 0.37\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1137\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[91m☒\u001b[0m 667 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 164 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[91m☒\u001b[0m 420 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 856 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1052\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1431\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 237 \n",
            "---\n",
            "검증 정확도 41.100%\n",
            "| 에폭 16 |  반복 1 / 351 | 시간 0[s] | 손실 0.36\n",
            "| 에폭 16 |  반복 21 / 351 | 시간 1[s] | 손실 0.36\n",
            "| 에폭 16 |  반복 41 / 351 | 시간 3[s] | 손실 0.36\n",
            "| 에폭 16 |  반복 61 / 351 | 시간 4[s] | 손실 0.36\n",
            "| 에폭 16 |  반복 81 / 351 | 시간 6[s] | 손실 0.37\n",
            "| 에폭 16 |  반복 101 / 351 | 시간 7[s] | 손실 0.36\n",
            "| 에폭 16 |  반복 121 / 351 | 시간 9[s] | 손실 0.37\n",
            "| 에폭 16 |  반복 141 / 351 | 시간 10[s] | 손실 0.36\n",
            "| 에폭 16 |  반복 161 / 351 | 시간 12[s] | 손실 0.37\n",
            "| 에폭 16 |  반복 181 / 351 | 시간 13[s] | 손실 0.36\n",
            "| 에폭 16 |  반복 201 / 351 | 시간 15[s] | 손실 0.38\n",
            "| 에폭 16 |  반복 221 / 351 | 시간 16[s] | 손실 0.38\n",
            "| 에폭 16 |  반복 241 / 351 | 시간 18[s] | 손실 0.36\n",
            "| 에폭 16 |  반복 261 / 351 | 시간 19[s] | 손실 0.35\n",
            "| 에폭 16 |  반복 281 / 351 | 시간 21[s] | 손실 0.35\n",
            "| 에폭 16 |  반복 301 / 351 | 시간 22[s] | 손실 0.35\n",
            "| 에폭 16 |  반복 321 / 351 | 시간 23[s] | 손실 0.35\n",
            "| 에폭 16 |  반복 341 / 351 | 시간 25[s] | 손실 0.37\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1142\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 858 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1054\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1430\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 237 \n",
            "---\n",
            "검증 정확도 42.700%\n",
            "| 에폭 17 |  반복 1 / 351 | 시간 0[s] | 손실 0.34\n",
            "| 에폭 17 |  반복 21 / 351 | 시간 1[s] | 손실 0.36\n",
            "| 에폭 17 |  반복 41 / 351 | 시간 3[s] | 손실 0.36\n",
            "| 에폭 17 |  반복 61 / 351 | 시간 4[s] | 손실 0.35\n",
            "| 에폭 17 |  반복 81 / 351 | 시간 6[s] | 손실 0.36\n",
            "| 에폭 17 |  반복 101 / 351 | 시간 7[s] | 손실 0.34\n",
            "| 에폭 17 |  반복 121 / 351 | 시간 9[s] | 손실 0.34\n",
            "| 에폭 17 |  반복 141 / 351 | 시간 10[s] | 손실 0.34\n",
            "| 에폭 17 |  반복 161 / 351 | 시간 12[s] | 손실 0.34\n",
            "| 에폭 17 |  반복 181 / 351 | 시간 13[s] | 손실 0.35\n",
            "| 에폭 17 |  반복 201 / 351 | 시간 15[s] | 손실 0.35\n",
            "| 에폭 17 |  반복 221 / 351 | 시간 16[s] | 손실 0.34\n",
            "| 에폭 17 |  반복 241 / 351 | 시간 18[s] | 손실 0.35\n",
            "| 에폭 17 |  반복 261 / 351 | 시간 19[s] | 손실 0.36\n",
            "| 에폭 17 |  반복 281 / 351 | 시간 21[s] | 손실 0.37\n",
            "| 에폭 17 |  반복 301 / 351 | 시간 22[s] | 손실 0.37\n",
            "| 에폭 17 |  반복 321 / 351 | 시간 24[s] | 손실 0.37\n",
            "| 에폭 17 |  반복 341 / 351 | 시간 25[s] | 손실 0.36\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1138\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 162 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 856 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1051\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1429\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[91m☒\u001b[0m 865 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 42.860%\n",
            "| 에폭 18 |  반복 1 / 351 | 시간 0[s] | 손실 0.34\n",
            "| 에폭 18 |  반복 21 / 351 | 시간 1[s] | 손실 0.36\n",
            "| 에폭 18 |  반복 41 / 351 | 시간 3[s] | 손실 0.35\n",
            "| 에폭 18 |  반복 61 / 351 | 시간 4[s] | 손실 0.35\n",
            "| 에폭 18 |  반복 81 / 351 | 시간 6[s] | 손실 0.34\n",
            "| 에폭 18 |  반복 101 / 351 | 시간 7[s] | 손실 0.34\n",
            "| 에폭 18 |  반복 121 / 351 | 시간 9[s] | 손실 0.33\n",
            "| 에폭 18 |  반복 141 / 351 | 시간 10[s] | 손실 0.33\n",
            "| 에폭 18 |  반복 161 / 351 | 시간 12[s] | 손실 0.34\n",
            "| 에폭 18 |  반복 181 / 351 | 시간 13[s] | 손실 0.33\n",
            "| 에폭 18 |  반복 201 / 351 | 시간 15[s] | 손실 0.33\n",
            "| 에폭 18 |  반복 221 / 351 | 시간 16[s] | 손실 0.33\n",
            "| 에폭 18 |  반복 241 / 351 | 시간 18[s] | 손실 0.33\n",
            "| 에폭 18 |  반복 261 / 351 | 시간 19[s] | 손실 0.33\n",
            "| 에폭 18 |  반복 281 / 351 | 시간 21[s] | 손실 0.33\n",
            "| 에폭 18 |  반복 301 / 351 | 시간 22[s] | 손실 0.33\n",
            "| 에폭 18 |  반복 321 / 351 | 시간 24[s] | 손실 0.35\n",
            "| 에폭 18 |  반복 341 / 351 | 시간 25[s] | 손실 0.35\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1138\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 162 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[91m☒\u001b[0m 423 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 856 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1052\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1425\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 237 \n",
            "---\n",
            "검증 정확도 40.640%\n",
            "| 에폭 19 |  반복 1 / 351 | 시간 0[s] | 손실 0.34\n",
            "| 에폭 19 |  반복 21 / 351 | 시간 1[s] | 손실 0.34\n",
            "| 에폭 19 |  반복 41 / 351 | 시간 3[s] | 손실 0.34\n",
            "| 에폭 19 |  반복 61 / 351 | 시간 4[s] | 손실 0.34\n",
            "| 에폭 19 |  반복 81 / 351 | 시간 6[s] | 손실 0.34\n",
            "| 에폭 19 |  반복 101 / 351 | 시간 7[s] | 손실 0.32\n",
            "| 에폭 19 |  반복 121 / 351 | 시간 9[s] | 손실 0.31\n",
            "| 에폭 19 |  반복 141 / 351 | 시간 10[s] | 손실 0.33\n",
            "| 에폭 19 |  반복 161 / 351 | 시간 12[s] | 손실 0.31\n",
            "| 에폭 19 |  반복 181 / 351 | 시간 13[s] | 손실 0.30\n",
            "| 에폭 19 |  반복 201 / 351 | 시간 15[s] | 손실 0.31\n",
            "| 에폭 19 |  반복 221 / 351 | 시간 16[s] | 손실 0.31\n",
            "| 에폭 19 |  반복 241 / 351 | 시간 18[s] | 손실 0.32\n",
            "| 에폭 19 |  반복 261 / 351 | 시간 19[s] | 손실 0.33\n",
            "| 에폭 19 |  반복 281 / 351 | 시간 21[s] | 손실 0.33\n",
            "| 에폭 19 |  반복 301 / 351 | 시간 22[s] | 손실 0.32\n",
            "| 에폭 19 |  반복 321 / 351 | 시간 24[s] | 손실 0.33\n",
            "| 에폭 19 |  반복 341 / 351 | 시간 25[s] | 손실 0.33\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[91m☒\u001b[0m 161 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1140\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 858 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1052\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1430\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 238 \n",
            "---\n",
            "검증 정확도 47.560%\n",
            "| 에폭 20 |  반복 1 / 351 | 시간 0[s] | 손실 0.32\n",
            "| 에폭 20 |  반복 21 / 351 | 시간 1[s] | 손실 0.35\n",
            "| 에폭 20 |  반복 41 / 351 | 시간 3[s] | 손실 0.33\n",
            "| 에폭 20 |  반복 61 / 351 | 시간 5[s] | 손실 0.33\n",
            "| 에폭 20 |  반복 81 / 351 | 시간 6[s] | 손실 0.31\n",
            "| 에폭 20 |  반복 101 / 351 | 시간 8[s] | 손실 0.31\n",
            "| 에폭 20 |  반복 121 / 351 | 시간 10[s] | 손실 0.33\n",
            "| 에폭 20 |  반복 141 / 351 | 시간 12[s] | 손실 0.32\n",
            "| 에폭 20 |  반복 161 / 351 | 시간 13[s] | 손실 0.33\n",
            "| 에폭 20 |  반복 181 / 351 | 시간 15[s] | 손실 0.31\n",
            "| 에폭 20 |  반복 201 / 351 | 시간 17[s] | 손실 0.30\n",
            "| 에폭 20 |  반복 221 / 351 | 시간 18[s] | 손실 0.32\n",
            "| 에폭 20 |  반복 241 / 351 | 시간 20[s] | 손실 0.33\n",
            "| 에폭 20 |  반복 261 / 351 | 시간 22[s] | 손실 0.35\n",
            "| 에폭 20 |  반복 281 / 351 | 시간 23[s] | 손실 0.36\n",
            "| 에폭 20 |  반복 301 / 351 | 시간 25[s] | 손실 0.34\n",
            "| 에폭 20 |  반복 321 / 351 | 시간 27[s] | 손실 0.32\n",
            "| 에폭 20 |  반복 341 / 351 | 시간 28[s] | 손실 0.32\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[91m☒\u001b[0m 665 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 162 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[91m☒\u001b[0m 421 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 859 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1051\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1428\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[91m☒\u001b[0m 862 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 50.020%\n",
            "| 에폭 21 |  반복 1 / 351 | 시간 0[s] | 손실 0.31\n",
            "| 에폭 21 |  반복 21 / 351 | 시간 1[s] | 손실 0.31\n",
            "| 에폭 21 |  반복 41 / 351 | 시간 3[s] | 손실 0.30\n",
            "| 에폭 21 |  반복 61 / 351 | 시간 5[s] | 손실 0.30\n",
            "| 에폭 21 |  반복 81 / 351 | 시간 6[s] | 손실 0.32\n",
            "| 에폭 21 |  반복 101 / 351 | 시간 8[s] | 손실 0.32\n",
            "| 에폭 21 |  반복 121 / 351 | 시간 10[s] | 손실 0.31\n",
            "| 에폭 21 |  반복 141 / 351 | 시간 12[s] | 손실 0.30\n",
            "| 에폭 21 |  반복 161 / 351 | 시간 13[s] | 손실 0.31\n",
            "| 에폭 21 |  반복 181 / 351 | 시간 15[s] | 손실 0.31\n",
            "| 에폭 21 |  반복 201 / 351 | 시간 17[s] | 손실 0.31\n",
            "| 에폭 21 |  반복 221 / 351 | 시간 19[s] | 손실 0.32\n",
            "| 에폭 21 |  반복 241 / 351 | 시간 20[s] | 손실 0.31\n",
            "| 에폭 21 |  반복 261 / 351 | 시간 22[s] | 손실 0.29\n",
            "| 에폭 21 |  반복 281 / 351 | 시간 23[s] | 손실 0.30\n",
            "| 에폭 21 |  반복 301 / 351 | 시간 25[s] | 손실 0.29\n",
            "| 에폭 21 |  반복 321 / 351 | 시간 26[s] | 손실 0.29\n",
            "| 에폭 21 |  반복 341 / 351 | 시간 28[s] | 손실 0.29\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 162 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[91m☒\u001b[0m 420 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 859 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1052\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1428\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 50.860%\n",
            "| 에폭 22 |  반복 1 / 351 | 시간 0[s] | 손실 0.32\n",
            "| 에폭 22 |  반복 21 / 351 | 시간 1[s] | 손실 0.29\n",
            "| 에폭 22 |  반복 41 / 351 | 시간 3[s] | 손실 0.28\n",
            "| 에폭 22 |  반복 61 / 351 | 시간 4[s] | 손실 0.29\n",
            "| 에폭 22 |  반복 81 / 351 | 시간 6[s] | 손실 0.29\n",
            "| 에폭 22 |  반복 101 / 351 | 시간 7[s] | 손실 0.28\n",
            "| 에폭 22 |  반복 121 / 351 | 시간 9[s] | 손실 0.29\n",
            "| 에폭 22 |  반복 141 / 351 | 시간 10[s] | 손실 0.31\n",
            "| 에폭 22 |  반복 161 / 351 | 시간 12[s] | 손실 0.31\n",
            "| 에폭 22 |  반복 181 / 351 | 시간 13[s] | 손실 0.30\n",
            "| 에폭 22 |  반복 201 / 351 | 시간 15[s] | 손실 0.28\n",
            "| 에폭 22 |  반복 221 / 351 | 시간 16[s] | 손실 0.32\n",
            "| 에폭 22 |  반복 241 / 351 | 시간 18[s] | 손실 0.33\n",
            "| 에폭 22 |  반복 261 / 351 | 시간 19[s] | 손실 0.32\n",
            "| 에폭 22 |  반복 281 / 351 | 시간 21[s] | 손실 0.31\n",
            "| 에폭 22 |  반복 301 / 351 | 시간 22[s] | 손실 0.30\n",
            "| 에폭 22 |  반복 321 / 351 | 시간 24[s] | 손실 0.29\n",
            "| 에폭 22 |  반복 341 / 351 | 시간 25[s] | 손실 0.31\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[91m☒\u001b[0m 423 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 856 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1430\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[91m☒\u001b[0m 865 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 238 \n",
            "---\n",
            "검증 정확도 47.720%\n",
            "| 에폭 23 |  반복 1 / 351 | 시간 0[s] | 손실 0.31\n",
            "| 에폭 23 |  반복 21 / 351 | 시간 1[s] | 손실 0.31\n",
            "| 에폭 23 |  반복 41 / 351 | 시간 3[s] | 손실 0.28\n",
            "| 에폭 23 |  반복 61 / 351 | 시간 4[s] | 손실 0.28\n",
            "| 에폭 23 |  반복 81 / 351 | 시간 6[s] | 손실 0.29\n",
            "| 에폭 23 |  반복 101 / 351 | 시간 7[s] | 손실 0.27\n",
            "| 에폭 23 |  반복 121 / 351 | 시간 9[s] | 손실 0.28\n",
            "| 에폭 23 |  반복 141 / 351 | 시간 10[s] | 손실 0.28\n",
            "| 에폭 23 |  반복 161 / 351 | 시간 12[s] | 손실 0.30\n",
            "| 에폭 23 |  반복 181 / 351 | 시간 13[s] | 손실 0.30\n",
            "| 에폭 23 |  반복 201 / 351 | 시간 15[s] | 손실 0.29\n",
            "| 에폭 23 |  반복 221 / 351 | 시간 16[s] | 손실 0.29\n",
            "| 에폭 23 |  반복 241 / 351 | 시간 18[s] | 손실 0.28\n",
            "| 에폭 23 |  반복 261 / 351 | 시간 19[s] | 손실 0.28\n",
            "| 에폭 23 |  반복 281 / 351 | 시간 21[s] | 손실 0.30\n",
            "| 에폭 23 |  반복 301 / 351 | 시간 22[s] | 손실 0.29\n",
            "| 에폭 23 |  반복 321 / 351 | 시간 24[s] | 손실 0.29\n",
            "| 에폭 23 |  반복 341 / 351 | 시간 25[s] | 손실 0.28\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1142\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 858 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1429\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[91m☒\u001b[0m 866 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 238 \n",
            "---\n",
            "검증 정확도 45.180%\n",
            "| 에폭 24 |  반복 1 / 351 | 시간 0[s] | 손실 0.31\n",
            "| 에폭 24 |  반복 21 / 351 | 시간 1[s] | 손실 0.29\n",
            "| 에폭 24 |  반복 41 / 351 | 시간 3[s] | 손실 0.28\n",
            "| 에폭 24 |  반복 61 / 351 | 시간 4[s] | 손실 0.29\n",
            "| 에폭 24 |  반복 81 / 351 | 시간 6[s] | 손실 0.30\n",
            "| 에폭 24 |  반복 101 / 351 | 시간 7[s] | 손실 0.29\n",
            "| 에폭 24 |  반복 121 / 351 | 시간 9[s] | 손실 0.29\n",
            "| 에폭 24 |  반복 141 / 351 | 시간 10[s] | 손실 0.29\n",
            "| 에폭 24 |  반복 161 / 351 | 시간 12[s] | 손실 0.28\n",
            "| 에폭 24 |  반복 181 / 351 | 시간 13[s] | 손실 0.29\n",
            "| 에폭 24 |  반복 201 / 351 | 시간 15[s] | 손실 0.28\n",
            "| 에폭 24 |  반복 221 / 351 | 시간 16[s] | 손실 0.28\n",
            "| 에폭 24 |  반복 241 / 351 | 시간 18[s] | 손실 0.29\n",
            "| 에폭 24 |  반복 261 / 351 | 시간 19[s] | 손실 0.29\n",
            "| 에폭 24 |  반복 281 / 351 | 시간 21[s] | 손실 0.29\n",
            "| 에폭 24 |  반복 301 / 351 | 시간 22[s] | 손실 0.28\n",
            "| 에폭 24 |  반복 321 / 351 | 시간 24[s] | 손실 0.27\n",
            "| 에폭 24 |  반복 341 / 351 | 시간 25[s] | 손실 0.29\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[91m☒\u001b[0m 421 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1054\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1428\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 237 \n",
            "---\n",
            "검증 정확도 51.920%\n",
            "| 에폭 25 |  반복 1 / 351 | 시간 0[s] | 손실 0.29\n",
            "| 에폭 25 |  반복 21 / 351 | 시간 1[s] | 손실 0.29\n",
            "| 에폭 25 |  반복 41 / 351 | 시간 3[s] | 손실 0.28\n",
            "| 에폭 25 |  반복 61 / 351 | 시간 4[s] | 손실 0.26\n",
            "| 에폭 25 |  반복 81 / 351 | 시간 6[s] | 손실 0.26\n",
            "| 에폭 25 |  반복 101 / 351 | 시간 7[s] | 손실 0.27\n",
            "| 에폭 25 |  반복 121 / 351 | 시간 9[s] | 손실 0.29\n",
            "| 에폭 25 |  반복 141 / 351 | 시간 10[s] | 손실 0.28\n",
            "| 에폭 25 |  반복 161 / 351 | 시간 12[s] | 손실 0.28\n",
            "| 에폭 25 |  반복 181 / 351 | 시간 13[s] | 손실 0.28\n",
            "| 에폭 25 |  반복 201 / 351 | 시간 15[s] | 손실 0.27\n",
            "| 에폭 25 |  반복 221 / 351 | 시간 16[s] | 손실 0.29\n",
            "| 에폭 25 |  반복 241 / 351 | 시간 18[s] | 손실 0.27\n",
            "| 에폭 25 |  반복 261 / 351 | 시간 19[s] | 손실 0.28\n",
            "| 에폭 25 |  반복 281 / 351 | 시간 20[s] | 손실 0.28\n",
            "| 에폭 25 |  반복 301 / 351 | 시간 22[s] | 손실 0.27\n",
            "| 에폭 25 |  반복 321 / 351 | 시간 23[s] | 손실 0.28\n",
            "| 에폭 25 |  반복 341 / 351 | 시간 25[s] | 손실 0.28\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1140\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 856 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1052\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1426\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 54.200%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cOp9ORCv8TM"
      },
      "source": [
        "# 엿보기(Peeky)\n",
        "# coding: utf-8\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master')\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master/ch07')\n",
        "from common.time_layers import *\n",
        "from seq2seq import Seq2seq, Encoder\n",
        "\n",
        "\n",
        "class PeekyDecoder:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(H + D, 4 * H) / np.sqrt(H + D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(H + H, V) / np.sqrt(H + H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
        "        self.affine = TimeAffine(affine_W, affine_b)\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in (self.embed, self.lstm, self.affine):\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, xs, h):\n",
        "        N, T = xs.shape\n",
        "        N, H = h.shape\n",
        "\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        out = self.embed.forward(xs)\n",
        "        hs = np.repeat(h, T, axis=0).reshape(N, T, H)\n",
        "        out = np.concatenate((hs, out), axis=2)\n",
        "\n",
        "        out = self.lstm.forward(out)\n",
        "        out = np.concatenate((hs, out), axis=2)\n",
        "\n",
        "        score = self.affine.forward(out)\n",
        "        self.cache = H\n",
        "        return score\n",
        "\n",
        "    def backward(self, dscore):\n",
        "        H = self.cache\n",
        "\n",
        "        dout = self.affine.backward(dscore)\n",
        "        dout, dhs0 = dout[:, :, H:], dout[:, :, :H]\n",
        "        dout = self.lstm.backward(dout)\n",
        "        dembed, dhs1 = dout[:, :, H:], dout[:, :, :H]\n",
        "        self.embed.backward(dembed)\n",
        "\n",
        "        dhs = dhs0 + dhs1\n",
        "        dh = self.lstm.dh + np.sum(dhs, axis=1)\n",
        "        return dh\n",
        "\n",
        "    def generate(self, h, start_id, sample_size):\n",
        "        sampled = []\n",
        "        char_id = start_id\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        H = h.shape[1]\n",
        "        peeky_h = h.reshape(1, 1, H)\n",
        "        for _ in range(sample_size):\n",
        "            x = np.array([char_id]).reshape((1, 1))\n",
        "            out = self.embed.forward(x)\n",
        "\n",
        "            out = np.concatenate((peeky_h, out), axis=2)\n",
        "            out = self.lstm.forward(out)\n",
        "            out = np.concatenate((peeky_h, out), axis=2)\n",
        "            score = self.affine.forward(out)\n",
        "\n",
        "            char_id = np.argmax(score.flatten())\n",
        "            sampled.append(char_id)\n",
        "\n",
        "        return sampled\n",
        "\n",
        "\n",
        "class PeekySeq2seq(Seq2seq):\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        self.encoder = Encoder(V, D, H)\n",
        "        self.decoder = PeekyDecoder(V, D, H)\n",
        "        self.softmax = TimeSoftmaxWithLoss()\n",
        "\n",
        "        self.params = self.encoder.params + self.decoder.params\n",
        "        self.grads = self.encoder.grads + self.decoder.grads\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsbTkHNQESUC"
      },
      "source": [
        "hidden_size = 128\n",
        "wordvec_size = 16\n",
        "vocab_size = len(char_to_id)\n",
        "model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WSo4SGVhmyN",
        "outputId": "33697789-d81e-4f14-99f3-83ebf03f0f0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master')\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master/ch07')\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from dataset import sequence\n",
        "from common.optimizer import Adam\n",
        "from common.trainer import Trainer\n",
        "from common.util import eval_seq2seq\n",
        "from seq2seq import Seq2seq\n",
        "from peeky_seq2seq import PeekySeq2seq\n",
        "\n",
        "# 데이터셋 읽기\n",
        "(x_train, t_train), (x_test, t_test) = sequence.load_data('addition.txt')\n",
        "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]  # 배열의 행을 반전시키면 정답률이 매우 높아짐.\n",
        "char_to_id, id_to_char = sequence.get_vocab()\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "vocab_size=  len(char_to_id)\n",
        "wordvec_size = 16\n",
        "hidden_size = 128\n",
        "batch_size = 128\n",
        "max_epoch = 25\n",
        "max_grad = 5.0\n",
        "\n",
        "# 모델 / 옵티마이저 / 트레이너 생성\n",
        "\n",
        "model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "acc_list = []\n",
        "for epoch in range(max_epoch):\n",
        "  trainer.fit(x_train ,t_train, max_epoch = 1, batch_size = batch_size, max_grad = max_grad)\n",
        "\n",
        "  correct_num = 0 \n",
        "  for i in range(len(x_test)):\n",
        "    question, correct = x_test[[i]], t_test[[i]]\n",
        "    verbose = i < 10\n",
        "    correct_num += eval_seq2seq(model, question, correct, id_to_char, verbose)\n",
        "\n",
        "  acc = float(correct_num) / len(x_test)\n",
        "  acc_list.append(acc)\n",
        "  print('검증 정확도 %.3f%%' %(acc*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 2.57\n",
            "| 에폭 1 |  반복 21 / 351 | 시간 1[s] | 손실 2.48\n",
            "| 에폭 1 |  반복 41 / 351 | 시간 2[s] | 손실 2.20\n",
            "| 에폭 1 |  반복 61 / 351 | 시간 4[s] | 손실 1.99\n",
            "| 에폭 1 |  반복 81 / 351 | 시간 5[s] | 손실 1.89\n",
            "| 에폭 1 |  반복 101 / 351 | 시간 6[s] | 손실 1.82\n",
            "| 에폭 1 |  반복 121 / 351 | 시간 8[s] | 손실 1.82\n",
            "| 에폭 1 |  반복 141 / 351 | 시간 9[s] | 손실 1.80\n",
            "| 에폭 1 |  반복 161 / 351 | 시간 10[s] | 손실 1.79\n",
            "| 에폭 1 |  반복 181 / 351 | 시간 12[s] | 손실 1.78\n",
            "| 에폭 1 |  반복 201 / 351 | 시간 13[s] | 손실 1.77\n",
            "| 에폭 1 |  반복 221 / 351 | 시간 15[s] | 손실 1.76\n",
            "| 에폭 1 |  반복 241 / 351 | 시간 16[s] | 손실 1.76\n",
            "| 에폭 1 |  반복 261 / 351 | 시간 17[s] | 손실 1.75\n",
            "| 에폭 1 |  반복 281 / 351 | 시간 19[s] | 손실 1.74\n",
            "| 에폭 1 |  반복 301 / 351 | 시간 20[s] | 손실 1.74\n",
            "| 에폭 1 |  반복 321 / 351 | 시간 22[s] | 손실 1.73\n",
            "| 에폭 1 |  반복 341 / 351 | 시간 23[s] | 손실 1.73\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[91m☒\u001b[0m 100 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1013\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[91m☒\u001b[0m 102 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 100 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[91m☒\u001b[0m 1023\n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 1023\n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1023\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1111\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[91m☒\u001b[0m 102 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 102 \n",
            "---\n",
            "검증 정확도 0.280%\n",
            "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 1.71\n",
            "| 에폭 2 |  반복 21 / 351 | 시간 1[s] | 손실 1.71\n",
            "| 에폭 2 |  반복 41 / 351 | 시간 2[s] | 손실 1.71\n",
            "| 에폭 2 |  반복 61 / 351 | 시간 4[s] | 손실 1.71\n",
            "| 에폭 2 |  반복 81 / 351 | 시간 5[s] | 손실 1.70\n",
            "| 에폭 2 |  반복 101 / 351 | 시간 7[s] | 손실 1.68\n",
            "| 에폭 2 |  반복 121 / 351 | 시간 8[s] | 손실 1.69\n",
            "| 에폭 2 |  반복 141 / 351 | 시간 9[s] | 손실 1.68\n",
            "| 에폭 2 |  반복 161 / 351 | 시간 11[s] | 손실 1.67\n",
            "| 에폭 2 |  반복 181 / 351 | 시간 12[s] | 손실 1.67\n",
            "| 에폭 2 |  반복 201 / 351 | 시간 14[s] | 손실 1.65\n",
            "| 에폭 2 |  반복 221 / 351 | 시간 15[s] | 손실 1.65\n",
            "| 에폭 2 |  반복 241 / 351 | 시간 17[s] | 손실 1.65\n",
            "| 에폭 2 |  반복 261 / 351 | 시간 18[s] | 손실 1.63\n",
            "| 에폭 2 |  반복 281 / 351 | 시간 20[s] | 손실 1.62\n",
            "| 에폭 2 |  반복 301 / 351 | 시간 21[s] | 손실 1.61\n",
            "| 에폭 2 |  반복 321 / 351 | 시간 23[s] | 손실 1.61\n",
            "| 에폭 2 |  반복 341 / 351 | 시간 24[s] | 손실 1.60\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[91m☒\u001b[0m 100 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1200\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[91m☒\u001b[0m 690 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 100 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[91m☒\u001b[0m 690 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 999 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1029\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1240\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[91m☒\u001b[0m 792 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 290 \n",
            "---\n",
            "검증 정확도 0.400%\n",
            "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 1.58\n",
            "| 에폭 3 |  반복 21 / 351 | 시간 1[s] | 손실 1.59\n",
            "| 에폭 3 |  반복 41 / 351 | 시간 2[s] | 손실 1.58\n",
            "| 에폭 3 |  반복 61 / 351 | 시간 4[s] | 손실 1.56\n",
            "| 에폭 3 |  반복 81 / 351 | 시간 5[s] | 손실 1.55\n",
            "| 에폭 3 |  반복 101 / 351 | 시간 7[s] | 손실 1.53\n",
            "| 에폭 3 |  반복 121 / 351 | 시간 8[s] | 손실 1.51\n",
            "| 에폭 3 |  반복 141 / 351 | 시간 10[s] | 손실 1.50\n",
            "| 에폭 3 |  반복 161 / 351 | 시간 11[s] | 손실 1.49\n",
            "| 에폭 3 |  반복 181 / 351 | 시간 13[s] | 손실 1.47\n",
            "| 에폭 3 |  반복 201 / 351 | 시간 14[s] | 손실 1.46\n",
            "| 에폭 3 |  반복 221 / 351 | 시간 16[s] | 손실 1.43\n",
            "| 에폭 3 |  반복 241 / 351 | 시간 17[s] | 손실 1.42\n",
            "| 에폭 3 |  반복 261 / 351 | 시간 18[s] | 손실 1.41\n",
            "| 에폭 3 |  반복 281 / 351 | 시간 20[s] | 손실 1.39\n",
            "| 에폭 3 |  반복 301 / 351 | 시간 21[s] | 손실 1.37\n",
            "| 에폭 3 |  반복 321 / 351 | 시간 23[s] | 손실 1.36\n",
            "| 에폭 3 |  반복 341 / 351 | 시간 24[s] | 손실 1.35\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[91m☒\u001b[0m 154 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1033\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[91m☒\u001b[0m 644 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 161 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[91m☒\u001b[0m 433 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 818 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1018\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1344\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[91m☒\u001b[0m 834 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 211 \n",
            "---\n",
            "검증 정확도 1.600%\n",
            "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 1.32\n",
            "| 에폭 4 |  반복 21 / 351 | 시간 1[s] | 손실 1.32\n",
            "| 에폭 4 |  반복 41 / 351 | 시간 3[s] | 손실 1.30\n",
            "| 에폭 4 |  반복 61 / 351 | 시간 4[s] | 손실 1.30\n",
            "| 에폭 4 |  반복 81 / 351 | 시간 6[s] | 손실 1.28\n",
            "| 에폭 4 |  반복 101 / 351 | 시간 7[s] | 손실 1.27\n",
            "| 에폭 4 |  반복 121 / 351 | 시간 8[s] | 손실 1.25\n",
            "| 에폭 4 |  반복 141 / 351 | 시간 10[s] | 손실 1.24\n",
            "| 에폭 4 |  반복 161 / 351 | 시간 11[s] | 손실 1.22\n",
            "| 에폭 4 |  반복 181 / 351 | 시간 13[s] | 손실 1.21\n",
            "| 에폭 4 |  반복 201 / 351 | 시간 15[s] | 손실 1.20\n",
            "| 에폭 4 |  반복 221 / 351 | 시간 16[s] | 손실 1.20\n",
            "| 에폭 4 |  반복 241 / 351 | 시간 17[s] | 손실 1.17\n",
            "| 에폭 4 |  반복 261 / 351 | 시간 19[s] | 손실 1.16\n",
            "| 에폭 4 |  반복 281 / 351 | 시간 20[s] | 손실 1.14\n",
            "| 에폭 4 |  반복 301 / 351 | 시간 22[s] | 손실 1.12\n",
            "| 에폭 4 |  반복 321 / 351 | 시간 23[s] | 손실 1.11\n",
            "| 에폭 4 |  반복 341 / 351 | 시간 25[s] | 손실 1.10\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[91m☒\u001b[0m 158 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1123\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[91m☒\u001b[0m 657 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 165 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[91m☒\u001b[0m 423 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 777 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1023\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1388\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[91m☒\u001b[0m 887 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 223 \n",
            "---\n",
            "검증 정확도 5.140%\n",
            "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 1.08\n",
            "| 에폭 5 |  반복 21 / 351 | 시간 1[s] | 손실 1.07\n",
            "| 에폭 5 |  반복 41 / 351 | 시간 3[s] | 손실 1.05\n",
            "| 에폭 5 |  반복 61 / 351 | 시간 4[s] | 손실 1.04\n",
            "| 에폭 5 |  반복 81 / 351 | 시간 6[s] | 손실 1.02\n",
            "| 에폭 5 |  반복 101 / 351 | 시간 7[s] | 손실 1.01\n",
            "| 에폭 5 |  반복 121 / 351 | 시간 8[s] | 손실 1.00\n",
            "| 에폭 5 |  반복 141 / 351 | 시간 10[s] | 손실 0.99\n",
            "| 에폭 5 |  반복 161 / 351 | 시간 12[s] | 손실 0.99\n",
            "| 에폭 5 |  반복 181 / 351 | 시간 13[s] | 손실 0.96\n",
            "| 에폭 5 |  반복 201 / 351 | 시간 14[s] | 손실 0.95\n",
            "| 에폭 5 |  반복 221 / 351 | 시간 16[s] | 손실 0.94\n",
            "| 에폭 5 |  반복 241 / 351 | 시간 17[s] | 손실 0.92\n",
            "| 에폭 5 |  반복 261 / 351 | 시간 19[s] | 손실 0.91\n",
            "| 에폭 5 |  반복 281 / 351 | 시간 20[s] | 손실 0.90\n",
            "| 에폭 5 |  반복 301 / 351 | 시간 22[s] | 손실 0.89\n",
            "| 에폭 5 |  반복 321 / 351 | 시간 24[s] | 손실 0.88\n",
            "| 에폭 5 |  반복 341 / 351 | 시간 25[s] | 손실 0.87\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[91m☒\u001b[0m 160 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1135\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[91m☒\u001b[0m 668 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 169 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 861 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1045\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1324\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[91m☒\u001b[0m 861 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 239 \n",
            "---\n",
            "검증 정확도 9.380%\n",
            "| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 0.90\n",
            "| 에폭 6 |  반복 21 / 351 | 시간 1[s] | 손실 0.86\n",
            "| 에폭 6 |  반복 41 / 351 | 시간 3[s] | 손실 0.83\n",
            "| 에폭 6 |  반복 61 / 351 | 시간 4[s] | 손실 0.84\n",
            "| 에폭 6 |  반복 81 / 351 | 시간 6[s] | 손실 0.82\n",
            "| 에폭 6 |  반복 101 / 351 | 시간 7[s] | 손실 0.81\n",
            "| 에폭 6 |  반복 121 / 351 | 시간 9[s] | 손실 0.80\n",
            "| 에폭 6 |  반복 141 / 351 | 시간 10[s] | 손실 0.79\n",
            "| 에폭 6 |  반복 161 / 351 | 시간 11[s] | 손실 0.78\n",
            "| 에폭 6 |  반복 181 / 351 | 시간 13[s] | 손실 0.77\n",
            "| 에폭 6 |  반복 201 / 351 | 시간 14[s] | 손실 0.76\n",
            "| 에폭 6 |  반복 221 / 351 | 시간 16[s] | 손실 0.76\n",
            "| 에폭 6 |  반복 241 / 351 | 시간 17[s] | 손실 0.74\n",
            "| 에폭 6 |  반복 261 / 351 | 시간 19[s] | 손실 0.74\n",
            "| 에폭 6 |  반복 281 / 351 | 시간 20[s] | 손실 0.73\n",
            "| 에폭 6 |  반복 301 / 351 | 시간 22[s] | 손실 0.72\n",
            "| 에폭 6 |  반복 321 / 351 | 시간 23[s] | 손실 0.72\n",
            "| 에폭 6 |  반복 341 / 351 | 시간 25[s] | 손실 0.71\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[91m☒\u001b[0m 163 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1138\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[91m☒\u001b[0m 668 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 166 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[91m☒\u001b[0m 423 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 858 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1048\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1428\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[91m☒\u001b[0m 873 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 239 \n",
            "---\n",
            "검증 정확도 15.040%\n",
            "| 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 7 |  반복 21 / 351 | 시간 1[s] | 손실 0.69\n",
            "| 에폭 7 |  반복 41 / 351 | 시간 3[s] | 손실 0.67\n",
            "| 에폭 7 |  반복 61 / 351 | 시간 4[s] | 손실 0.66\n",
            "| 에폭 7 |  반복 81 / 351 | 시간 6[s] | 손실 0.66\n",
            "| 에폭 7 |  반복 101 / 351 | 시간 7[s] | 손실 0.65\n",
            "| 에폭 7 |  반복 121 / 351 | 시간 9[s] | 손실 0.65\n",
            "| 에폭 7 |  반복 141 / 351 | 시간 10[s] | 손실 0.64\n",
            "| 에폭 7 |  반복 161 / 351 | 시간 12[s] | 손실 0.63\n",
            "| 에폭 7 |  반복 181 / 351 | 시간 13[s] | 손실 0.61\n",
            "| 에폭 7 |  반복 201 / 351 | 시간 15[s] | 손실 0.61\n",
            "| 에폭 7 |  반복 221 / 351 | 시간 16[s] | 손실 0.60\n",
            "| 에폭 7 |  반복 241 / 351 | 시간 18[s] | 손실 0.57\n",
            "| 에폭 7 |  반복 261 / 351 | 시간 19[s] | 손실 0.57\n",
            "| 에폭 7 |  반복 281 / 351 | 시간 21[s] | 손실 0.57\n",
            "| 에폭 7 |  반복 301 / 351 | 시간 22[s] | 손실 0.55\n",
            "| 에폭 7 |  반복 321 / 351 | 시간 23[s] | 손실 0.54\n",
            "| 에폭 7 |  반복 341 / 351 | 시간 25[s] | 손실 0.53\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[91m☒\u001b[0m 665 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 156 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 858 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1052\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1428\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 235 \n",
            "---\n",
            "검증 정확도 39.100%\n",
            "| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 8 |  반복 21 / 351 | 시간 1[s] | 손실 0.50\n",
            "| 에폭 8 |  반복 41 / 351 | 시간 3[s] | 손실 0.49\n",
            "| 에폭 8 |  반복 61 / 351 | 시간 4[s] | 손실 0.48\n",
            "| 에폭 8 |  반복 81 / 351 | 시간 6[s] | 손실 0.47\n",
            "| 에폭 8 |  반복 101 / 351 | 시간 7[s] | 손실 0.46\n",
            "| 에폭 8 |  반복 121 / 351 | 시간 9[s] | 손실 0.46\n",
            "| 에폭 8 |  반복 141 / 351 | 시간 10[s] | 손실 0.44\n",
            "| 에폭 8 |  반복 161 / 351 | 시간 12[s] | 손실 0.41\n",
            "| 에폭 8 |  반복 181 / 351 | 시간 13[s] | 손실 0.42\n",
            "| 에폭 8 |  반복 201 / 351 | 시간 15[s] | 손실 0.41\n",
            "| 에폭 8 |  반복 221 / 351 | 시간 16[s] | 손실 0.40\n",
            "| 에폭 8 |  반복 241 / 351 | 시간 18[s] | 손실 0.39\n",
            "| 에폭 8 |  반복 261 / 351 | 시간 19[s] | 손실 0.37\n",
            "| 에폭 8 |  반복 281 / 351 | 시간 21[s] | 손실 0.36\n",
            "| 에폭 8 |  반복 301 / 351 | 시간 22[s] | 손실 0.36\n",
            "| 에폭 8 |  반복 321 / 351 | 시간 24[s] | 손실 0.35\n",
            "| 에폭 8 |  반복 341 / 351 | 시간 25[s] | 손실 0.34\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[91m☒\u001b[0m 161 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[91m☒\u001b[0m 657 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 155 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1438\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 65.060%\n",
            "| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 0.32\n",
            "| 에폭 9 |  반복 21 / 351 | 시간 1[s] | 손실 0.31\n",
            "| 에폭 9 |  반복 41 / 351 | 시간 3[s] | 손실 0.31\n",
            "| 에폭 9 |  반복 61 / 351 | 시간 4[s] | 손실 0.31\n",
            "| 에폭 9 |  반복 81 / 351 | 시간 5[s] | 손실 0.29\n",
            "| 에폭 9 |  반복 101 / 351 | 시간 7[s] | 손실 0.29\n",
            "| 에폭 9 |  반복 121 / 351 | 시간 8[s] | 손실 0.29\n",
            "| 에폭 9 |  반복 141 / 351 | 시간 10[s] | 손실 0.27\n",
            "| 에폭 9 |  반복 161 / 351 | 시간 11[s] | 손실 0.27\n",
            "| 에폭 9 |  반복 181 / 351 | 시간 13[s] | 손실 0.26\n",
            "| 에폭 9 |  반복 201 / 351 | 시간 14[s] | 손실 0.25\n",
            "| 에폭 9 |  반복 221 / 351 | 시간 16[s] | 손실 0.25\n",
            "| 에폭 9 |  반복 241 / 351 | 시간 17[s] | 손실 0.24\n",
            "| 에폭 9 |  반복 261 / 351 | 시간 19[s] | 손실 0.24\n",
            "| 에폭 9 |  반복 281 / 351 | 시간 20[s] | 손실 0.23\n",
            "| 에폭 9 |  반복 301 / 351 | 시간 22[s] | 손실 0.22\n",
            "| 에폭 9 |  반복 321 / 351 | 시간 23[s] | 손실 0.22\n",
            "| 에폭 9 |  반복 341 / 351 | 시간 25[s] | 손실 0.21\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1140\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[91m☒\u001b[0m 657 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[92m☑\u001b[0m 1427\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 83.280%\n",
            "| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 0.22\n",
            "| 에폭 10 |  반복 21 / 351 | 시간 1[s] | 손실 0.20\n",
            "| 에폭 10 |  반복 41 / 351 | 시간 3[s] | 손실 0.20\n",
            "| 에폭 10 |  반복 61 / 351 | 시간 4[s] | 손실 0.20\n",
            "| 에폭 10 |  반복 81 / 351 | 시간 6[s] | 손실 0.18\n",
            "| 에폭 10 |  반복 101 / 351 | 시간 7[s] | 손실 0.17\n",
            "| 에폭 10 |  반복 121 / 351 | 시간 9[s] | 손실 0.18\n",
            "| 에폭 10 |  반복 141 / 351 | 시간 10[s] | 손실 0.17\n",
            "| 에폭 10 |  반복 161 / 351 | 시간 12[s] | 손실 0.17\n",
            "| 에폭 10 |  반복 181 / 351 | 시간 13[s] | 손실 0.17\n",
            "| 에폭 10 |  반복 201 / 351 | 시간 15[s] | 손실 0.17\n",
            "| 에폭 10 |  반복 221 / 351 | 시간 16[s] | 손실 0.16\n",
            "| 에폭 10 |  반복 241 / 351 | 시간 18[s] | 손실 0.15\n",
            "| 에폭 10 |  반복 261 / 351 | 시간 19[s] | 손실 0.15\n",
            "| 에폭 10 |  반복 281 / 351 | 시간 21[s] | 손실 0.15\n",
            "| 에폭 10 |  반복 301 / 351 | 시간 22[s] | 손실 0.15\n",
            "| 에폭 10 |  반복 321 / 351 | 시간 23[s] | 손실 0.14\n",
            "| 에폭 10 |  반복 341 / 351 | 시간 25[s] | 손실 0.14\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[91m☒\u001b[0m 656 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[92m☑\u001b[0m 1427\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 88.400%\n",
            "| 에폭 11 |  반복 1 / 351 | 시간 0[s] | 손실 0.13\n",
            "| 에폭 11 |  반복 21 / 351 | 시간 1[s] | 손실 0.13\n",
            "| 에폭 11 |  반복 41 / 351 | 시간 3[s] | 손실 0.13\n",
            "| 에폭 11 |  반복 61 / 351 | 시간 4[s] | 손실 0.12\n",
            "| 에폭 11 |  반복 81 / 351 | 시간 6[s] | 손실 0.12\n",
            "| 에폭 11 |  반복 101 / 351 | 시간 7[s] | 손실 0.12\n",
            "| 에폭 11 |  반복 121 / 351 | 시간 8[s] | 손실 0.11\n",
            "| 에폭 11 |  반복 141 / 351 | 시간 10[s] | 손실 0.12\n",
            "| 에폭 11 |  반복 161 / 351 | 시간 11[s] | 손실 0.11\n",
            "| 에폭 11 |  반복 181 / 351 | 시간 13[s] | 손실 0.11\n",
            "| 에폭 11 |  반복 201 / 351 | 시간 14[s] | 손실 0.12\n",
            "| 에폭 11 |  반복 221 / 351 | 시간 16[s] | 손실 0.11\n",
            "| 에폭 11 |  반복 241 / 351 | 시간 17[s] | 손실 0.11\n",
            "| 에폭 11 |  반복 261 / 351 | 시간 19[s] | 손실 0.10\n",
            "| 에폭 11 |  반복 281 / 351 | 시간 20[s] | 손실 0.10\n",
            "| 에폭 11 |  반복 301 / 351 | 시간 22[s] | 손실 0.10\n",
            "| 에폭 11 |  반복 321 / 351 | 시간 23[s] | 손실 0.09\n",
            "| 에폭 11 |  반복 341 / 351 | 시간 25[s] | 손실 0.09\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[92m☑\u001b[0m 1427\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 90.940%\n",
            "| 에폭 12 |  반복 1 / 351 | 시간 0[s] | 손실 0.09\n",
            "| 에폭 12 |  반복 21 / 351 | 시간 1[s] | 손실 0.09\n",
            "| 에폭 12 |  반복 41 / 351 | 시간 3[s] | 손실 0.09\n",
            "| 에폭 12 |  반복 61 / 351 | 시간 4[s] | 손실 0.09\n",
            "| 에폭 12 |  반복 81 / 351 | 시간 6[s] | 손실 0.09\n",
            "| 에폭 12 |  반복 101 / 351 | 시간 7[s] | 손실 0.08\n",
            "| 에폭 12 |  반복 121 / 351 | 시간 8[s] | 손실 0.08\n",
            "| 에폭 12 |  반복 141 / 351 | 시간 10[s] | 손실 0.08\n",
            "| 에폭 12 |  반복 161 / 351 | 시간 11[s] | 손실 0.08\n",
            "| 에폭 12 |  반복 181 / 351 | 시간 13[s] | 손실 0.08\n",
            "| 에폭 12 |  반복 201 / 351 | 시간 14[s] | 손실 0.08\n",
            "| 에폭 12 |  반복 221 / 351 | 시간 16[s] | 손실 0.09\n",
            "| 에폭 12 |  반복 241 / 351 | 시간 17[s] | 손실 0.09\n",
            "| 에폭 12 |  반복 261 / 351 | 시간 19[s] | 손실 0.09\n",
            "| 에폭 12 |  반복 281 / 351 | 시간 20[s] | 손실 0.08\n",
            "| 에폭 12 |  반복 301 / 351 | 시간 22[s] | 손실 0.08\n",
            "| 에폭 12 |  반복 321 / 351 | 시간 23[s] | 손실 0.07\n",
            "| 에폭 12 |  반복 341 / 351 | 시간 25[s] | 손실 0.08\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[92m☑\u001b[0m 1427\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 92.220%\n",
            "| 에폭 13 |  반복 1 / 351 | 시간 0[s] | 손실 0.07\n",
            "| 에폭 13 |  반복 21 / 351 | 시간 1[s] | 손실 0.07\n",
            "| 에폭 13 |  반복 41 / 351 | 시간 3[s] | 손실 0.07\n",
            "| 에폭 13 |  반복 61 / 351 | 시간 4[s] | 손실 0.07\n",
            "| 에폭 13 |  반복 81 / 351 | 시간 6[s] | 손실 0.06\n",
            "| 에폭 13 |  반복 101 / 351 | 시간 7[s] | 손실 0.06\n",
            "| 에폭 13 |  반복 121 / 351 | 시간 9[s] | 손실 0.07\n",
            "| 에폭 13 |  반복 141 / 351 | 시간 10[s] | 손실 0.06\n",
            "| 에폭 13 |  반복 161 / 351 | 시간 11[s] | 손실 0.06\n",
            "| 에폭 13 |  반복 181 / 351 | 시간 13[s] | 손실 0.06\n",
            "| 에폭 13 |  반복 201 / 351 | 시간 15[s] | 손실 0.06\n",
            "| 에폭 13 |  반복 221 / 351 | 시간 16[s] | 손실 0.06\n",
            "| 에폭 13 |  반복 241 / 351 | 시간 18[s] | 손실 0.06\n",
            "| 에폭 13 |  반복 261 / 351 | 시간 19[s] | 손실 0.06\n",
            "| 에폭 13 |  반복 281 / 351 | 시간 20[s] | 손실 0.06\n",
            "| 에폭 13 |  반복 301 / 351 | 시간 22[s] | 손실 0.05\n",
            "| 에폭 13 |  반복 321 / 351 | 시간 23[s] | 손실 0.05\n",
            "| 에폭 13 |  반복 341 / 351 | 시간 25[s] | 손실 0.06\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[92m☑\u001b[0m 1427\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 94.380%\n",
            "| 에폭 14 |  반복 1 / 351 | 시간 0[s] | 손실 0.05\n",
            "| 에폭 14 |  반복 21 / 351 | 시간 1[s] | 손실 0.05\n",
            "| 에폭 14 |  반복 41 / 351 | 시간 3[s] | 손실 0.05\n",
            "| 에폭 14 |  반복 61 / 351 | 시간 4[s] | 손실 0.05\n",
            "| 에폭 14 |  반복 81 / 351 | 시간 5[s] | 손실 0.05\n",
            "| 에폭 14 |  반복 101 / 351 | 시간 7[s] | 손실 0.05\n",
            "| 에폭 14 |  반복 121 / 351 | 시간 8[s] | 손실 0.05\n",
            "| 에폭 14 |  반복 141 / 351 | 시간 10[s] | 손실 0.05\n",
            "| 에폭 14 |  반복 161 / 351 | 시간 11[s] | 손실 0.05\n",
            "| 에폭 14 |  반복 181 / 351 | 시간 13[s] | 손실 0.05\n",
            "| 에폭 14 |  반복 201 / 351 | 시간 14[s] | 손실 0.05\n",
            "| 에폭 14 |  반복 221 / 351 | 시간 16[s] | 손실 0.06\n",
            "| 에폭 14 |  반복 241 / 351 | 시간 17[s] | 손실 0.06\n",
            "| 에폭 14 |  반복 261 / 351 | 시간 19[s] | 손실 0.07\n",
            "| 에폭 14 |  반복 281 / 351 | 시간 20[s] | 손실 0.06\n",
            "| 에폭 14 |  반복 301 / 351 | 시간 22[s] | 손실 0.06\n",
            "| 에폭 14 |  반복 321 / 351 | 시간 23[s] | 손실 0.05\n",
            "| 에폭 14 |  반복 341 / 351 | 시간 25[s] | 손실 0.05\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[92m☑\u001b[0m 1427\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 94.880%\n",
            "| 에폭 15 |  반복 1 / 351 | 시간 0[s] | 손실 0.05\n",
            "| 에폭 15 |  반복 21 / 351 | 시간 1[s] | 손실 0.05\n",
            "| 에폭 15 |  반복 41 / 351 | 시간 3[s] | 손실 0.04\n",
            "| 에폭 15 |  반복 61 / 351 | 시간 4[s] | 손실 0.04\n",
            "| 에폭 15 |  반복 81 / 351 | 시간 6[s] | 손실 0.04\n",
            "| 에폭 15 |  반복 101 / 351 | 시간 7[s] | 손실 0.05\n",
            "| 에폭 15 |  반복 121 / 351 | 시간 8[s] | 손실 0.04\n",
            "| 에폭 15 |  반복 141 / 351 | 시간 10[s] | 손실 0.04\n",
            "| 에폭 15 |  반복 161 / 351 | 시간 11[s] | 손실 0.05\n",
            "| 에폭 15 |  반복 181 / 351 | 시간 13[s] | 손실 0.06\n",
            "| 에폭 15 |  반복 201 / 351 | 시간 14[s] | 손실 0.05\n",
            "| 에폭 15 |  반복 221 / 351 | 시간 16[s] | 손실 0.04\n",
            "| 에폭 15 |  반복 241 / 351 | 시간 17[s] | 손실 0.04\n",
            "| 에폭 15 |  반복 261 / 351 | 시간 19[s] | 손실 0.05\n",
            "| 에폭 15 |  반복 281 / 351 | 시간 20[s] | 손실 0.05\n",
            "| 에폭 15 |  반복 301 / 351 | 시간 22[s] | 손실 0.05\n",
            "| 에폭 15 |  반복 321 / 351 | 시간 23[s] | 손실 0.05\n",
            "| 에폭 15 |  반복 341 / 351 | 시간 25[s] | 손실 0.05\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[92m☑\u001b[0m 1427\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 94.660%\n",
            "| 에폭 16 |  반복 1 / 351 | 시간 0[s] | 손실 0.03\n",
            "| 에폭 16 |  반복 21 / 351 | 시간 1[s] | 손실 0.05\n",
            "| 에폭 16 |  반복 41 / 351 | 시간 3[s] | 손실 0.04\n",
            "| 에폭 16 |  반복 61 / 351 | 시간 4[s] | 손실 0.03\n",
            "| 에폭 16 |  반복 81 / 351 | 시간 6[s] | 손실 0.03\n",
            "| 에폭 16 |  반복 101 / 351 | 시간 7[s] | 손실 0.03\n",
            "| 에폭 16 |  반복 121 / 351 | 시간 9[s] | 손실 0.04\n",
            "| 에폭 16 |  반복 141 / 351 | 시간 10[s] | 손실 0.04\n",
            "| 에폭 16 |  반복 161 / 351 | 시간 12[s] | 손실 0.03\n",
            "| 에폭 16 |  반복 181 / 351 | 시간 13[s] | 손실 0.04\n",
            "| 에폭 16 |  반복 201 / 351 | 시간 15[s] | 손실 0.03\n",
            "| 에폭 16 |  반복 221 / 351 | 시간 16[s] | 손실 0.03\n",
            "| 에폭 16 |  반복 241 / 351 | 시간 18[s] | 손실 0.03\n",
            "| 에폭 16 |  반복 261 / 351 | 시간 19[s] | 손실 0.03\n",
            "| 에폭 16 |  반복 281 / 351 | 시간 21[s] | 손실 0.03\n",
            "| 에폭 16 |  반복 301 / 351 | 시간 22[s] | 손실 0.03\n",
            "| 에폭 16 |  반복 321 / 351 | 시간 24[s] | 손실 0.03\n",
            "| 에폭 16 |  반복 341 / 351 | 시간 25[s] | 손실 0.04\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 162 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[92m☑\u001b[0m 1427\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 95.280%\n",
            "| 에폭 17 |  반복 1 / 351 | 시간 0[s] | 손실 0.04\n",
            "| 에폭 17 |  반복 21 / 351 | 시간 1[s] | 손실 0.03\n",
            "| 에폭 17 |  반복 41 / 351 | 시간 3[s] | 손실 0.04\n",
            "| 에폭 17 |  반복 61 / 351 | 시간 4[s] | 손실 0.03\n",
            "| 에폭 17 |  반복 81 / 351 | 시간 5[s] | 손실 0.03\n",
            "| 에폭 17 |  반복 101 / 351 | 시간 7[s] | 손실 0.04\n",
            "| 에폭 17 |  반복 121 / 351 | 시간 8[s] | 손실 0.04\n",
            "| 에폭 17 |  반복 141 / 351 | 시간 10[s] | 손실 0.05\n",
            "| 에폭 17 |  반복 161 / 351 | 시간 11[s] | 손실 0.05\n",
            "| 에폭 17 |  반복 181 / 351 | 시간 13[s] | 손실 0.05\n",
            "| 에폭 17 |  반복 201 / 351 | 시간 14[s] | 손실 0.05\n",
            "| 에폭 17 |  반복 221 / 351 | 시간 16[s] | 손실 0.04\n",
            "| 에폭 17 |  반복 241 / 351 | 시간 17[s] | 손실 0.03\n",
            "| 에폭 17 |  반복 261 / 351 | 시간 19[s] | 손실 0.03\n",
            "| 에폭 17 |  반복 281 / 351 | 시간 20[s] | 손실 0.03\n",
            "| 에폭 17 |  반복 301 / 351 | 시간 22[s] | 손실 0.03\n",
            "| 에폭 17 |  반복 321 / 351 | 시간 23[s] | 손실 0.03\n",
            "| 에폭 17 |  반복 341 / 351 | 시간 25[s] | 손실 0.04\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[92m☑\u001b[0m 1427\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 94.500%\n",
            "| 에폭 18 |  반복 1 / 351 | 시간 0[s] | 손실 0.04\n",
            "| 에폭 18 |  반복 21 / 351 | 시간 1[s] | 손실 0.03\n",
            "| 에폭 18 |  반복 41 / 351 | 시간 3[s] | 손실 0.02\n",
            "| 에폭 18 |  반복 61 / 351 | 시간 4[s] | 손실 0.02\n",
            "| 에폭 18 |  반복 81 / 351 | 시간 6[s] | 손실 0.03\n",
            "| 에폭 18 |  반복 101 / 351 | 시간 7[s] | 손실 0.02\n",
            "| 에폭 18 |  반복 121 / 351 | 시간 9[s] | 손실 0.02\n",
            "| 에폭 18 |  반복 141 / 351 | 시간 10[s] | 손실 0.02\n",
            "| 에폭 18 |  반복 161 / 351 | 시간 11[s] | 손실 0.02\n",
            "| 에폭 18 |  반복 181 / 351 | 시간 13[s] | 손실 0.02\n",
            "| 에폭 18 |  반복 201 / 351 | 시간 14[s] | 손실 0.02\n",
            "| 에폭 18 |  반복 221 / 351 | 시간 16[s] | 손실 0.02\n",
            "| 에폭 18 |  반복 241 / 351 | 시간 17[s] | 손실 0.03\n",
            "| 에폭 18 |  반복 261 / 351 | 시간 19[s] | 손실 0.04\n",
            "| 에폭 18 |  반복 281 / 351 | 시간 20[s] | 손실 0.05\n",
            "| 에폭 18 |  반복 301 / 351 | 시간 22[s] | 손실 0.04\n",
            "| 에폭 18 |  반복 321 / 351 | 시간 23[s] | 손실 0.04\n",
            "| 에폭 18 |  반복 341 / 351 | 시간 25[s] | 손실 0.03\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[92m☑\u001b[0m 1427\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 94.660%\n",
            "| 에폭 19 |  반복 1 / 351 | 시간 0[s] | 손실 0.04\n",
            "| 에폭 19 |  반복 21 / 351 | 시간 1[s] | 손실 0.04\n",
            "| 에폭 19 |  반복 41 / 351 | 시간 3[s] | 손실 0.07\n",
            "| 에폭 19 |  반복 61 / 351 | 시간 4[s] | 손실 0.06\n",
            "| 에폭 19 |  반복 81 / 351 | 시간 6[s] | 손실 0.05\n",
            "| 에폭 19 |  반복 101 / 351 | 시간 7[s] | 손실 0.04\n",
            "| 에폭 19 |  반복 121 / 351 | 시간 8[s] | 손실 0.03\n",
            "| 에폭 19 |  반복 141 / 351 | 시간 10[s] | 손실 0.03\n",
            "| 에폭 19 |  반복 161 / 351 | 시간 12[s] | 손실 0.03\n",
            "| 에폭 19 |  반복 181 / 351 | 시간 13[s] | 손실 0.03\n",
            "| 에폭 19 |  반복 201 / 351 | 시간 15[s] | 손실 0.02\n",
            "| 에폭 19 |  반복 221 / 351 | 시간 16[s] | 손실 0.02\n",
            "| 에폭 19 |  반복 241 / 351 | 시간 18[s] | 손실 0.02\n",
            "| 에폭 19 |  반복 261 / 351 | 시간 19[s] | 손실 0.02\n",
            "| 에폭 19 |  반복 281 / 351 | 시간 21[s] | 손실 0.02\n",
            "| 에폭 19 |  반복 301 / 351 | 시간 23[s] | 손실 0.02\n",
            "| 에폭 19 |  반복 321 / 351 | 시간 24[s] | 손실 0.02\n",
            "| 에폭 19 |  반복 341 / 351 | 시간 26[s] | 손실 0.02\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[92m☑\u001b[0m 1427\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 98.640%\n",
            "| 에폭 20 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
            "| 에폭 20 |  반복 21 / 351 | 시간 1[s] | 손실 0.01\n",
            "| 에폭 20 |  반복 41 / 351 | 시간 3[s] | 손실 0.01\n",
            "| 에폭 20 |  반복 61 / 351 | 시간 4[s] | 손실 0.01\n",
            "| 에폭 20 |  반복 81 / 351 | 시간 6[s] | 손실 0.01\n",
            "| 에폭 20 |  반복 101 / 351 | 시간 7[s] | 손실 0.01\n",
            "| 에폭 20 |  반복 121 / 351 | 시간 9[s] | 손실 0.01\n",
            "| 에폭 20 |  반복 141 / 351 | 시간 11[s] | 손실 0.01\n",
            "| 에폭 20 |  반복 161 / 351 | 시간 12[s] | 손실 0.02\n",
            "| 에폭 20 |  반복 181 / 351 | 시간 14[s] | 손실 0.02\n",
            "| 에폭 20 |  반복 201 / 351 | 시간 15[s] | 손실 0.02\n",
            "| 에폭 20 |  반복 221 / 351 | 시간 17[s] | 손실 0.02\n",
            "| 에폭 20 |  반복 241 / 351 | 시간 18[s] | 손실 0.02\n",
            "| 에폭 20 |  반복 261 / 351 | 시간 20[s] | 손실 0.02\n",
            "| 에폭 20 |  반복 281 / 351 | 시간 22[s] | 손실 0.02\n",
            "| 에폭 20 |  반복 301 / 351 | 시간 23[s] | 손실 0.02\n",
            "| 에폭 20 |  반복 321 / 351 | 시간 25[s] | 손실 0.02\n",
            "| 에폭 20 |  반복 341 / 351 | 시간 26[s] | 손실 0.03\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 162 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1426\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 95.200%\n",
            "| 에폭 21 |  반복 1 / 351 | 시간 0[s] | 손실 0.06\n",
            "| 에폭 21 |  반복 21 / 351 | 시간 1[s] | 손실 0.03\n",
            "| 에폭 21 |  반복 41 / 351 | 시간 3[s] | 손실 0.03\n",
            "| 에폭 21 |  반복 61 / 351 | 시간 4[s] | 손실 0.03\n",
            "| 에폭 21 |  반복 81 / 351 | 시간 6[s] | 손실 0.03\n",
            "| 에폭 21 |  반복 101 / 351 | 시간 7[s] | 손실 0.04\n",
            "| 에폭 21 |  반복 121 / 351 | 시간 9[s] | 손실 0.05\n",
            "| 에폭 21 |  반복 141 / 351 | 시간 11[s] | 손실 0.06\n",
            "| 에폭 21 |  반복 161 / 351 | 시간 12[s] | 손실 0.04\n",
            "| 에폭 21 |  반복 181 / 351 | 시간 14[s] | 손실 0.03\n",
            "| 에폭 21 |  반복 201 / 351 | 시간 15[s] | 손실 0.03\n",
            "| 에폭 21 |  반복 221 / 351 | 시간 17[s] | 손실 0.03\n",
            "| 에폭 21 |  반복 241 / 351 | 시간 18[s] | 손실 0.02\n",
            "| 에폭 21 |  반복 261 / 351 | 시간 20[s] | 손실 0.03\n",
            "| 에폭 21 |  반복 281 / 351 | 시간 22[s] | 손실 0.03\n",
            "| 에폭 21 |  반복 301 / 351 | 시간 23[s] | 손실 0.02\n",
            "| 에폭 21 |  반복 321 / 351 | 시간 25[s] | 손실 0.02\n",
            "| 에폭 21 |  반복 341 / 351 | 시간 26[s] | 손실 0.02\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[92m☑\u001b[0m 1427\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 97.720%\n",
            "| 에폭 22 |  반복 1 / 351 | 시간 0[s] | 손실 0.02\n",
            "| 에폭 22 |  반복 21 / 351 | 시간 1[s] | 손실 0.01\n",
            "| 에폭 22 |  반복 41 / 351 | 시간 3[s] | 손실 0.02\n",
            "| 에폭 22 |  반복 61 / 351 | 시간 4[s] | 손실 0.01\n",
            "| 에폭 22 |  반복 81 / 351 | 시간 6[s] | 손실 0.01\n",
            "| 에폭 22 |  반복 101 / 351 | 시간 7[s] | 손실 0.01\n",
            "| 에폭 22 |  반복 121 / 351 | 시간 9[s] | 손실 0.01\n",
            "| 에폭 22 |  반복 141 / 351 | 시간 11[s] | 손실 0.01\n",
            "| 에폭 22 |  반복 161 / 351 | 시간 12[s] | 손실 0.01\n",
            "| 에폭 22 |  반복 181 / 351 | 시간 14[s] | 손실 0.01\n",
            "| 에폭 22 |  반복 201 / 351 | 시간 15[s] | 손실 0.01\n",
            "| 에폭 22 |  반복 221 / 351 | 시간 17[s] | 손실 0.01\n",
            "| 에폭 22 |  반복 241 / 351 | 시간 19[s] | 손실 0.01\n",
            "| 에폭 22 |  반복 261 / 351 | 시간 20[s] | 손실 0.01\n",
            "| 에폭 22 |  반복 281 / 351 | 시간 22[s] | 손실 0.01\n",
            "| 에폭 22 |  반복 301 / 351 | 시간 23[s] | 손실 0.01\n",
            "| 에폭 22 |  반복 321 / 351 | 시간 25[s] | 손실 0.01\n",
            "| 에폭 22 |  반복 341 / 351 | 시간 27[s] | 손실 0.02\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[92m☑\u001b[0m 1427\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 97.860%\n",
            "| 에폭 23 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
            "| 에폭 23 |  반복 21 / 351 | 시간 1[s] | 손실 0.01\n",
            "| 에폭 23 |  반복 41 / 351 | 시간 3[s] | 손실 0.01\n",
            "| 에폭 23 |  반복 61 / 351 | 시간 4[s] | 손실 0.01\n",
            "| 에폭 23 |  반복 81 / 351 | 시간 6[s] | 손실 0.01\n",
            "| 에폭 23 |  반복 101 / 351 | 시간 7[s] | 손실 0.01\n",
            "| 에폭 23 |  반복 121 / 351 | 시간 9[s] | 손실 0.01\n",
            "| 에폭 23 |  반복 141 / 351 | 시간 11[s] | 손실 0.02\n",
            "| 에폭 23 |  반복 161 / 351 | 시간 12[s] | 손실 0.02\n",
            "| 에폭 23 |  반복 181 / 351 | 시간 14[s] | 손실 0.02\n",
            "| 에폭 23 |  반복 201 / 351 | 시간 15[s] | 손실 0.02\n",
            "| 에폭 23 |  반복 221 / 351 | 시간 17[s] | 손실 0.02\n",
            "| 에폭 23 |  반복 241 / 351 | 시간 18[s] | 손실 0.03\n",
            "| 에폭 23 |  반복 261 / 351 | 시간 20[s] | 손실 0.05\n",
            "| 에폭 23 |  반복 281 / 351 | 시간 22[s] | 손실 0.09\n",
            "| 에폭 23 |  반복 301 / 351 | 시간 23[s] | 손실 0.08\n",
            "| 에폭 23 |  반복 321 / 351 | 시간 25[s] | 손실 0.05\n",
            "| 에폭 23 |  반복 341 / 351 | 시간 26[s] | 손실 0.04\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[92m☑\u001b[0m 1427\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 96.600%\n",
            "| 에폭 24 |  반복 1 / 351 | 시간 0[s] | 손실 0.02\n",
            "| 에폭 24 |  반복 21 / 351 | 시간 1[s] | 손실 0.02\n",
            "| 에폭 24 |  반복 41 / 351 | 시간 3[s] | 손실 0.02\n",
            "| 에폭 24 |  반복 61 / 351 | 시간 4[s] | 손실 0.02\n",
            "| 에폭 24 |  반복 81 / 351 | 시간 6[s] | 손실 0.01\n",
            "| 에폭 24 |  반복 101 / 351 | 시간 7[s] | 손실 0.01\n",
            "| 에폭 24 |  반복 121 / 351 | 시간 9[s] | 손실 0.01\n",
            "| 에폭 24 |  반복 141 / 351 | 시간 11[s] | 손실 0.01\n",
            "| 에폭 24 |  반복 161 / 351 | 시간 12[s] | 손실 0.01\n",
            "| 에폭 24 |  반복 181 / 351 | 시간 14[s] | 손실 0.01\n",
            "| 에폭 24 |  반복 201 / 351 | 시간 15[s] | 손실 0.01\n",
            "| 에폭 24 |  반복 221 / 351 | 시간 17[s] | 손실 0.01\n",
            "| 에폭 24 |  반복 241 / 351 | 시간 19[s] | 손실 0.01\n",
            "| 에폭 24 |  반복 261 / 351 | 시간 20[s] | 손실 0.01\n",
            "| 에폭 24 |  반복 281 / 351 | 시간 22[s] | 손실 0.01\n",
            "| 에폭 24 |  반복 301 / 351 | 시간 24[s] | 손실 0.01\n",
            "| 에폭 24 |  반복 321 / 351 | 시간 25[s] | 손실 0.01\n",
            "| 에폭 24 |  반복 341 / 351 | 시간 27[s] | 손실 0.01\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[92m☑\u001b[0m 1427\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 99.480%\n",
            "| 에폭 25 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
            "| 에폭 25 |  반복 21 / 351 | 시간 1[s] | 손실 0.01\n",
            "| 에폭 25 |  반복 41 / 351 | 시간 3[s] | 손실 0.01\n",
            "| 에폭 25 |  반복 61 / 351 | 시간 4[s] | 손실 0.01\n",
            "| 에폭 25 |  반복 81 / 351 | 시간 5[s] | 손실 0.01\n",
            "| 에폭 25 |  반복 101 / 351 | 시간 7[s] | 손실 0.01\n",
            "| 에폭 25 |  반복 121 / 351 | 시간 8[s] | 손실 0.01\n",
            "| 에폭 25 |  반복 141 / 351 | 시간 10[s] | 손실 0.01\n",
            "| 에폭 25 |  반복 161 / 351 | 시간 11[s] | 손실 0.01\n",
            "| 에폭 25 |  반복 181 / 351 | 시간 13[s] | 손실 0.01\n",
            "| 에폭 25 |  반복 201 / 351 | 시간 15[s] | 손실 0.01\n",
            "| 에폭 25 |  반복 221 / 351 | 시간 16[s] | 손실 0.01\n",
            "| 에폭 25 |  반복 241 / 351 | 시간 17[s] | 손실 0.01\n",
            "| 에폭 25 |  반복 261 / 351 | 시간 19[s] | 손실 0.01\n",
            "| 에폭 25 |  반복 281 / 351 | 시간 20[s] | 손실 0.01\n",
            "| 에폭 25 |  반복 301 / 351 | 시간 22[s] | 손실 0.01\n",
            "| 에폭 25 |  반복 321 / 351 | 시간 23[s] | 손실 0.01\n",
            "| 에폭 25 |  반복 341 / 351 | 시간 25[s] | 손실 0.01\n",
            "Q   58+77\n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 461+579\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q  48+285\n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q   551+8\n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q  55+763\n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 752+006\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 292+167\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 795+038\n",
            "T 1427\n",
            "\u001b[92m☑\u001b[0m 1427\n",
            "---\n",
            "Q  838+62\n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q  39+341\n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 98.840%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DkC60IAywE4"
      },
      "source": [
        "Peeky 기법의 요약\n",
        "seq2seq의 encoder 동작은 고정길이 벡터 h로 변환하는데 이때 h안에는 decoder에게 필요한 정보가 담겨있음\n",
        "그러나 seq2seq는 최초 시각의 lstm(그림참조) 만이 벡터를 이용하고 있기 때문에\n",
        " 이 중요한 정보인 h를 더 활용하기위해 전체 lstm과 전체 affine 계층에 값을 분배해주어서\n",
        "계산할 때 concat 형태로 값을 더 추가해줌.\n",
        "코드 보면 326쪽 세 줄 보면 알 수 있음.(hs, out 확인)\n",
        " 따라서 seq2seq의 개선 2가지 방법인 reverse, peeky를 이용해서 정답률을 매우 높일 수 있음.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkHrkCIxyCof"
      },
      "source": [
        "# 8장 - 어텐션"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8dZ_wTjyZE4"
      },
      "source": [
        "-seq2seq의 문제점 \n",
        "\n",
        "```\n",
        "encoder의 출력이 '고정 길이의 벡터' 인데 사실\n",
        "여기서 고정 길이가 문제임. 예로, 아무리 긴 문장이 입력되더라도 항상 똑같은 길이의 벡터에 밀어 넣어야함.\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv5OTK_jy-4o"
      },
      "source": [
        "지금까지 우리는 LSTM 계층의 마지막 은닉 상태만을 Decoder에 전달했다. \n",
        "\n",
        "그러나 Encoder 출력의 길이는 입력 문장의 길이에 따라 바꿔주는게 좋음.\n",
        "이 점이 Encoder의 개선 포인트\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TzFS2HzzTvd"
      },
      "source": [
        "시각별 LSTM계층의 은닉 상태에는 직전에 입력된 단어에 대한 정보가 많이 포함되어있음."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn_b29yYzn4l"
      },
      "source": [
        "**Decoder 개선 1**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "앞의 Decoder는 Encoder의 LSTM 계층의 마지막 은닉 상태만을 이용함. \n",
        "이 Encoder의 출력 전부를 활용할 수 있도록 Decoder를 개선해야함.\n",
        "\n",
        "+\n",
        "\n",
        "입력과 출력의 여러 단어 중 어떤 단어끼리 서로 관련되어 있는가 라는 대응관계를 seq2seq에 학습시킬 수는 없을까?\n",
        "\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNys-w2C0Z1-"
      },
      "source": [
        "앞으로의 목표는 '도착어 단어'와 대응 관계에 있는 '출발어 단어'의 정보를 골라내는 것, 그리고 그 정보를 이용하여 번역을 수행하는 것이 됨. \n",
        "\n",
        "다시 말해서, 필요한 정보에만 주목하여 그 정보로부터 시계열 변환을 수행하는 것이 목표.\n",
        "\n",
        "이 구조를 **어텐션**이라고 함."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-rAB_me1ayX"
      },
      "source": [
        "그림 8-6\n",
        "\n",
        "```\n",
        "각 시각에서 Decoder에 입력된 단어와 대응관계인 단어의 벡터를 hs에서 골라냄.\n",
        "예컨데 Decoder가 \"I\"를 출력할 때, hs에서 \"나\"에 대응하는 벡터를 선택하면 됨.\n",
        "그리고 이러한 선택작업을 어떤 계산으로 해내겠다는 것.\n",
        "하지만 여기서 문제는 선택하는 작업은 미분 불가능하다는 점.\n",
        "미분이 불가능 하다는 것은 오차역전파법을 사용할 수 없다는 뜻.\n",
        "그러면 선택하는 작업을 미분 가능한 연산으로 대체할 수는 없을까?\n",
        "이건 아주 단순함. 모든 것을 선택하고 각 단어의 중요도를 나타내는 '가중치'를 별도록 계산하도록 함.\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogg7C87H0X6x",
        "outputId": "adbdeb16-5a08-4986-fabf-f988b8e62a64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Encoder가 출력하는 hs와 각 단어의 가중치 a를 적당하게 작성하고\n",
        "# 그 가중합을 구하는 구현을 볼 수 있음. 다차원 배열의 형상에 주의\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "T, H = 5,4\n",
        "hs = np.random.randn(T, H)\n",
        "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
        "print(hs)\n",
        "# (5, 4) matrix\n",
        "print()\n",
        "\n",
        "ar = a.reshape(5, 1).repeat(4, axis = 1) # 인덱스가 1인 축이 복사됨.\n",
        "# (5, ) 인 a를 reshpae 를 통해 (5, 1)로 만들어 주었고, \n",
        "# 이 때, 인덱스가 1인 축은 1이기 때문에 얘를 4로 만들어서 (5, 4)가 됨.\n",
        "print(ar)\n",
        "# (5, 4) matrix\n",
        "print()\n",
        "\n",
        "t = hs *ar # 는 원소별 곱임.\n",
        "print(t)\n",
        "# (5, 4) matrix\n",
        "print()\n",
        "\n",
        "c = np.sum(t, axis = 0) # 0번 째 축을 사라지게 했으므로 (4, )인 행렬(벡터)가 됨. \n",
        "print(c)\n",
        "# (4,) matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.32324532  0.56055382 -1.78052958  0.17092869]\n",
            " [ 0.10126337 -0.64188376 -0.68944941 -0.67340752]\n",
            " [-0.486619   -0.91728497 -0.26664458 -0.44066177]\n",
            " [ 2.96765858  1.36507741 -0.67282477 -0.27457303]\n",
            " [ 0.33064809  1.43003607  1.09433449 -2.04925985]]\n",
            "\n",
            "[[0.8  0.8  0.8  0.8 ]\n",
            " [0.1  0.1  0.1  0.1 ]\n",
            " [0.03 0.03 0.03 0.03]\n",
            " [0.05 0.05 0.05 0.05]\n",
            " [0.02 0.02 0.02 0.02]]\n",
            "\n",
            "[[-0.25859626  0.44844306 -1.42442366  0.13674295]\n",
            " [ 0.01012634 -0.06418838 -0.06894494 -0.06734075]\n",
            " [-0.01459857 -0.02751855 -0.00799934 -0.01321985]\n",
            " [ 0.14838293  0.06825387 -0.03364124 -0.01372865]\n",
            " [ 0.00661296  0.02860072  0.02188669 -0.0409852 ]]\n",
            "\n",
            "[-1.08072599e-01  4.53590722e-01 -1.51312249e+00  1.46849978e-03]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBLVn5Jd2e1f",
        "outputId": "2c14de10-dc75-457a-d745-08fdda86bcc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 미니배치 처리용 가중합을 구현\n",
        "N, T, H = 10, 5, 4\n",
        "hs = np.random.randn(N, T, H)\n",
        "a = np.random.randn(N, T)\n",
        "ar = a.reshape(N, T, 1).repeat(H, axis = 2) # (N, T, H)\n",
        "# ar = a.reshape(N, T, 1) # 브로드캐스트를 사용하는 경우\n",
        "\n",
        "t = hs * ar\n",
        "print(t.shape)\n",
        "print()\n",
        "\n",
        "c = np.sum(t, axis = 1)\n",
        "print(c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 5, 4)\n",
            "\n",
            "[[-0.97017467 -0.12536326 -2.21145688 -0.82885679]\n",
            " [-3.2887129  -0.73763362  1.5216417   3.69981184]\n",
            " [-0.43066919 -0.97167624  1.88205878 -2.06787287]\n",
            " [-2.11867124  0.35151809 -0.1731855  -0.13746932]\n",
            " [ 0.51858182 -0.46345075  0.78704953 -1.24007048]\n",
            " [-1.23538548  0.55757632  3.5650881   1.63020288]\n",
            " [ 0.61038181  4.20620317  1.24454928 -1.80313437]\n",
            " [ 0.50495739  1.52402914 -1.08304405 -0.40176934]\n",
            " [ 2.34232631  1.81122343  3.52361982 -0.25108729]\n",
            " [ 4.78246371  4.50810305  0.84944852 -0.47229628]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWG2nxa79NLO"
      },
      "source": [
        "# 역전파도 살펴 보겠음.\n",
        "# sum의 역전파는 repeat 이고, repeat의 역전파는 sum임.\n",
        "\n",
        "\n",
        "class WeightSum:\n",
        "  def __init__(self):\n",
        "    self.params, self.grads = [], []\n",
        "    self.cache = None\n",
        "\n",
        "  def forward(self, hs, a):\n",
        "    N, T, H = hs.shape\n",
        "\n",
        "    ar = a.reshape(N, T, 1).repeat(H, axis = 2)\n",
        "    t = hs * ar\n",
        "    c = np.sum(t, axis = 1)\n",
        "\n",
        "    self.cache = (hs, ar)\n",
        "    return c\n",
        "\n",
        "  def backward(self, dc):\n",
        "    hs, ar = self.cache\n",
        "    N, T, H = hs.shape\n",
        "\n",
        "    # 그림 8-11 확인하면 이해하기 더 쉬움.\n",
        "    dt = dc.reshape(N, 1, H).repeat(T, axis = 1) # sum의 역전파\n",
        "    dar = dt * hs # ar 쪽으로 가는 기울기\n",
        "    dhs = dt * ar # hs 쪽으로 가는 기울기\n",
        "    da = np.sum(dar, axis = 2) # repeat의 역전파\n",
        "\n",
        "    return dhs, da"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Hq0OPK-lwM"
      },
      "source": [
        "이것이 맥락벡터를 구하는 Weight Sum 계층의 구현임.\n",
        "이 계층은 학습하는 매개변수가 없으므로, 이 책의 구현 규칙에 따라 self.params = []로 설정.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkZLjrzt-wCT"
      },
      "source": [
        "**Decoder 개선 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mEAXh-V-_aD"
      },
      "source": [
        "각 단어의 중요도를 나타내는 가중치 a가 있다면, 가중합을 이용해 '맥락 벡터'를 얻을 수 있다.\n",
        "\n",
        "근데 이 a는 어떻게 구해야하는가?\n",
        "\n",
        "당연히 데이터로부터 자동으로 학습할 수 있도록 준비해야하겠다.(딥러닝 특징)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMr8oav__Rcq"
      },
      "source": [
        "각 단어의 가중치 a를 구하는 방법을 살펴보자.\n",
        "\n",
        " 우선 Decoder의 첫 번째 LSTM 계층이 은닉 상태 벡터를 출력할 때까지의 처리부터 알아봐야한다.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Decoder의 LSTM 계층의 은닉 상태 벡터를 h라 하자.\n",
        "\n",
        "지금 목표는 hs(Encoder의 출력 결과)의 각 단어 벡터와 h가 얼마나 '비슷한가'를 수치로 나타내는 것이다.\n",
        "\n",
        "이 때 계산 방법은 가장 단순한 방법인 벡터의 '내적'을 이용하자.\n",
        "\n",
        "내적의 직관적인 의미는 '두 벡터가 얼마나 같은 방향을 향하고 있는가' 이다.\n",
        "\n",
        "따라서 두 벡터의 '유사도'를 표현하는 척도로 내적을 이용하는 것은 자연스러운 선택이락도 할 수 있다.\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls2c5TidAGtS"
      },
      "source": [
        "벡터의 내적을 이용해 h와 hs의 각 단어 벡터와의 유사도를 구하고 이를 s라고 하자.\n",
        "\n",
        "s는 정규화하기 전의 값이며 'score'라고도 한다.\n",
        "\n",
        "s를 정규화하기 위해서는 일반적으로 소프트맥스 함수를 적용한다.\n",
        "\n",
        "softmax 함수를 이용하면 a의 각 원소는 0.0~ 1.0 사이의 값이 되고, 모든 원소의 총합은 1이 된다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs_r5Pgi-vxx",
        "outputId": "c9d15979-ffea-416b-c7ff-b7a02f344bd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master')\n",
        "from common.layers import Softmax\n",
        "import numpy as np\n",
        "\n",
        "N, T, H = 10, 5, 4\n",
        "hs = np.random.randn(N, T, H)\n",
        "h = np.random.randn(N, H)\n",
        "hr = h.reshape(N, 1, H).repeat(T, axis = 1)\n",
        "# hr = h.reshape(N, 1, H) # 브로드캐스트를 사용하는 경우\n",
        "\n",
        "t = hs * hr\n",
        "print(t.shape)\n",
        "# (10, 5, 4)\n",
        "print()\n",
        "\n",
        "s = np.sum(t, axis = 2)\n",
        "print(s.shape)\n",
        "# (10, 5)\n",
        "print()\n",
        "\n",
        "softmax = Softmax()\n",
        "a = softmax.forward(s)\n",
        "print(a.shape)\n",
        "# (10, 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 5, 4)\n",
            "\n",
            "(10, 5)\n",
            "\n",
            "(10, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo0XEzzdDZ6D"
      },
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.np import *  # import numpy as np\n",
        "from common.layers import Softmax\n",
        "\n",
        "class AttentionWeight:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.softmax = Softmax()\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, hs, h):\n",
        "        N, T, H = hs.shape\n",
        "\n",
        "        hr = h.reshape(N, 1, H)#.repeat(T, axis=1)\n",
        "        t = hs * hr\n",
        "        s = np.sum(t, axis=2)\n",
        "        a = self.softmax.forward(s)\n",
        "\n",
        "        self.cache = (hs, hr)\n",
        "        return a\n",
        "\n",
        "    def backward(self, da):\n",
        "        hs, hr = self.cache\n",
        "        N, T, H = hs.shape\n",
        "\n",
        "        ds = self.softmax.backward(da)\n",
        "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
        "        dhs = dt * hr\n",
        "        dhr = dt * hs\n",
        "        dh = np.sum(dhr, axis=1)\n",
        "\n",
        "        return dhs, dh\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5juqAlFR9fi"
      },
      "source": [
        "**Decoder 개선 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysNP2awfSCre"
      },
      "source": [
        "지금까지 Weight Sum과 Attention Weight 계층을 각각 구현했다.\n",
        "\n",
        "이제 이 두계층을 하나로 결합해보자.\n",
        "\n",
        "다시 말하지만, Attention Weight 계층은 Encoder가 출력하는 각 단어의 벡터 hs에 주목하여 해당 단어의 가중치 a를 구한다.\n",
        "\n",
        "이어 Weight Sum 게층이 a와 hs의 가중합을 구하고, 그 결과를 맥락 벡터 c로 출력한다.\n",
        "\n",
        "앞으로 이 일련의 계산을 수행하는 계층을 Attention 계층이라고 부르겠다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrQnw220Robj"
      },
      "source": [
        "class Attention:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.attention_weight_layer = AttentionWeight()\n",
        "        self.weight_sum_layer = WeightSum()\n",
        "        self.attention_weight = None\n",
        "\n",
        "    def forward(self, hs, h):\n",
        "        a = self.attention_weight_layer.forward(hs, h)\n",
        "        out = self.weight_sum_layer.forward(hs, a)\n",
        "        self.attention_weight = a\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
        "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
        "        dhs = dhs0 + dhs1\n",
        "        return dhs, dh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvpzugaDUQsS"
      },
      "source": [
        "이 코드는 2개의 계층에 의한 순전파와 역전파를 수행할 뿐임.\n",
        "\n",
        "이때 각 단어의 가중치를 나중에 참조할 수 있도록 attention_weight이라는 인스턴스 변수에 저장함.\n",
        "\n",
        "이제 이 Attention 계층을 우리는 LSTM 계층과 Affine 계층 사이에 삽입하면 끝.\n",
        "\n",
        "그림에서 보듯, 각 시각의 Attention 계층에는 Encoder의 출력인 hs가 입력됨.\n",
        "\n",
        "또 여기서는 LSTM 계층의 은닉 상태 벡터를 Affine 계층에 입력함.\n",
        "\n",
        "이는 앞 장에서 본 Decoder의 개선으로부터 자연스럽게 확장된 것으로 볼 수 있음.\n",
        "\n",
        "그림처럼 앞 장의 Decoder에 어텐션 정보를 '추가'할 수 있기 때문."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nSakQauVNZe"
      },
      "source": [
        "그림 [8-19]의 오른쪽은 앞 장의 Decoder에 Attention 계층이 구한 맥락 벡터 정보를 '추가'한 것으로 생각할 수 있음.\n",
        "\n",
        "Affine 계층에는 기존과 마찬가지로 LSTM 계층의 은닉 상태 벡터를 주고, 여기에 더해 Attention 계층의 맥락 벡터까지 입력하는 것.\n",
        "\n",
        "앞장에서는 Affine 계층에 '맥락 벡터'와 '은닉 상태 벡터'라는 2개의 벡터가 입력 되었다. 이는 두 벡터를 '연결한 벡터'를 Affine 계층에 입력한다는 뜻.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iakga6wfTfPR"
      },
      "source": [
        "# 단지 다수의 Attention 계층을 모았을 뿐임.\n",
        "class TimeAttention:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.layers = None\n",
        "        self.attention_weights = None\n",
        "\n",
        "    def forward(self, hs_enc, hs_dec):\n",
        "        N, T, H = hs_dec.shape\n",
        "        out = np.empty_like(hs_dec)\n",
        "        self.layers = []\n",
        "        self.attention_weights = []\n",
        "\n",
        "        for t in range(T): # Attention 계층을 필요한 수만큼 만듬.\n",
        "            layer = Attention()\n",
        "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:]) \n",
        "            self.layers.append(layer)\n",
        "            self.attention_weights.append(layer.attention_weight) # 각 단어의 가중치를 리스트에서 보관\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        N, T, H = dout.shape\n",
        "        dhs_enc = 0\n",
        "        dhs_dec = np.empty_like(dout)\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = self.layers[t]\n",
        "            dhs, dh = layer.backward(dout[:, t, :])\n",
        "            dhs_enc += dhs\n",
        "            dhs_dec[:,t,:] = dh\n",
        "\n",
        "        return dhs_enc, dhs_dec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJw2Uu2YV7FO"
      },
      "source": [
        "**어텐션을 갖춘 seq2seq 구현**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqu_PPPpWACv"
      },
      "source": [
        "AttentionEncoder 클래스부터 구현하는데, 이 클래스는 앞 장에서 구현한 Encoder 클래스와 거의 같음.\n",
        "\n",
        "앞 장의 Encoder 클래스의 forward() 메서느는 LSTM 계층의 마지막 은닉 상태 벡터만을 반환했지만, 이번에는 모든 은닉 상태를 반환함.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WguuJE8MVvn6"
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.time_layers import *\n",
        "from ch07.seq2seq import Encoder, Seq2seq\n",
        "from ch08.attention_layer import TimeAttention\n",
        "\n",
        "# Encoder 구현\n",
        "class AttentionEncoder(Encoder):\n",
        "    def forward(self, xs):\n",
        "        xs = self.embed.forward(xs)\n",
        "        hs = self.lstm.forward(xs)\n",
        "        return hs\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        dout = self.lstm.backward(dhs)\n",
        "        dout = self.embed.backward(dout)\n",
        "        return dout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-14mamp5Wakb"
      },
      "source": [
        "이어서 Attention 계층을 이용한 Decoder의 구현임.\n",
        "\n",
        "그림 [8-21]에서 보듯, 앞 장의 구현과 마찬가지로 Softmax 계층의 앞까지를 Decoder로 구현하겠음. \n",
        "\n",
        "역시 앞 장과 마찬가지로, 순전파의 forward()와 역전파의 backward() 메서드뿐 아니라 새로운 단어열(혹은 문자열) 을 생성하는 generate() 메서드도 추가함. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeBtsV0FWTcQ"
      },
      "source": [
        "class AttentionDecoder:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
        "        self.attention = TimeAttention() ##\n",
        "        self.affine = TimeAffine(affine_W, affine_b)\n",
        "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def forward(self, xs, enc_hs):\n",
        "        h = enc_hs[:,-1]\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        out = self.embed.forward(xs)\n",
        "        dec_hs = self.lstm.forward(out)\n",
        "        c = self.attention.forward(enc_hs, dec_hs) ## \n",
        "        out = np.concatenate((c, dec_hs), axis=2) # Time Attention 계층의 출력과 LSTM 계층의 출력 연결\n",
        "        score = self.affine.forward(out)\n",
        "\n",
        "        return score\n",
        "\n",
        "    def backward(self, dscore):\n",
        "        dout = self.affine.backward(dscore)\n",
        "        N, T, H2 = dout.shape\n",
        "        H = H2 // 2\n",
        "\n",
        "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
        "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
        "        ddec_hs = ddec_hs0 + ddec_hs1\n",
        "        dout = self.lstm.backward(ddec_hs)\n",
        "        dh = self.lstm.dh\n",
        "        denc_hs[:, -1] += dh\n",
        "        self.embed.backward(dout)\n",
        "\n",
        "        return denc_hs\n",
        "\n",
        "    def generate(self, enc_hs, start_id, sample_size):\n",
        "        sampled = []\n",
        "        sample_id = start_id\n",
        "        h = enc_hs[:, -1]\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        for _ in range(sample_size):\n",
        "            x = np.array([sample_id]).reshape((1, 1))\n",
        "\n",
        "            out = self.embed.forward(x)\n",
        "            dec_hs = self.lstm.forward(out)\n",
        "            c = self.attention.forward(enc_hs, dec_hs)\n",
        "            out = np.concatenate((c, dec_hs), axis=2)\n",
        "            score = self.affine.forward(out)\n",
        "\n",
        "            sample_id = np.argmax(score.flatten())\n",
        "            sampled.append(sample_id)\n",
        "\n",
        "        return sampled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weXvMdzEXHok"
      },
      "source": [
        "**seq2seq 구현**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "takem6H6XKB8"
      },
      "source": [
        "앞 장에서 본 Seq2seq와 거의 같은데 다른 점은 Encoder 대신 AttentionEncoder, Decoder 대신 AttentionDecoder을 사용하는 것뿐임.\n",
        "\n",
        "따라서 앞 장의 Seq2seq 클래스를 상속하고, 초기화 메서드를 수정하는 것만으로 AttentionSeq2seq 클래스를 구현할 수 있음."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN6KqYGNWtaA"
      },
      "source": [
        "class AttentionSeq2seq(Seq2seq):\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        args = vocab_size, wordvec_size, hidden_size\n",
        "        self.encoder = AttentionEncoder(*args)\n",
        "        self.decoder = AttentionDecoder(*args)\n",
        "        self.softmax = TimeSoftmaxWithLoss()\n",
        "\n",
        "        self.params = self.encoder.params + self.decoder.params\n",
        "        self.grads = self.encoder.grads + self.decoder.grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VjMre4eX8Ws"
      },
      "source": [
        "**어텐션 평가**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgr6WplUX6eI"
      },
      "source": [
        "이 책에서는 '날짜 형식'을 변경하는 문제로 어텐션을 갖춘 seq2seq의 효과를 확인해보려 함.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Zob2blsYMTG"
      },
      "source": [
        "이번 절에서 '날짜 형식 변환' 문제를 살펴볼 것.\n",
        "\n",
        "영어권에서 사용되는 다양한 날짜 형식을 표준형식으로 변환하는 것이 목표임.\n",
        "\n",
        "예를 들어, 'september 27, 1994' 등의 날짜 데이터를 '1994-09-27' 같은 표준 형식으로 변환할 것임.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDyIo-zZYcM8"
      },
      "source": [
        "날짜 형식 변환 문제를 채용한 이유\n",
        "\n",
        "1. 이 문제가 겉보기만큼 간단하지 않다는 점\n",
        " - 입력되는 날짜 데이터에는 다양한 변형이 존재하여 변환 규칙이 나름 복잡해지기 때문.\n",
        "\n",
        "2. 문제의 입력(질문)과 출력(답변) 사이에 알기 쉬운 대응 관계가 있기 때문.\n",
        " - 년, 월, 일의 대응 관계가 존재하므로 어텐션이 각각의 원소에 올바르게 주목하고 있는지를 확인할 수 있음.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7uZy3bDXF_O",
        "outputId": "c0a75a1a-3bcf-44d1-8f73-244a1a7908f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "sys.path.append('../ch07')\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/deep-learning-from-scratch-2-master/deep-learning-from-scratch-2-master/ch08')\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset import sequence\n",
        "from common.optimizer import Adam\n",
        "from common.trainer import Trainer\n",
        "from common.util import eval_seq2seq\n",
        "from attention_seq2seq import AttentionSeq2seq\n",
        "from ch07.seq2seq import Seq2seq\n",
        "from ch07.peeky_seq2seq import PeekySeq2seq\n",
        "\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
        "char_to_id, id_to_char = sequence.get_vocab()\n",
        "\n",
        "# 입력 문장 반전\n",
        "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "vocab_size = len(char_to_id)\n",
        "wordvec_size = 16\n",
        "hidden_size = 256\n",
        "batch_size = 128\n",
        "max_epoch = 10\n",
        "max_grad = 5.0\n",
        "\n",
        "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "# model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "# model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "acc_list = []\n",
        "for epoch in range(max_epoch):\n",
        "    trainer.fit(x_train, t_train, max_epoch=1,\n",
        "                batch_size=batch_size, max_grad=max_grad)\n",
        "\n",
        "    correct_num = 0\n",
        "    for i in range(len(x_test)):\n",
        "        question, correct = x_test[[i]], t_test[[i]]\n",
        "        verbose = i < 10\n",
        "        correct_num += eval_seq2seq(model, question, correct,\n",
        "                                    id_to_char, verbose, is_reverse=True)\n",
        "\n",
        "    acc = float(correct_num) / len(x_test)\n",
        "    acc_list.append(acc)\n",
        "    print('정확도 %.3f%%' % (acc * 100))\n",
        "\n",
        "\n",
        "model.save_params()\n",
        "\n",
        "# 그래프 그리기\n",
        "x = np.arange(len(acc_list))\n",
        "plt.plot(x, acc_list, marker='o')\n",
        "plt.xlabel('에폭')\n",
        "plt.ylabel('정확도')\n",
        "plt.ylim(-0.05, 1.05)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.08\n",
            "| 에폭 1 |  반복 21 / 351 | 시간 13[s] | 손실 3.09\n",
            "| 에폭 1 |  반복 41 / 351 | 시간 27[s] | 손실 1.90\n",
            "| 에폭 1 |  반복 61 / 351 | 시간 41[s] | 손실 1.72\n",
            "| 에폭 1 |  반복 81 / 351 | 시간 55[s] | 손실 1.46\n",
            "| 에폭 1 |  반복 101 / 351 | 시간 69[s] | 손실 1.19\n",
            "| 에폭 1 |  반복 121 / 351 | 시간 82[s] | 손실 1.14\n",
            "| 에폭 1 |  반복 141 / 351 | 시간 96[s] | 손실 1.09\n",
            "| 에폭 1 |  반복 161 / 351 | 시간 110[s] | 손실 1.06\n",
            "| 에폭 1 |  반복 181 / 351 | 시간 124[s] | 손실 1.04\n",
            "| 에폭 1 |  반복 201 / 351 | 시간 138[s] | 손실 1.03\n",
            "| 에폭 1 |  반복 221 / 351 | 시간 151[s] | 손실 1.02\n",
            "| 에폭 1 |  반복 241 / 351 | 시간 165[s] | 손실 1.02\n",
            "| 에폭 1 |  반복 261 / 351 | 시간 179[s] | 손실 1.01\n",
            "| 에폭 1 |  반복 281 / 351 | 시간 193[s] | 손실 1.00\n",
            "| 에폭 1 |  반복 301 / 351 | 시간 207[s] | 손실 1.00\n",
            "| 에폭 1 |  반복 321 / 351 | 시간 221[s] | 손실 1.00\n",
            "| 에폭 1 |  반복 341 / 351 | 시간 234[s] | 손실 1.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "정확도 0.000%\n",
            "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 2 |  반복 21 / 351 | 시간 14[s] | 손실 1.00\n",
            "| 에폭 2 |  반복 41 / 351 | 시간 28[s] | 손실 0.99\n",
            "| 에폭 2 |  반복 61 / 351 | 시간 41[s] | 손실 0.99\n",
            "| 에폭 2 |  반복 81 / 351 | 시간 55[s] | 손실 0.99\n",
            "| 에폭 2 |  반복 101 / 351 | 시간 69[s] | 손실 0.99\n",
            "| 에폭 2 |  반복 121 / 351 | 시간 83[s] | 손실 0.99\n",
            "| 에폭 2 |  반복 141 / 351 | 시간 96[s] | 손실 0.98\n",
            "| 에폭 2 |  반복 161 / 351 | 시간 110[s] | 손실 0.98\n",
            "| 에폭 2 |  반복 181 / 351 | 시간 124[s] | 손실 0.97\n",
            "| 에폭 2 |  반복 201 / 351 | 시간 138[s] | 손실 0.95\n",
            "| 에폭 2 |  반복 221 / 351 | 시간 152[s] | 손실 0.94\n",
            "| 에폭 2 |  반복 241 / 351 | 시간 165[s] | 손실 0.90\n",
            "| 에폭 2 |  반복 261 / 351 | 시간 179[s] | 손실 0.83\n",
            "| 에폭 2 |  반복 281 / 351 | 시간 193[s] | 손실 0.74\n",
            "| 에폭 2 |  반복 301 / 351 | 시간 207[s] | 손실 0.66\n",
            "| 에폭 2 |  반복 321 / 351 | 시간 221[s] | 손실 0.58\n",
            "| 에폭 2 |  반복 341 / 351 | 시간 235[s] | 손실 0.46\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[91m☒\u001b[0m 2006-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[91m☒\u001b[0m 2007-08-09\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[91m☒\u001b[0m 1983-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[91m☒\u001b[0m 2016-11-08\n",
            "---\n",
            "정확도 51.680%\n",
            "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 0.35\n",
            "| 에폭 3 |  반복 21 / 351 | 시간 14[s] | 손실 0.30\n",
            "| 에폭 3 |  반복 41 / 351 | 시간 28[s] | 손실 0.21\n",
            "| 에폭 3 |  반복 61 / 351 | 시간 42[s] | 손실 0.14\n",
            "| 에폭 3 |  반복 81 / 351 | 시간 56[s] | 손실 0.09\n",
            "| 에폭 3 |  반복 101 / 351 | 시간 70[s] | 손실 0.07\n",
            "| 에폭 3 |  반복 121 / 351 | 시간 84[s] | 손실 0.05\n",
            "| 에폭 3 |  반복 141 / 351 | 시간 98[s] | 손실 0.04\n",
            "| 에폭 3 |  반복 161 / 351 | 시간 112[s] | 손실 0.03\n",
            "| 에폭 3 |  반복 181 / 351 | 시간 126[s] | 손실 0.03\n",
            "| 에폭 3 |  반복 201 / 351 | 시간 140[s] | 손실 0.02\n",
            "| 에폭 3 |  반복 221 / 351 | 시간 154[s] | 손실 0.02\n",
            "| 에폭 3 |  반복 241 / 351 | 시간 167[s] | 손실 0.02\n",
            "| 에폭 3 |  반복 261 / 351 | 시간 181[s] | 손실 0.01\n",
            "| 에폭 3 |  반복 281 / 351 | 시간 195[s] | 손실 0.01\n",
            "| 에폭 3 |  반복 301 / 351 | 시간 209[s] | 손실 0.01\n",
            "| 에폭 3 |  반복 321 / 351 | 시간 223[s] | 손실 0.01\n",
            "| 에폭 3 |  반복 341 / 351 | 시간 237[s] | 손실 0.01\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 99.900%\n",
            "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 21 / 351 | 시간 14[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 41 / 351 | 시간 28[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 61 / 351 | 시간 42[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 81 / 351 | 시간 56[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 101 / 351 | 시간 70[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 121 / 351 | 시간 84[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 141 / 351 | 시간 98[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 161 / 351 | 시간 112[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 181 / 351 | 시간 126[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 201 / 351 | 시간 141[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 221 / 351 | 시간 155[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 241 / 351 | 시간 170[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 261 / 351 | 시간 185[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 281 / 351 | 시간 200[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 301 / 351 | 시간 215[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 321 / 351 | 시간 229[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 341 / 351 | 시간 244[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 99.900%\n",
            "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 21 / 351 | 시간 15[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 41 / 351 | 시간 29[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 61 / 351 | 시간 44[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 81 / 351 | 시간 58[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 101 / 351 | 시간 72[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 121 / 351 | 시간 87[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 141 / 351 | 시간 101[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 161 / 351 | 시간 115[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 181 / 351 | 시간 129[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 201 / 351 | 시간 143[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 221 / 351 | 시간 158[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 241 / 351 | 시간 173[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 261 / 351 | 시간 188[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 281 / 351 | 시간 203[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 301 / 351 | 시간 217[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 321 / 351 | 시간 232[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 341 / 351 | 시간 247[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 99.920%\n",
            "| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 21 / 351 | 시간 15[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 41 / 351 | 시간 29[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 61 / 351 | 시간 43[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 81 / 351 | 시간 57[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 101 / 351 | 시간 71[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 121 / 351 | 시간 85[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 141 / 351 | 시간 99[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 161 / 351 | 시간 113[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 181 / 351 | 시간 127[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 201 / 351 | 시간 141[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 221 / 351 | 시간 155[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 241 / 351 | 시간 169[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 261 / 351 | 시간 183[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 281 / 351 | 시간 197[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 301 / 351 | 시간 211[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 321 / 351 | 시간 225[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 341 / 351 | 시간 239[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[91m☒\u001b[0m 1994-05-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[91m☒\u001b[0m 2013-08-22\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[91m☒\u001b[0m 2016-01-06\n",
            "---\n",
            "정확도 81.040%\n",
            "| 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 0.11\n",
            "| 에폭 7 |  반복 21 / 351 | 시간 14[s] | 손실 0.03\n",
            "| 에폭 7 |  반복 41 / 351 | 시간 28[s] | 손실 0.01\n",
            "| 에폭 7 |  반복 61 / 351 | 시간 42[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 81 / 351 | 시간 56[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 101 / 351 | 시간 70[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 121 / 351 | 시간 84[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 141 / 351 | 시간 98[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 161 / 351 | 시간 112[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 181 / 351 | 시간 126[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 201 / 351 | 시간 140[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 221 / 351 | 시간 154[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 241 / 351 | 시간 168[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 261 / 351 | 시간 182[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 281 / 351 | 시간 196[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 301 / 351 | 시간 210[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 321 / 351 | 시간 224[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 341 / 351 | 시간 238[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 100.000%\n",
            "| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 21 / 351 | 시간 14[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 41 / 351 | 시간 28[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 61 / 351 | 시간 42[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 81 / 351 | 시간 56[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 101 / 351 | 시간 70[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 121 / 351 | 시간 84[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 141 / 351 | 시간 98[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 161 / 351 | 시간 111[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 181 / 351 | 시간 125[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 201 / 351 | 시간 139[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 221 / 351 | 시간 153[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 241 / 351 | 시간 167[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 261 / 351 | 시간 181[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 281 / 351 | 시간 195[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 301 / 351 | 시간 209[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 321 / 351 | 시간 223[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 341 / 351 | 시간 237[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 100.000%\n",
            "| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 21 / 351 | 시간 14[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 41 / 351 | 시간 28[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 61 / 351 | 시간 42[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 81 / 351 | 시간 56[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 101 / 351 | 시간 70[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 121 / 351 | 시간 84[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 141 / 351 | 시간 98[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 161 / 351 | 시간 112[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 181 / 351 | 시간 126[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 201 / 351 | 시간 139[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 221 / 351 | 시간 153[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 241 / 351 | 시간 168[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 261 / 351 | 시간 182[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 281 / 351 | 시간 195[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 301 / 351 | 시간 209[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 321 / 351 | 시간 223[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 341 / 351 | 시간 237[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 100.000%\n",
            "| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 21 / 351 | 시간 15[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 41 / 351 | 시간 29[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 61 / 351 | 시간 43[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 81 / 351 | 시간 57[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 101 / 351 | 시간 71[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 121 / 351 | 시간 85[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 141 / 351 | 시간 99[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 161 / 351 | 시간 114[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 181 / 351 | 시간 128[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 201 / 351 | 시간 142[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 221 / 351 | 시간 156[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 241 / 351 | 시간 170[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 261 / 351 | 시간 184[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 281 / 351 | 시간 198[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 301 / 351 | 시간 212[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 321 / 351 | 시간 226[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 341 / 351 | 시간 240[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 100.000%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50640 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54253 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 51221 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54869 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 46020 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50640 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54253 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 51221 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54869 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 46020 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TSLA44MZdDO"
      },
      "source": [
        "이 코드는 앞 장에서 본 '덧셈'의 학습용 코드와 거의 같음.\n",
        "\n",
        "다른 점은 학습 데이터가 날짜 데이터라는 것, 그리고 모델로 AttentionSeq2seq를 사용한다는 점임.\n",
        "\n",
        "Reverse도 적용하였음"
      ]
    }
  ]
}